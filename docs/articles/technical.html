<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="spmodel">
<title>Technical Details • spmodel</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.1.3/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.1.3/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Technical Details">
<meta property="og:description" content="spmodel">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">spmodel</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.2.0</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="active nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <a class="dropdown-item" href="../articles/basics.html">An Overview of Basic Features in spmodel</a>
    <a class="dropdown-item" href="../articles/guide.html">A Detailed Guide to spmodel</a>
    <a class="dropdown-item" href="../articles/technical.html">Technical Details</a>
  </div>
</li>
<li class="nav-item">
  <a class="nav-link" href="../news/index.html">Changelog</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/USEPA/spmodel/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">



<script src="technical_files/accessible-code-block-0.0.1/empty-anchor.js"></script><div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Technical Details</h1>
                        <h4 data-toc-skip class="author">Michael Dumelle, Matt Higham, and Jay M. Ver Hoef</h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/USEPA/spmodel/blob/HEAD/vignettes/technical.Rmd" class="external-link"><code>vignettes/technical.Rmd</code></a></small>
      <div class="d-none name"><code>technical.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>This vignette covers technical details regarding the functions in <code>spmodel</code> that perform computations. We first provide a notation guide and then describe relevant details for each function.</p>
<p>If you use <code>spmodel</code> in a formal publication or report, please cite it. Citing <code>spmodel</code> lets us devote more resources to it in the future. To view the <code>spmodel</code> citation, run</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/citation.html" class="external-link">citation</a></span><span class="op">(</span>package <span class="op">=</span> <span class="st">"spmodel"</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; To cite spmodel in publications use:</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;   Michael Dumelle, Matt Higham, and Jay M. Ver Hoef (2022). spmodel:</span></span>
<span><span class="co">#&gt;   Spatial Statistical Modeling and Prediction. R package version 0.2.0.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; A BibTeX entry for LaTeX users is</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;   @Manual{,</span></span>
<span><span class="co">#&gt;     title = {spmodel: Spatial Statistical Modeling and Prediction},</span></span>
<span><span class="co">#&gt;     author = {Michael Dumelle and Matt Higham and Jay M. {Ver Hoef}},</span></span>
<span><span class="co">#&gt;     year = {2022},</span></span>
<span><span class="co">#&gt;     note = {R package version 0.2.0},</span></span>
<span><span class="co">#&gt;   }</span></span></code></pre>
<p>In addition to this document on the technical details of <code>spmodel</code>, there are three other vignettes:</p>
<ul>
<li>An overview of basic features in <code>spmodel</code>: <code><a href="../articles/basics.html">vignette("basics", "spmodel")</a></code>
</li>
<li>A detailed guide to <code>spmodel</code>: <code><a href="../articles/guide.html">vignette("guide", "spmodel")</a></code>
</li>
</ul>
</div>
<div class="section level2">
<h2 id="notation-guide">Notation Guide<a class="anchor" aria-label="anchor" href="#notation-guide"></a>
</h2>
<p><span class="math display">\[\begin{equation*}
  \begin{split}
   n &amp; = \text{Sample size} \\
   \mathbf{y} &amp; = \text{Response vector} \\
   \boldsymbol{\beta} &amp; = \text{Fixed effect parameter vector} \\
   \mathbf{X} &amp; = \text{Design matrix of known explanatory variables (covariates)} \\
   p &amp; = \text{The number of linearly independent columns in } \mathbf{X} \\
   \mathbf{Z} &amp; = \text{Design matrix of known random effect variables} \\
   \boldsymbol{\theta} &amp; = \text{Covariance parameter vector} \\   
   \boldsymbol{\Sigma} &amp; = \text{Covariance matrix evaluated at } \boldsymbol{\theta} \\
   \boldsymbol{\Sigma}^{-1} &amp; = \text{The inverse of } \boldsymbol{\Sigma} \\
   \boldsymbol{\Sigma}^{1/2} &amp; = \text{The square root of } \boldsymbol{\Sigma} \\
   \boldsymbol{\Sigma}^{-1/2} &amp; = \text{The inverse of } \boldsymbol{\Sigma}^{1/2} \\
   \boldsymbol{\Theta} &amp; = \text{General parameter vector} \\  
   \ell(\boldsymbol{\Theta}) &amp; = \text{Log-likelihood evaluated at } \boldsymbol{\Theta} \\
   \boldsymbol{\tau} &amp; = \text{Spatial (dependent) random error} \\
   \mathbf{A}^* &amp; = \boldsymbol{\Sigma}^{-1/2}\mathbf{A} \text{ for a general matrix } \mathbf{A} \text{ (this is known as whitening $\mathbf{A}$)} 
  \end{split}
\end{equation*}\]</span></p>
<p>A hat indicates the parameters are estimated (i.e., <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>) or evaluated at a relevant estimated parameter vector (e.g., <span class="math inline">\(\hat{\boldsymbol{\Sigma}}\)</span> is evaluated at <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span>). When <span class="math inline">\(\ell(\boldsymbol{\hat{\Theta}})\)</span> is written, it means the log-likelihood evaluated at its maximum, <span class="math inline">\(\boldsymbol{\hat{\Theta}}\)</span>. When the covariance matrix of <span class="math inline">\(\mathbf{A}\)</span> is <span class="math inline">\(\boldsymbol{\Sigma}\)</span>, we say <span class="math inline">\(\mathbf{A}^*\)</span> “whitens” <span class="math inline">\(\mathbf{A}\)</span> because <span class="math display">\[\begin{equation*}
\text{Cov}(\mathbf{A}^*) = \text{Cov}(\boldsymbol{\Sigma}^{-1/2}\mathbf{A}) = \boldsymbol{\Sigma}^{-1/2}\text{Cov}(\mathbf{A})\boldsymbol{\Sigma}^{-1/2} = \boldsymbol{\Sigma}^{-1/2}\boldsymbol{\Sigma} \boldsymbol{\Sigma}^{-1/2} = (\boldsymbol{\Sigma}^{-1/2}\boldsymbol{\Sigma}^{1/2})(\boldsymbol{\Sigma}^{1/2}\boldsymbol{\Sigma}^{-1/2}) = \mathbf{I}.
\end{equation*}\]</span> See Section<span class="math inline">\(~\)</span> for a discussion on obtaining <span class="math inline">\(\boldsymbol{\Sigma}^{1/2}\)</span>.</p>
<p>Additional notation is used in Section<span class="math inline">\(~\)</span> (<code><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict()</a></code>): <span class="math display">\[\begin{equation*}
  \begin{split}
   \mathbf{y}_o &amp; = \text{Observed response vector} \\
   \mathbf{y}_u &amp; = \text{Unobserved response vector} \\
   \mathbf{X}_o &amp; = \text{Design matrix of known explanatory variables at observed response variable locations} \\
   \mathbf{X}_u &amp; = \text{Design matrix of known explanatory variables at unobserved response variable locations} \\
   \boldsymbol{\Sigma}_o &amp; = \text{Covariance matrix of $\mathbf{y}_o$ evaluated at } \boldsymbol{\theta} \\
   \boldsymbol{\Sigma}_u &amp; = \text{Covariance matrix of $\mathbf{y}_u$ evaluated at } \boldsymbol{\theta} \\
   \boldsymbol{\Sigma}_{uo} &amp; = \text{A matrix of covariances between $\mathbf{y}_u$ and $\mathbf{y}_o$ evaluated at } \boldsymbol{\theta} \\
  \end{split}
\end{equation*}\]</span></p>
</div>
<div class="section level2">
<h2 id="sec:aic">
<code>AIC()</code> and <code>AICc()</code><a class="anchor" aria-label="anchor" href="#sec:aic"></a>
</h2>
<p>The <code><a href="https://rdrr.io/r/stats/AIC.html" class="external-link">AIC()</a></code> and <code><a href="../reference/AIC.spmod.html">AICc()</a></code> functions in <code>spmodel</code> are defined for restricted maximum likelihood and maximum likelihood estimation, which maximize a likelihood. The AIC and AICc as defined by <span class="citation">Hoeting et al. (2006)</span> are given by <span class="math display">\[\begin{equation*}\label{eq:sp_aic}
  \begin{split}
    \text{AIC} &amp; = -2\ell(\hat{\boldsymbol{\Theta}}) + 2(|\hat{\boldsymbol{\Theta}}|) \\
    \text{AICc} &amp; = -2\ell(\hat{\boldsymbol{\Theta}}) + 2n(|\hat{\boldsymbol{\Theta}}|) / (n - |\hat{\boldsymbol{\Theta}}| - 1),
  \end{split}
\end{equation*}\]</span> where <span class="math inline">\(|\hat{\boldsymbol{\Theta}}|\)</span> is the cardinality of <span class="math inline">\(\hat{\boldsymbol{\Theta}}\)</span>. For restricted maximum likelihood, <span class="math inline">\(\hat{\boldsymbol{\Theta}} \equiv \{\hat{\boldsymbol{\theta}}\}\)</span>. For maximum likelihood, <span class="math inline">\(\hat{\boldsymbol{\Theta}} \equiv \{\hat{\boldsymbol{\theta}}, \hat{\boldsymbol{\beta}}\}\)</span> The discrepancy arises because restricted maximum likelihood integrates the fixed effects out of the likelihood, and so the likelihood does not depend on <span class="math inline">\(\boldsymbol{\beta}\)</span>.</p>
<p>AIC comparisons between a model fit using restricted maximum likelihood and a model fit using maximum likelihood are meaningless, as the models are fit with different likelihoods. AIC comparisons between models fit using restricted maximum likelihood are only valid when the models have the same fixed effect structure. In contrast, AIC comparisons between models fit using maximum likelihood are valid when the models have different fixed effect structures.</p>
</div>
<div class="section level2">
<h2 id="sec:anova">
<code>anova()</code><a class="anchor" aria-label="anchor" href="#sec:anova"></a>
</h2>
<p>Test statistics from  are formed using the general linear hypothesis test. Let <span class="math inline">\(\mathbf{L}\)</span> be an <span class="math inline">\(l \times p\)</span> contrast matrix and <span class="math inline">\(l_0\)</span> be an <span class="math inline">\(l \times 1\)</span> vector. The null hypothesis is that <span class="math inline">\(\mathbf{L} \boldsymbol{\hat{\beta}} = l_0\)</span> and the alternative hypothesis is that <span class="math inline">\(\mathbf{L} \boldsymbol{\hat{\beta}} \neq l_0\)</span>. Usually, <span class="math inline">\(l_0\)</span> is the zero vector (in <code>spmodel</code>, this is assumed). The test statistic is denoted <span class="math inline">\(Chi2\)</span> and is given by <span class="math display">\[\begin{equation*}\label{eq:glht}
  Chi2 = [(\mathbf{L} \boldsymbol{\hat{\beta}} - l_0)^\top(\mathbf{L} (\mathbf{X}^\top \mathbf{\hat{\Sigma}} \mathbf{X})^{-1} \mathbf{L}^\top)^{-1}(\mathbf{L} \boldsymbol{\hat{\beta}} - l_0)]
\end{equation*}\]</span> By default, <span class="math inline">\(\mathbf{L}\)</span> is chosen such that each variable in the data used to fit the model is tested marginally (i.e., controlling for the other variables) against <span class="math inline">\(l_0 = \mathbf{0}\)</span>. If this default is not desired, the  and  arguments can be used to pass user-defined <span class="math inline">\(\mathbf{L}\)</span> matrices to . They must be constructed in such a way that <span class="math inline">\(l_0 = \mathbf{0}\)</span>.</p>
<p>It is notoriously difficult to determine appropriate p-values for linear mixed models based on the general linear hypothesis test. lme4, for example, does not report p-values by default. A few reasons why obtaining p-values is so challenging:</p>
<ul>
<li>The first (and often most important) challenge is that when estimating <span class="math inline">\(\boldsymbol{\theta}\)</span> using a finite sample, it is usually not clear what the null distribution of <span class="math inline">\(Chi2\)</span> is. In certain cases such as ordinary least squares regression or some experimental designs (e.g., blocked design, split plot design, etc.), <span class="math inline">\(Chi2 / rank(\mathbf{L})\)</span> is F-distributed with known numerator and denominator degrees of freedom. But outside of these well-studied cases, no general results exist.</li>
<li>The second challenge is that the standard error of <span class="math inline">\(Chi2\)</span> does not account for the uncertainty in <span class="math inline">\(\boldsymbol{\hat{\theta}}\)</span>. For some approaches to addressing this problem, see <span class="citation">Kackar and Harville (1984)</span>, <span class="citation">Prasad and Rao (1990)</span>, <span class="citation">Harville and Jeske (1992)</span>, and <span class="citation">Kenward and Roger (1997)</span>.</li>
<li>The third challenge is in determining denominator degrees of freedom. Again, in some cases, these are known – but this is not true in general. For some approaches to addressing this problem, see <span class="citation">Satterthwaite (1946)</span>, <span class="citation">Schluchter and Elashoff (1990)</span>, <span class="citation">Hrong-Tai Fai and Cornelius (1996)</span>, <span class="citation">Kenward and Roger (1997)</span>, <span class="citation">Littell et al. (2006)</span>, <span class="citation">Pinheiro and Bates (2006)</span>, and <span class="citation">Kenward and Roger (2009)</span>.</li>
</ul>
<p>For these reasons, <code>spmodel</code> uses an asymptotic (i.e., large sample) Chi-squared test when calculating p-values using <code><a href="https://rdrr.io/r/stats/anova.html" class="external-link">anova()</a></code>. This approach addresses the three points above by assuming that with a large enough sample size:</p>
<ul>
<li>
<span class="math inline">\(Chi2\)</span> is asymptotically Chi-squared (under certain conditions) with <span class="math inline">\(rank(\mathbf{L})\)</span> degrees of freedom when the null hypothesis is true.</li>
<li>The uncertainty from estimating <span class="math inline">\(\boldsymbol{\hat{\theta}}\)</span> is small enough to be safely ignored.</li>
</ul>
<p>Because the approximation is asymptotic, degree of freedom adjustments can be ignored (it is also worth noting that an F distribution with infinite denominator degrees of freedom is a Chi-squared distribution scaled by <span class="math inline">\(rank(\mathbf{L})\)</span>. This asymptotic approximation implies these p-values are likely unreliable with small samples.</p>
<p>Note that when comparing full and reduced models, the general linear hypothesis test is analogous to an extra sum of (whitened) squares approach <span class="citation">(Myers et al. 2012)</span>.</p>
<p>A second approach to determining p-values is a likelihood ratio test. Let <span class="math inline">\(\ell(\boldsymbol{\hat{\Theta}})\)</span> be the log-likelihood for some full model and <span class="math inline">\(\ell(\boldsymbol{\hat{\Theta}}_0)\)</span> be the log-likelihood for some reduced model. For the likelihood ratio test to be valid, the reduced model must be nested in the full model, which means that <span class="math inline">\(\ell(\boldsymbol{\hat{\Theta}}_0)\)</span> is obtained by fixing some parameters in <span class="math inline">\(\boldsymbol{\Theta}\)</span>. When the likelihood ratio test is valid, <span class="math inline">\(X^2 = 2\ell(\boldsymbol{\hat{\Theta}}) - 2\ell(\boldsymbol{\hat{\Theta}}_0)\)</span> is asymptotically Chi-squared with degrees of freedom equal to the difference in estimated parameters between the full and reduced models.</p>
<p>For restricted maximum likelihood estimation, likelihood ratio tests can only be used to compare nested models with the same explanatory variables. To use likelihood ratio tests for comparing different explanatory variable structures, parameters must be estimated using maximum likelihood estimation. When using likelihood ratio tests to assess the importance of parameters on the boundary of a parameter space (e.g., a variance parameter being zero), p-values tend to be too large <span class="citation">(Self and Liang 1987; Stram and Lee 1994; Goldman and Whelan 2000; Pinheiro and Bates 2006)</span>.</p>
</div>
<div class="section level2">
<h2 id="sec:coef">
<code>coef()</code><a class="anchor" aria-label="anchor" href="#sec:coef"></a>
</h2>
<p><code><a href="https://rdrr.io/r/stats/coef.html" class="external-link">coef()</a></code> returns relevant coefficients based on the <code>type</code> argument. When <code>type = "fixed"</code> (the default), <code><a href="https://rdrr.io/r/stats/coef.html" class="external-link">coef()</a></code> returns <span class="math display">\[\begin{equation*}
\hat{\boldsymbol{\beta}} = (\mathbf{X}^\top \hat{\boldsymbol{\Sigma}}^{-1} \mathbf{X})^{-1}\mathbf{X}^\top \hat{\boldsymbol{\Sigma}}^{-1} \mathbf{y} .
\end{equation*}\]</span> If the estimation method is restricted maximum likelihood or maximum likelihood, <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> is known as the restricted maximum likelihood or maximum likelihood estimator of <span class="math inline">\(\boldsymbol{\beta}\)</span>. If the estimation method is semivariogram weighted least squares or semivariogram composite likelihood, <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> is known as the empirical generalized least squares estimator of <span class="math inline">\(\boldsymbol{\beta}\)</span>. When <code>type = "spcov"</code>, the estimated spatial covariance parameters are returned (available for all estimation methods). When <code>type = "randcov"</code>, the estimated random effect variance parameters are returned (available for restricted maximum likelihood and maximum likelihood estimation).</p>
</div>
<div class="section level2">
<h2 id="sec:confint">
<code>confint()</code><a class="anchor" aria-label="anchor" href="#sec:confint"></a>
</h2>
<p><code><a href="https://rdrr.io/r/stats/confint.html" class="external-link">confint()</a></code> returns confidence intervals for estimated parameters. Currently, <code><a href="https://rdrr.io/r/stats/confint.html" class="external-link">confint()</a></code> only returns confidence intervals for <span class="math inline">\(\boldsymbol{\beta}\)</span>. The <span class="math inline">\((1 - \alpha)\)</span>% confidence interval for <span class="math inline">\(\beta_i\)</span> is <span class="math display">\[\begin{equation*}
\hat{\beta}_i \pm z^* \sqrt{(\mathbf{X}^\top \hat{\boldsymbol{\Sigma}}^{-1} \mathbf{X})^{-1}_{i, i}},
\end{equation*}\]</span> where <span class="math inline">\((\mathbf{X}^\top \hat{\boldsymbol{\Sigma}}^{-1} \mathbf{X})^{-1}_{i, i}\)</span> is the <span class="math inline">\(i\)</span>th diagonal element in <span class="math inline">\((\mathbf{X}^\top \hat{\boldsymbol{\Sigma}}^{-1} \mathbf{X})^{-1}\)</span>, <span class="math inline">\(\Phi(z^*) = 1 - \alpha / 2\)</span>, <span class="math inline">\(\Phi(\cdot)\)</span> is the standard normal (Gaussian) cumulative distribution function, and <span class="math inline">\(\alpha = 1 -\)</span> <code>level</code>, where <code>level</code> is an argument to <code><a href="https://rdrr.io/r/stats/confint.html" class="external-link">confint()</a></code>. The default for <code>level</code> is 0.95, which corresponds to a <span class="math inline">\(z^*\)</span> of approximately 1.96.</p>
</div>
<div class="section level2">
<h2 id="sec:cooks">
<code>cooks.distance()</code><a class="anchor" aria-label="anchor" href="#sec:cooks"></a>
</h2>
<p>Cook’s distance measures the influence of an observation <span class="citation">(Cook 1979; Cook and Weisberg 1982)</span>. An influential observation has a large impact on the model fit. The vector of Cook’s distances for the spatial linear model is given by <span class="math display">\[\begin{equation} \label{eq:cooksd}
\frac{\mathbf{e}_p^2}{p}\frac{diag(\mathbf{H}_s)}{1 - diag(\mathbf{H}_s)},
\end{equation}\]</span> where <span class="math inline">\(\mathbf{e}_p\)</span> are the Pearson and <span class="math inline">\(diag(\mathbf{H}_s)\)</span> is the diagonal of the spatial hat matrix, <span class="math inline">\(\mathbf{H}_s \equiv \mathbf{X}^* (\mathbf{X}^{* \top} \mathbf{X}^*)^{-1} \mathbf{X}^{* \top}\)</span> <span class="citation">(Montgomery, Peck, and Vining 2021)</span>. The larger the Cook’s distance, the larger the influence.</p>
<p>To better understand the form in Equation , recall that the the non-spatial linear model <span class="math inline">\(\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}\)</span> assumes elements of <span class="math inline">\(\boldsymbol{\epsilon}\)</span> are independent and identically distributed (iid) with constant variance. In this context the vector of non-spatial Cook’s distances is given by <span class="math display">\[\begin{equation*}
\frac{\mathbf{e}_p^2}{p}\frac{diag(\mathbf{H})}{1 - diag(\mathbf{H})},
\end{equation*}\]</span> where <span class="math inline">\(diag(\mathbf{H})\)</span> is the diagonal of the non-spatial hat matrix, <span class="math inline">\(\mathbf{H} \equiv \mathbf{X} (\mathbf{X}^{\top} \mathbf{X})^{-1} \mathbf{X}^{\top}\)</span>. When the elements of <span class="math inline">\(\boldsymbol{\epsilon}\)</span> are not iid or do not have constant variance or both, the spatial Cook’s distance cannot be calculated using <span class="math inline">\(\mathbf{H}\)</span>. First the linear model must be whitened according to <span class="math inline">\(\mathbf{y}^* = \mathbf{X}^* \boldsymbol{\beta} + \boldsymbol{\epsilon}^*\)</span>, where <span class="math inline">\(\boldsymbol{\epsilon}^*\)</span> is the whitened version of the sum of all random errors in the model. Then the spatial Cook’s distance follows using the whitened version of <span class="math inline">\(\mathbf{X}\)</span>, <span class="math inline">\(\mathbf{X}^*\)</span>.</p>
</div>
<div class="section level2">
<h2 id="sec:deviance">
<code>deviance()</code><a class="anchor" aria-label="anchor" href="#sec:deviance"></a>
</h2>
<p>The deviance of a fitted model is <span class="math display">\[\begin{equation*}
\mathcal{D}_{\boldsymbol{\Theta}} = 2\ell(\boldsymbol{\Theta}_s) - 2\ell(\boldsymbol{\hat{\Theta}}),
\end{equation*}\]</span> where <span class="math inline">\(\ell(\boldsymbol{\Theta}_s)\)</span> is the log-likelihood of a “saturated” model that fits every observation perfectly. For normal (Gaussian) random errors, <span class="math display">\[\begin{equation*}
\mathcal{D}_{\boldsymbol{\Theta}} = (\mathbf{y} - \mathbf{X} \hat{\boldsymbol{\beta}})^\top \hat{\boldsymbol{\Sigma}}^{-1} (\mathbf{y} - \mathbf{X} \hat{\boldsymbol{\beta}})
\end{equation*}\]</span></p>
</div>
<div class="section level2">
<h2 id="sec:esv">
<code>esv()</code><a class="anchor" aria-label="anchor" href="#sec:esv"></a>
</h2>
<p>The empirical semivariogram is a moment-based estimate of the theoretical semivariogram. The empirical semivariogram quantifies half of the average squared difference in the response among observations in several distance classes. More formally, the empirical semivariogram is defined as <span class="math display">\[\begin{equation}\label{eq:esv}
  \hat{\gamma}(h) = \frac{1}{2|N(h)|} \sum_{N(h)} (y_i - y_j)^2, 
\end{equation}\]</span> where <span class="math inline">\(N(h)\)</span> is the set of observations in <span class="math inline">\(\mathbf{y}\)</span> that are <span class="math inline">\(h\)</span> distance units apart (distance classes) and <span class="math inline">\(|N(h)|\)</span> is the cardinality of <span class="math inline">\(N(h)\)</span> <span class="citation">(Cressie 1993)</span>. Often the set <span class="math inline">\(N(h)\)</span> contains observations that are <span class="math inline">\(h \pm c\)</span> apart, where <span class="math inline">\(c\)</span> is some constant. This approach is known as “binning” the empirical semivariogram. The default in <code>spmodel</code> is to construct the semivariogram using 15 equally spaced bins where <span class="math inline">\(h\)</span> is contained in <span class="math inline">\((0, h_{max}]\)</span>, and <span class="math inline">\(h_{max}\)</span> is known as a “distance cutoff”. Distance cutoffs are commonly used when constructing Equation<span class="math inline">\(~\)</span> because there tend to be few pairs with large distances. The default in <code>spmodel</code> is to use a cutoff of half the maximum distance (hypotenuse) of the domain’s bounding box.</p>
<p>The main purpose of the empirical semivariogram is its use in semivariogram weighted least squares estimation, though it can also be used as a visual diagnostic to assess the fit of a spatial covariance function.</p>
</div>
<div class="section level2">
<h2 id="sec:fitted">
<code>fitted()</code><a class="anchor" aria-label="anchor" href="#sec:fitted"></a>
</h2>
<p>Fitted values can be obtained for the response, spatial random errors, and random effects. The fitted values for the response (<code>type = "fixed"</code>), denoted <span class="math inline">\(\mathbf{\hat{y}}\)</span>, are given by <span class="math display">\[\begin{equation*}\label{eq:fit_resp}
  \mathbf{\hat{y}} = \mathbf{X} \boldsymbol{\hat{\beta}} .
\end{equation*}\]</span> They are the estimated mean response given the set of explanatory variables for each observation.</p>
<p>Fitted values for spatial random errors (<code>type = "spcov"</code>) and random effects (<code>type = "randcov"</code>) are linked to best linear unbiased predictors from linear mixed model theory. Consider the standard random effects parameterization <span class="math display">\[\begin{equation*}
  \mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \mathbf{Z} \mathbf{u} + \boldsymbol{\epsilon},
\end{equation*}\]</span> where <span class="math inline">\(\mathbf{Z}\)</span> denotes the random effects design matrix, <span class="math inline">\(\mathbf{u}\)</span> denotes the random effects, and <span class="math inline">\(\boldsymbol{\epsilon}\)</span> denotes independent random error. <span class="citation">Henderson (1975)</span> states that the best linear unbiased predictor (BLUP) of a single random effect <span class="math inline">\(\mathbf{u}\)</span>, denoted <span class="math inline">\(\mathbf{\hat{u}}\)</span>, is given by <span class="math display">\[\begin{equation}\label{eq:blup_mm}
  \mathbf{\hat{u}} = \sigma^2_u \mathbf{Z}^\top \mathbf{\Sigma}^{-1}(\mathbf{y} - \mathbf{X} \boldsymbol{\hat{\beta}}),
\end{equation}\]</span> where <span class="math inline">\(\sigma^2_u\)</span> is the variance of <span class="math inline">\(\mathbf{u}\)</span>.</p>
<p><span class="citation">Searle, Casella, and McCulloch (2009)</span> generalize this idea by showing that for a random variable <span class="math inline">\(\boldsymbol{\alpha}\)</span> in a linear model, the best linear unbiased predictor (based on the response, <span class="math inline">\(\mathbf{y}\)</span>) of <span class="math inline">\(\boldsymbol{\alpha}\)</span>, denoted <span class="math inline">\(\boldsymbol{\hat{\alpha}}\)</span>, is given by <span class="math display">\[\begin{equation}\label{eq:blup_gen}
  \boldsymbol{\hat{\alpha}} = \text{E}(\boldsymbol{\alpha}) + \boldsymbol{\Sigma}_\alpha \boldsymbol{\Sigma}^{-1}(\mathbf{y} - \mathbf{X} \boldsymbol{\hat{\beta}}),
\end{equation}\]</span> where <span class="math inline">\(\boldsymbol{\Sigma}_\alpha = \text{Cov}(\boldsymbol{\alpha}, \mathbf{y})\)</span>. Evaluating Equation<span class="math inline">\(~\)</span> at the plug-in (empirical) estimates of the covariance parameters yields the empirical best linear unbiased predictor (EBLUP) of <span class="math inline">\(\boldsymbol{\alpha}\)</span>.</p>
<p>Recall that the spatial linear model with random effects is <span class="math display">\[\begin{equation*}
  \mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \mathbf{Z} \mathbf{u} + \boldsymbol{\tau} + \boldsymbol{\epsilon},
\end{equation*}\]</span> Building from the result in Equation<span class="math inline">\(~\)</span>, we can find BLUPs for each random term in the spatial linear model (<span class="math inline">\(\mathbf{u}\)</span>, <span class="math inline">\(\boldsymbol{\tau}\)</span>, and <span class="math inline">\(\boldsymbol{\epsilon}\)</span>). For example, the BLUP of <span class="math inline">\(\mathbf{u}\)</span> is found by noting that <span class="math inline">\(\text{E}(\mathbf{u}) = \mathbf{0}\)</span> and <span class="math display">\[\begin{equation*}
  \mathbf{\Sigma}_u = \text{Cov}(\mathbf{u}, \mathbf{y}) = \text{Cov}(\mathbf{u}, \mathbf{X} \boldsymbol{\beta} + \mathbf{Z} \mathbf{u} + \boldsymbol{\tau} + \boldsymbol{\epsilon}) = \text{Cov}(\mathbf{u}, \mathbf{Z}\mathbf{u}) = \text{Cov}(\mathbf{u}, \mathbf{u})\mathbf{Z}^\top = \sigma^2_u \mathbf{Z}^\top,
\end{equation*}\]</span> where the result follows because the random terms in <span class="math inline">\(\mathbf{y}\)</span> are independent and <span class="math inline">\(\text{Cov}(\mathbf{u}, \mathbf{u}) = \sigma^2_u \mathbf{I}\)</span>. Then it follows that <span class="math display">\[\begin{equation*}
  \hat{\mathbf{u}} = \text{E}(\mathbf{u}) + \boldsymbol{\Sigma}_u \boldsymbol{\Sigma}^{-1}(\mathbf{y} - \mathbf{X} \boldsymbol{\hat{\beta}}) = \sigma^2_u \mathbf{Z}^\top \boldsymbol{\Sigma}^{-1}(\mathbf{y} - \mathbf{X} \boldsymbol{\hat{\beta}}),
\end{equation*}\]</span> which matches Equation<span class="math inline">\(~\)</span>. Similarly, the BLUP of <span class="math inline">\(\boldsymbol{\tau}\)</span> is found by noting that <span class="math inline">\(\text{E}(\boldsymbol{\tau}) = \mathbf{0}\)</span> and <span class="math display">\[\begin{equation*}
  \mathbf{\Sigma}_{de} = \text{Cov}(\boldsymbol{\tau}, \mathbf{y}) = \text{Cov}(\boldsymbol{\tau}, \mathbf{X} \boldsymbol{\beta} + \mathbf{Z} \boldsymbol{\tau} + \boldsymbol{\epsilon}) = \text{Cov}(\boldsymbol{\tau}, \boldsymbol{\tau}) = \sigma^2_{de} \mathbf{R},
\end{equation*}\]</span> where the result follows because the random terms in <span class="math inline">\(\mathbf{y}\)</span> are independent and <span class="math inline">\(\text{Cov}(\boldsymbol{\tau}, \boldsymbol{\tau}) = \sigma^2_{de} \mathbf{R}\)</span>, and <span class="math inline">\(\sigma^2_{de}\)</span> is the variance of <span class="math inline">\(\boldsymbol{\tau}\)</span>. Then it follows that <span class="math display">\[\begin{equation}\label{eq:blup_sp}
  \hat{\boldsymbol{\tau}} = \text{E}(\boldsymbol{\tau}) + \boldsymbol{\Sigma}_{de} \boldsymbol{\Sigma}^{-1}(\mathbf{y} - \mathbf{X} \boldsymbol{\hat{\beta}}) = \sigma^2_{de} \mathbf{R} \boldsymbol{\Sigma}^{-1}(\mathbf{y} - \mathbf{X} \boldsymbol{\hat{\beta}}).
\end{equation}\]</span> Fitted values for <span class="math inline">\(\boldsymbol{\epsilon}\)</span> are obtained using similar arguments. Evaluating these equations at the plug-in (empirical) estimates of the covariance parameters yields EBLUPs.</p>
<p>When partition factors are used, the covariance matrix of all random effects (spatial and non-spatial) can be viewed as the interaction between the non-partitioned covariance matrix and the partition matrix, <span class="math inline">\(\mathbf{P}\)</span>. The <span class="math inline">\(ij\)</span>th entry in <span class="math inline">\(\mathbf{P}\)</span> equals one if observation <span class="math inline">\(i\)</span> and observation <span class="math inline">\(j\)</span> share the same level of the partition factor and zero otherwise. For spatial random effects, an adjustment is straightforward, as each column in <span class="math inline">\(\boldsymbol{\Sigma_{de}}\)</span> corresponds to a distinct spatial random effect. Thus with partition factors, <span class="math inline">\(\boldsymbol{\Sigma_{de}}^* = \boldsymbol{\Sigma_{de}} \odot \mathbf{P} = \sigma^2_{de} \mathbf{R} \odot \mathbf{P}\)</span>, where <span class="math inline">\(\odot\)</span> denotes the Hadmart (element-wise) product, is used instead used of <span class="math inline">\(\boldsymbol{\Sigma_{de}}\)</span> in Equation<span class="math inline">\(~\)</span>. Note that <span class="math inline">\(\boldsymbol{\Sigma_{ie}}\)</span> is unchanged as it is proportional to the identity matrix. For non-spatial random effects, however, the situation is more complicated. Applying Equation<span class="math inline">\(~\)</span> directly yields BLUPs of random effects corresponding to the interaction between random effect levels and partition levels. Thus a logical approach is to average the non-zero BLUPs for each random effect level across partition levels, yielding a prediction for the random effect level. This does not imply, however, that these estimates are BLUPs of the random effect.</p>
<p>For big data without partition factors, the local indexes act as partition factors. That is, the BLUPs correspond to random effects interacted with each local index. For big data with partition factors, an adjusted partition factor is created as the interaction between each local index and the partition factor. Then this adjusted partition factor is applied to Equation<span class="math inline">\(~\)</span>.</p>
</div>
<div class="section level2">
<h2 id="sec:hatvalues">
<code>hatvalues()</code><a class="anchor" aria-label="anchor" href="#sec:hatvalues"></a>
</h2>
<p>Hat values measure the leverage of an observation. An observation has high leverage if its combination of explanatory variables is atypical (far from the mean explanatory vector). The spatial leverage (hat) matrix is given by <span class="math display">\[\begin{equation}
\label{eq:leverage}
 \mathbf{H}_s = \mathbf{X}^* (\mathbf{X}^{* \top} \mathbf{X}^*)^{-1} \mathbf{X}^{* \top}.
\end{equation}\]</span> The diagonal of this matrix yields the leverage (hat) values for each observation <span class="citation">(Montgomery, Peck, and Vining 2021)</span>. The larger the hat value, the larger the leverage</p>
<p>To better understand the form in Equation , recall that the the non-spatial linear model <span class="math inline">\(\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}\)</span> assumes elements of <span class="math inline">\(\boldsymbol{\epsilon}\)</span> are independent and identically distributed (iid) with constant variance. In this context, the leverage (hat) matrix is given by <span class="math display">\[\begin{equation*}
\mathbf{H} \equiv \mathbf{X} (\mathbf{X}^{\top} \mathbf{X})^{-1} \mathbf{X}^{\top},
\end{equation*}\]</span> When the elements of <span class="math inline">\(\boldsymbol{\epsilon}\)</span> are not iid or do not have constant variance or both, the spatial leverage (hat) matrix is not <span class="math inline">\(\mathbf{H}\)</span>. First the linear model must be whitened according to <span class="math inline">\(\mathbf{y}^* = \mathbf{X}^* \boldsymbol{\beta} + \boldsymbol{\epsilon}^*\)</span>, where <span class="math inline">\(\boldsymbol{\epsilon}^*\)</span> is the whitened version of the sum of all random errors in the model. Then the spatial leverage (hat) matrix follows using the whitened version of <span class="math inline">\(\mathbf{X}\)</span>, <span class="math inline">\(\mathbf{X}^*\)</span>.</p>
</div>
<div class="section level2">
<h2 id="sec:loglik">
<code>logLik()</code><a class="anchor" aria-label="anchor" href="#sec:loglik"></a>
</h2>
<p>The log-likelihood is given by <span class="math inline">\(\ell(\boldsymbol{\hat{\Theta}})\)</span>.</p>
</div>
<div class="section level2">
<h2 id="sec:loocv">
<code>loocv()</code><a class="anchor" aria-label="anchor" href="#sec:loocv"></a>
</h2>
<p><span class="math inline">\(k\)</span>-fold cross validation is a useful tool for evaluating model fits using “hold-out” data. The data are split into <span class="math inline">\(k\)</span> sets. One-by-one, one of the <span class="math inline">\(k\)</span> sets is held out, the model is fit to the remaining <span class="math inline">\(k - 1\)</span> sets, and predictions at each observation in the hold-out set are compared to their true values. The closer the predictions are to the true observations, the better the model fit. A special case where <span class="math inline">\(k = n\)</span> is known as leave-one-out cross validation (loocv), as each observation is left out one-by-one. Computationally efficient solutions exist for leave-one-out cross validation in the non-spatial linear model (with iid, constant variance errors). Outside of this case, however, fitting <span class="math inline">\(n\)</span> separate models can be computationally infeasible. <code><a href="../reference/loocv.html">loocv()</a></code> makes a compromise that balances an approximation to the true solution with computational feasibility. First <span class="math inline">\(\boldsymbol{\theta}\)</span> is estimated using all of the data. Then for each of the <span class="math inline">\(n\)</span> model fits, <code><a href="../reference/loocv.html">loocv()</a></code> does not re-estimate <span class="math inline">\(\boldsymbol{\theta}\)</span> but does re-estimate <span class="math inline">\(\boldsymbol{\beta}\)</span>. This approach relies on the assumption that the covariance parameter estimates obtained using <span class="math inline">\(n - 1\)</span> observations are approximately the same as the covariance parameter estimates obtained using all <span class="math inline">\(n\)</span> observations. For a large enough sample size, this is a reasonable assumption.</p>
<p>First define <span class="math inline">\(\boldsymbol{\Sigma}_{-i, -i}\)</span> as <span class="math inline">\(\boldsymbol{\Sigma}\)</span> with the <span class="math inline">\(i\)</span>th row and column deleted, <span class="math inline">\(\boldsymbol{\Sigma}_{i, -i}\)</span> as the <span class="math inline">\(i\)</span>th row of <span class="math inline">\(\boldsymbol{\Sigma}\)</span> with the <span class="math inline">\(i\)</span>th column deleted, <span class="math inline">\(\boldsymbol{\Sigma}_{i, i}\)</span> as the <span class="math inline">\(i\)</span>th row and column of <span class="math inline">\(\boldsymbol{\Sigma}\)</span>, <span class="math inline">\(\mathbf{X}_{-i}\)</span> as <span class="math inline">\(\mathbf{X}\)</span> with the <span class="math inline">\(i\)</span>th row deleted, <span class="math inline">\(\mathbf{X}_{i}\)</span> as the <span class="math inline">\(i\)</span>th row of <span class="math inline">\(\mathbf{X}\)</span>, <span class="math inline">\(y_{-i}\)</span> as <span class="math inline">\(\mathbf{y}\)</span> with the <span class="math inline">\(i\)</span>th element deleted, and <span class="math inline">\(\mathbf{y}_i\)</span> as the <span class="math inline">\(i\)</span>th element of <span class="math inline">\(\mathbf{y}\)</span>. <span class="citation">Wolf (1978)</span> shows that given <span class="math inline">\(\boldsymbol{\Sigma}^{-1}\)</span>, a computationally efficient form for <span class="math inline">\(\boldsymbol{\Sigma}^{-1}_{-i}\)</span> exists. First observe that <span class="math inline">\(\boldsymbol{\Sigma}^{-1}\)</span> can be represented blockwise as <span class="math display">\[\begin{equation*}
 \boldsymbol{\Sigma}^{-1} = 
 \begin{bmatrix}
  \tilde{\boldsymbol{\Sigma}}_{-i, -i} &amp; \tilde{\boldsymbol{\Sigma}}_{i,-i}^\top \\
  \tilde{\boldsymbol{\Sigma}}_{i,-i} &amp; \tilde{\boldsymbol{\Sigma}}_{i, i}
 \end{bmatrix},
\end{equation*}\]</span> where the dimensions of each <span class="math inline">\(\tilde{\boldsymbol{\Sigma}}\)</span> match the respective dimensions of relevant blocks in <span class="math inline">\(\boldsymbol{\Sigma}\)</span>. Then it follows that <span class="math display">\[\begin{equation*}
 \boldsymbol{\Sigma}^{-1}_{-i, -i} = \tilde{\boldsymbol{\Sigma}}_{-i, -i} - \tilde{\boldsymbol{\Sigma}}_{i,-i}^\top \tilde{\boldsymbol{\Sigma}}_{i, i}^{-1}\tilde{\boldsymbol{\Sigma}}_{i,-i}
\end{equation*}\]</span> and <span class="math display">\[\begin{equation*}
  \boldsymbol{\beta}_{-i} = (\mathbf{X}^\top_{-i} \boldsymbol{\Sigma}^{-1}_{-i, -i} \mathbf{X}_{-i})^{-1} \mathbf{X}^\top_{-i} \boldsymbol{\Sigma}^{-1}_{-i, -i} \mathbf{y}_{-i},
\end{equation*}\]</span> where <span class="math inline">\(\boldsymbol{\beta}_i\)</span> is the estimate of <span class="math inline">\(\boldsymbol{\beta}\)</span> constructed without the <span class="math inline">\(i\)</span>th observation.</p>
<p>The loocv prediction of <span class="math inline">\(y_i\)</span> is then given by <span class="math display">\[\begin{equation*}
  \hat{y}_i = \mathbf{X}_i \hat{\boldsymbol{\beta}}_{-i} + \hat{\boldsymbol{\Sigma}}_{i, -i}\hat{\boldsymbol{\Sigma}}_{-i, -i}(\mathbf{y}_i - \mathbf{X}_{-i} \hat{\boldsymbol{\beta}}_{-i})
\end{equation*}\]</span> and the prediction variance of the loocv prediction of <span class="math inline">\(y_i\)</span> is given by <span class="math display">\[\begin{equation*}
  \dot{\sigma}^2_i = \hat{\boldsymbol{\Sigma}}_{i, i} - \hat{\boldsymbol{\Sigma}}_{i, - i} \hat{\boldsymbol{\Sigma}}^{-1}_{-i, -i} \hat{\boldsymbol{\Sigma}}_{i, - i}^\top + \mathbf{Q}_i(\mathbf{X}_{-i}^\top \hat{\boldsymbol{\Sigma}}_{-i, -i}^{-1} \mathbf{X}_{-i})^{-1}\mathbf{Q}_i^\top ,
\end{equation*}\]</span> <span class="math inline">\(\mathbf{Q}_i = \mathbf{X}_i - \hat{\boldsymbol{\Sigma}}_{i, -i} \hat{\boldsymbol{\Sigma}}^{-1}_{-i, -i} \mathbf{X}_{-i}\)</span>. These formulas are analogous to the formulas used to obtain linear unbiased predictions of unobserved data (Equation<span class="math inline">\(~\)</span>) and prediction variances (Equation<span class="math inline">\(~\)</span>) in Section<span class="math inline">\(~\)</span>. Model fits are evaluated using mean squared prediction error (mspe), formally defined as <span class="math display">\[\begin{equation*}
 mspe = \frac{1}{n}\sum_{i = 1}^n(y_i - \hat{y}_i)^2.
\end{equation*}\]</span> Generally the lower the <span class="math inline">\(mspe\)</span>, the better the model fit.</p>
<div class="section level3">
<h3 id="big-data">Big Data<a class="anchor" aria-label="anchor" href="#big-data"></a>
</h3>
<p>Options for big data leave-one-out cross validation rely on the <code>local</code> argument, which is passed to <code><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict()</a></code>. The <code>local</code> list for <code><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict()</a></code> is explained in detail in Section<span class="math inline">\(~\)</span>, but we provide a short summary of how <code>local</code> interacts with <code><a href="../reference/loocv.html">loocv()</a></code> here.</p>
<p>For <code><a href="../reference/splm.html">splm()</a></code> and <code><a href="../reference/spautor.html">spautor()</a></code> objects, <code>local</code> can be <code>"all"</code>. When <code>local = "all"</code>, all of the data are used for leave-one-out cross validation (i.e., it is implemented exactly as previously described). Parallelization is implemented when setting <code>parallel = TRUE</code> in <code>local</code>, and the number of cores to use for parallelization is specified via <code>ncores</code>.</p>
<p>For <code><a href="../reference/splm.html">splm()</a></code> objects, <code>local</code> can be <code>"covariance"</code> or <code>"distance"</code>. When <code>local = "covariance"</code>, then a number of observations (specified via the <code>size</code> argument) having the highest covariance with the held-out observation are used in the local neighborhood prediction approach. When <code>local = "distance"</code>, then a number of observations (specified via the <code>size</code> argument) closest to the held-out observation are used in the local neighborhood prediction approach. When no random effects are used, no partition factor is used, and the spatial covariance function is monotone decreasing, <code>"covariance"</code> and <code>"distance"</code> are equivalent. The local neighborhood approach only uses the observations in the local neighborhood of the held-out observation to perform prediction, and is thus an approximation to the true solution. Its computational efficiency derives from using <span class="math inline">\(\boldsymbol{\Sigma}_{l, l}\)</span> (the covariance matrix of the observations in the local neighborhood) instead of <span class="math inline">\(\boldsymbol{\Sigma}\)</span> (the covariance matrix of all the observations). Parallelization is implemented when setting <code>parallel = TRUE</code> in <code>local</code>, and the number of cores to use for parallelization is specified via <code>ncores</code>.</p>
</div>
</div>
<div class="section level2">
<h2 id="sec:predict">
<code>predict()</code><a class="anchor" aria-label="anchor" href="#sec:predict"></a>
</h2>
<div class="section level3">
<h3 id="interval-none">
<code>interval = "none"</code><a class="anchor" aria-label="anchor" href="#interval-none"></a>
</h3>
<p>The empirical best linear unbiased predictions (i.e., empirical Kriging predictor) of <span class="math inline">\(\mathbf{y}_u\)</span> are given by <span class="math display">\[\begin{equation}\label{eq:blup}
  \mathbf{\dot{y}}_u =  \mathbf{X}_u \hat{\boldsymbol{\beta}} + \hat{\boldsymbol{\Sigma}}_{uo} \hat{\boldsymbol{\Sigma}}^{-1}_{o} (\mathbf{y}_o - \mathbf{X}_o \hat{\boldsymbol{\beta}}) .
\end{equation}\]</span></p>
<p>Equation  is sometimes called an empirical universal Kriging predictor, a Kriging with external drift predictor, or a regression Kriging predictor.</p>
<p>The covariance matrix of <span class="math inline">\(\mathbf{\dot{y}}_u\)</span> <span class="math display">\[\begin{equation}\label{eq:blup_cov}
  \dot{\boldsymbol{\Sigma}}_u = \hat{\boldsymbol{\Sigma}}_u - \hat{\boldsymbol{\Sigma}}_{uo} \hat{\boldsymbol{\Sigma}}^{-1}_o \hat{\boldsymbol{\Sigma}}^\top_{uo} + \mathbf{Q}(\mathbf{X}_o^\top \hat{\boldsymbol{\Sigma}}_o^{-1} \mathbf{X}_o)^{-1}\mathbf{Q}^\top ,
\end{equation}\]</span> where <span class="math inline">\(\mathbf{Q} = \mathbf{X}_u - \hat{\boldsymbol{\Sigma}}_{uo} \hat{\boldsymbol{\Sigma}}^{-1}_o \mathbf{X}_o\)</span>.</p>
<p>When <code>se.fit = TRUE</code>, standard errors are returned by taking the square root of the diagonal of <span class="math inline">\(\dot{\boldsymbol{\Sigma}}_u\)</span> in Equation<span class="math inline">\(~\)</span>.</p>
</div>
<div class="section level3">
<h3 id="interval-prediction">
<code>interval = "prediction"</code><a class="anchor" aria-label="anchor" href="#interval-prediction"></a>
</h3>
<p>The empirical best linear unbiased predictions are returned by evaluating Equation<span class="math inline">\(~\)</span>. The (100 <span class="math inline">\(\times\)</span> <code>level</code>)% prediction interval for <span class="math inline">\((y_u)_i\)</span> is <span class="math inline">\((\dot{y}_u)_i \pm z^* \sqrt{(\dot{\boldsymbol{\Sigma}}_u)_{i, i}}\)</span>, where <span class="math inline">\(\sqrt{(\dot{\boldsymbol{\Sigma}}_u)_{i, i}}\)</span> is the standard error of <span class="math inline">\((\dot{y}_u)_i\)</span> obtained from <code>se.fit = TRUE</code>, <span class="math inline">\(\Phi(z^*) = 1 - \alpha / 2\)</span>, <span class="math inline">\(\Phi(\cdot)\)</span> is the standard normal (Gaussian) cumulative distribution function, <span class="math inline">\(\alpha = 1 -\)</span> <code>level</code>, and <code>level</code> is an argument to <code><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict()</a></code>. The default for <code>level</code> is 0.95, which corresponds to a <span class="math inline">\(z^*\)</span> of approximately 1.96.</p>
</div>
<div class="section level3">
<h3 id="interval-confidence">
<code>interval = "confidence"</code><a class="anchor" aria-label="anchor" href="#interval-confidence"></a>
</h3>
<p>The best linear unbiased estimates of <span class="math inline">\(\text{E}[(y_u)_i]\)</span> (<span class="math inline">\(\text{E}(\cdot)\)</span> denotes expectation) are returned by evaluating <span class="math inline">\((\mathbf{X}_u)_i \hat{\boldsymbol{\beta}}\)</span> (i.e., fitted values corresponding to <span class="math inline">\((\mathbf{X}_u)_i)\)</span>. The (100 <span class="math inline">\(\times\)</span> <code>level</code>)% confidence interval for <span class="math inline">\(\text{E}[(y_u)_i]\)</span> is <span class="math inline">\((\mathbf{X}_u)_i \hat{\boldsymbol{\beta}} \pm z^* \sqrt{(\mathbf{X}_u)_i (\mathbf{X}^\top_o \hat{\boldsymbol{\Sigma}}_o^{-1} \mathbf{X}_o)^{-1} (\mathbf{X}_u)_i^\top}\)</span>, where <span class="math inline">\((\mathbf{X}_u)_i\)</span> is the <span class="math inline">\(i\)</span>th row of <span class="math inline">\(\mathbf{X}_u\)</span>, <span class="math inline">\(\sqrt{(\mathbf{X}_u)_i (\mathbf{X}^\top_o \hat{\boldsymbol{\Sigma}}_o^{-1} \mathbf{X}_o)^{-1} (\mathbf{X}_u)_i^\top}\)</span> is the standard error of <span class="math inline">\((\dot{y}_u)_i\)</span> obtained from <code>se.fit = TRUE</code>, <span class="math inline">\(\Phi(z^*) = 1 - \alpha / 2\)</span>, <span class="math inline">\(\Phi(\cdot)\)</span> is the standard normal (Gaussian) cumulative distribution function, <span class="math inline">\(\alpha = 1 -\)</span> <code>level</code>, and <code>level</code> is an argument to <code><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict()</a></code>. The default for <code>level</code> is 0.95, which corresponds to a <span class="math inline">\(z^*\)</span> of approximately 1.96.</p>
</div>
<div class="section level3">
<h3 id="spautor-extra-steps">
<code>spautor()</code> extra steps<a class="anchor" aria-label="anchor" href="#spautor-extra-steps"></a>
</h3>
<p>For spatial autoregressive models, an extra step is required to obtain <span class="math inline">\(\hat{\boldsymbol{\Sigma}}^{-1}_o\)</span>, <span class="math inline">\(\hat{\boldsymbol{\Sigma}}_u\)</span>, and <span class="math inline">\(\hat{\boldsymbol{\Sigma}}_{uo}\)</span> as they depend on one another through the neighborhood structure of <span class="math inline">\(\mathbf{y}_o\)</span> and <span class="math inline">\(\mathbf{y}_u\)</span>. Recall that for autoregressive models, it is <span class="math inline">\(\boldsymbol{\Sigma}^{-1}\)</span> that is straightforward to obtain, not <span class="math inline">\(\boldsymbol{\Sigma}\)</span>.</p>
<p>Let <span class="math inline">\(\boldsymbol{\Sigma}^{-1}\)</span> be the inverse covariance matrix of the observed and unobserved data, <span class="math inline">\(\mathbf{y}_o\)</span> and <span class="math inline">\(\mathbf{y}_u\)</span>. One approach to obtain <span class="math inline">\(\boldsymbol{\Sigma}_o\)</span> and <span class="math inline">\(\boldsymbol{\Sigma}_{uo}\)</span> is to directly invert <span class="math inline">\(\boldsymbol{\Sigma}^{-1}\)</span> and then subset <span class="math inline">\(\boldsymbol{\Sigma}\)</span> appropriately. This inversion can be prohibitive when <span class="math inline">\(n_o + n_u\)</span> is large. A faster way to obtain <span class="math inline">\(\boldsymbol{\Sigma}_o\)</span> and <span class="math inline">\(\boldsymbol{\Sigma}_{uo}\)</span> exists. Represent <span class="math inline">\(\boldsymbol{\Sigma}^{-1}\)</span> blockwise as <span class="math display">\[\begin{equation*}\label{eq:auto_hw}
  \boldsymbol{\Sigma}^{-1} =
  \begin{bmatrix}
    \tilde{\boldsymbol{\Sigma}}_{o} &amp; \tilde{\boldsymbol{\Sigma}}^{\top}_{uo} \\
    \tilde{\boldsymbol{\Sigma}}_{uo} &amp; \tilde{\boldsymbol{\Sigma}}_{u}
  \end{bmatrix},
\end{equation*}\]</span> where the dimensions of the blocks match the relevant dimensions of <span class="math inline">\(\boldsymbol{\Sigma}\)</span>. All of the terms required for prediction can be obtained from this block representation. <span class="citation">Wolf (1978)</span> shows that <span class="math display">\[\begin{equation*}\label{eq:hw_forms}
  \begin{split}
    \boldsymbol{\Sigma}^{-1}_o &amp; = \tilde{\boldsymbol{\Sigma}}_{o} - \tilde{\boldsymbol{\Sigma}}^{ \top}_{uo} (\tilde{\boldsymbol{\Sigma}}_{u})^{-1} \tilde{\boldsymbol{\Sigma}}_{uo} \\
    \boldsymbol{\Sigma}_u &amp; = (\tilde{\boldsymbol{\Sigma}}_{u} - \tilde{\boldsymbol{\Sigma}}_{uo} (\tilde{\boldsymbol{\Sigma}}_{o})^{-1} \tilde{\boldsymbol{\Sigma}}^\top_{uo})^{-1} \\
    \boldsymbol{\Sigma}_{uo} &amp; = - \boldsymbol{\Sigma}_u \tilde{\boldsymbol{\Sigma}}_{uo} \tilde{\boldsymbol{\Sigma}}^{-1}_{o}
  \end{split}
\end{equation*}\]</span> Evaluating these expressions at <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span> yields <span class="math inline">\(\hat{\boldsymbol{\Sigma}}^{-1}_o\)</span>, and <span class="math inline">\(\hat{\boldsymbol{\Sigma}}_u\)</span>, and <span class="math inline">\(\hat{\boldsymbol{\Sigma}}_{uo}\)</span>.</p>
<p>A similar result exists for the log determinant of <span class="math inline">\(\boldsymbol{\Sigma}_o\)</span>, which is not required for prediction but is required for restricted maximum likelihood and maximum likelihood estimation.</p>
</div>
<div class="section level3">
<h3 id="big-data-1">Big Data<a class="anchor" aria-label="anchor" href="#big-data-1"></a>
</h3>
<p>When the number of observations in the fitted model (observed data) are large or there are many locations to predict at or both, it is often necessary to implement computationally efficient big data approximations. Big data approximations are implemented in <code>spmodel</code> using the <code>local</code> argument to <code><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict()</a></code>. When the method in <code>local</code> is <code>"all"</code>, all of the fitted model data are used to make predictions. In this context, computational efficiency is only gained by parallelizing each prediction. The only available method for <code><a href="../reference/spautor.html">spautor()</a></code> fitted models is <code>"all"</code>. This is because the neighborhood structure of <code><a href="../reference/spautor.html">spautor()</a></code> fitted models does not permit the subsetting used by the <code>"covariance"</code> and <code>"distance"</code> methods that we discuss next.</p>
<p>When the <code>local</code> method is <code>"covariance"</code>, <span class="math inline">\(\hat{\boldsymbol{\Sigma}}_{uo}\)</span> is computed between the observation being predicted (<span class="math inline">\(\mathbf{y}_u\)</span>) and the rest of the observed data. This vector is then ordered and a number of observations (specified via the <code>size</code> argument) having the highest covariance with <span class="math inline">\(\mathbf{y}_u\)</span> are subset, yielding <span class="math inline">\(\check{\boldsymbol{\Sigma}}_{uo}\)</span>, which has dimension <span class="math inline">\(1 \times size\)</span>. Then similarly <span class="math inline">\(\hat{\boldsymbol{\Sigma}}_o\)</span>, <span class="math inline">\(\mathbf{y}_o\)</span>, and <span class="math inline">\(\mathbf{X}_u\)</span> are also subset by these <code>size</code> observations, yielding <span class="math inline">\(\check{\boldsymbol{\Sigma}}_{o}\)</span>, <span class="math inline">\(\check{\mathbf{y}}_o\)</span>, and <span class="math inline">\(\check{\mathbf{X}}_u\)</span>, respectively. Equations<span class="math inline">\(~\)</span> and  can be evaluated at <span class="math inline">\(\check{\boldsymbol{\Sigma}}_{uo}\)</span>, <span class="math inline">\(\check{\boldsymbol{\Sigma}}_{o}\)</span>, <span class="math inline">\(\check{\mathbf{y}}_o\)</span>, and <span class="math inline">\(\check{\mathbf{X}}_u\)</span>. When the local method is <code>"distance"</code>, a similar approach is used except a number of observations (specified via the <code>size</code> argument) closest (in terms of Euclidean distance) to <span class="math inline">\(\mathbf{y}_u\)</span> are subset instead. When random effects are not used, partition factors are not used, and the spatial covariance function is monotone decreasing, <code>"covariance"</code> and <code>"distance"</code> are equivalent. This approach of subsetting the observed data by the set of locations closest in covariance or proximity to <span class="math inline">\(\mathbf{y}_u\)</span> is known as the local neighborhood approach. As long as <code>size</code> is relatively small (the default is 50), the local neighborhood approach is very computationally efficient, mainly because <span class="math inline">\(\check{\boldsymbol{\Sigma}}_{o}^{-1}\)</span> is easy to compute. Additional computational efficiency is gained by parallelizing each prediction.</p>
</div>
<div class="section level3">
<h3 id="sec:rf_pred">
<code>splmRF()</code> and <code>spautorRF()</code><a class="anchor" aria-label="anchor" href="#sec:rf_pred"></a>
</h3>
<p>Random forest spatial residual model (Section<span class="math inline">\(~\)</span>) predictions are obtained by combining random forest predictions and spatial linear model predictions (i.e., Kriging) of the random forest residuals. Formally, the random forest spatial residual model predictions of <span class="math inline">\(\mathbf{y}_u\)</span> are given by <span class="math display">\[\begin{equation*}
  \mathbf{\dot{y}}_u = \mathbf{\dot{y}}_{u, rf} + \mathbf{\dot{e}}_{u, slm},
\end{equation*}\]</span> where <span class="math inline">\(\mathbf{\dot{y}}_{u, rf}\)</span> are the random forest predictions for <span class="math inline">\(\mathbf{y}_u\)</span> and <span class="math inline">\(\mathbf{\dot{e}}_{u, slm}\)</span> are the spatial linear model predictions of the random forest residuals for <span class="math inline">\(\mathbf{y}_u\)</span>. This process of obtaining predictions is sometimes analogously called random forest regression Kriging <span class="citation">(Fox, Ver Hoef, and Olsen 2020)</span>.</p>
<p>Uncertainty quantification in a random forest context has been studied <span class="citation">(Meinshausen and Ridgeway 2006)</span> but is not currently available in <code>spmodel</code>. Big data are accommodated by supplying the <code>local</code> argument to <code><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict()</a></code>.</p>
</div>
</div>
<div class="section level2">
<h2 id="sec:pr2">
<code>pseudoR2()</code><a class="anchor" aria-label="anchor" href="#sec:pr2"></a>
</h2>
<p>The pseudo R-squared is a generalization of the classical R-squared from non-spatial linear models. Like the classical R-squared, the pseudo R-squared measures the proportion of variability in the response explained by the fixed effects in the fitted model. Unlike the classical R-squared, the pseudo R-squared can be applied to models whose errors do not satisfy the iid and constant variance assumption. The pseudo R-squared is given by <span class="math display">\[\begin{equation*}
PR2 = 1 - \frac{\mathcal{D}(\boldsymbol{\hat{\Theta}})}{\mathcal{D}(\boldsymbol{\hat{\Theta}}_0)}.
\end{equation*}\]</span> For normal (Gaussian) random errors, the pseudo R-squared is <span class="math display">\[\begin{equation*}
PR2 = 1 - \frac{(\mathbf{y} - \mathbf{X} \hat{\boldsymbol{\beta}})^\top \hat{\boldsymbol{\Sigma}}^{-1}(\mathbf{y} - \mathbf{X} \hat{\boldsymbol{\beta}})}{(\mathbf{y} - \hat{\mu})^\top \hat{\boldsymbol{\Sigma}}^{-1}(\mathbf{y} - \hat{\mu})},
\end{equation*}\]</span> where <span class="math inline">\(\hat{\mu} = (\boldsymbol{1}^\top \hat{\boldsymbol{\Sigma}}^{-1} \boldsymbol{1})^{-1} \boldsymbol{1}^\top \hat{\boldsymbol{\Sigma}}^{-1} \mathbf{y}\)</span>. For the non-spatial model, the pseudo R-squared reduces to the classical R-squared, as <span class="math display">\[\begin{equation*}
PR2 = 1 - \frac{(\mathbf{y} - \mathbf{X} \hat{\boldsymbol{\beta}})^\top \hat{\boldsymbol{\Sigma}}^{-1}(\mathbf{y} - \mathbf{X} \hat{\boldsymbol{\beta}})}{(\mathbf{y} - \hat{\mu})^\top \hat{\boldsymbol{\Sigma}}^{-1}(\mathbf{y} - \hat{\mu})}  = 1 - \frac{(\mathbf{y} - \mathbf{X} \hat{\boldsymbol{\beta}})^\top (\mathbf{y} - \mathbf{X} \hat{\boldsymbol{\beta}})}{(\mathbf{y} - \hat{\mu})^\top (\mathbf{y} - \hat{\mu})} = 1 - \frac{\text{SSE}}{\text{SST}} = R2,
\end{equation*}\]</span> where SSE denotes the error sum of squares and SST denotes the total sum of squares. The result follows because for a non-spatial model, <span class="math inline">\(\boldsymbol{\Sigma}\)</span> is proportional to the identity matrix.</p>
<p>The adjusted pseudo r-squared adjusts for additional explanatory variables and is given by <span class="math display">\[\begin{equation*}
  PR2adj = 1 - (1 - PR2)\frac{n - 1}{n - p}.
\end{equation*}\]</span> If the fitted model does not have an intercept, the <span class="math inline">\(n - 1\)</span> term is instead <span class="math inline">\(n\)</span>.</p>
</div>
<div class="section level2">
<h2 id="sec:residuals">
<code>residuals()</code><a class="anchor" aria-label="anchor" href="#sec:residuals"></a>
</h2>
<p>Terminology regarding residual names if often conflicting and confusing. Because of this, next we explicitly define the residual options in <code>spmodel</code>. These definitions may be different from others you may have seen in the literature.</p>
<p>When <code>type = "raw"</code>, raw residuals are returned: <span class="math display">\[\begin{equation*}
 \mathbf{e}_{r} = \mathbf{y} - \mathbf{X} \hat{\boldsymbol{\beta}}.
\end{equation*}\]</span></p>
<p>When <code>type = "pearson"</code>, Pearson residuals are returned: <span class="math display">\[\begin{equation*}
 \mathbf{e}_{p} = \hat{\boldsymbol{\Sigma}}^{-1/2}\mathbf{e}_{r},
\end{equation*}\]</span> If the errors are normal (Gaussian), the pearson residuals should be approximately normally distributed with mean zero and variance one. The result follows when <span class="math inline">\(\hat{\boldsymbol{\Sigma}}^{-1/2} \approx \boldsymbol{\Sigma}^{-1/2}\)</span> because <span class="math display">\[\begin{equation*}
  \text{E}(\boldsymbol{\Sigma}^{-1/2} \mathbf{e}_{r}) = \boldsymbol{\Sigma}^{-1/2} \text{E}(\mathbf{e}_{r}) = \boldsymbol{\Sigma}^{-1/2} \boldsymbol{0} = \boldsymbol{0}
\end{equation*}\]</span> and <span class="math display">\[\begin{equation*}
  \begin{split}
  \text{Cov}(\boldsymbol{\Sigma}^{-1/2} \mathbf{e}_{r}) &amp; = \boldsymbol{\Sigma}^{-1/2} \text{Cov}(\mathbf{e}_{r}) \boldsymbol{\Sigma}^{-1/2} \\
  &amp; \approx \boldsymbol{\Sigma}^{-1/2} \boldsymbol{\Sigma} \boldsymbol{\Sigma}^{-1/2} \\
  &amp; = (\boldsymbol{\Sigma}^{-1/2} \boldsymbol{\Sigma}^{1/2})(\boldsymbol{\Sigma}^{1/2} \boldsymbol{\Sigma}^{-1/2}) \\
  &amp; = \mathbf{I}
  \end{split}
\end{equation*}\]</span></p>
<p>When <code>type = "standardized"</code>, standardized residuals are returned: <span class="math display">\[\begin{equation*}
 \mathbf{e}_{s} = \frac{\mathbf{e}_{p}}{\sqrt{1 - diag(\mathbf{H}^*)}},
\end{equation*}\]</span> where <span class="math inline">\(diag(\mathbf{H}^*)\)</span> is the diagonal of the spatial hat matrix, <span class="math inline">\(\mathbf{H}_s \equiv \mathbf{X}^* (\mathbf{X}^{* \top} \mathbf{X}^*)^{-1} \mathbf{X}^{* \top}\)</span>. This residual transformation “standardizes” the Pearson residuals. As such, the standardized residuals should also have mean zero and variance <span class="math display">\[\begin{equation*}
  \begin{split}
  \text{Cov}(\mathbf{e}_{s}) &amp; = \text{Cov}((\mathbf{I} - \mathbf{H}^*) \hat{\boldsymbol{\Sigma}}^{-1/2}\mathbf{y}) \\
  &amp; \approx \text{Cov}((\mathbf{I} - \mathbf{H}^*) \boldsymbol{\Sigma}^{-1/2}\mathbf{y}) \\
  &amp; = (\mathbf{I} - \mathbf{H}^*) \boldsymbol{\Sigma}^{-1/2} \text{Cov}(\mathbf{y}) \boldsymbol{\Sigma}^{-1/2}(\mathbf{I} - \mathbf{H}^*)^\top \\
  &amp; = (\mathbf{I} - \mathbf{H}^*) \boldsymbol{\Sigma}^{-1/2} \boldsymbol{\Sigma} \boldsymbol{\Sigma}^{-1/2}(\mathbf{I} - \mathbf{H}^*)^\top \\
  &amp; = (\mathbf{I} - \mathbf{H}^*) \mathbf{I} (\mathbf{I} - \mathbf{H}^*)^\top \\
  &amp; = (\mathbf{I} - \mathbf{H}^*),
  \end{split}
\end{equation*}\]</span> because <span class="math inline">\((\mathbf{I} - \mathbf{H}^*)\)</span> is symmetric and idempotent. Note that the average value of <span class="math inline">\(diag(\mathbf{H}^*)\)</span> is <span class="math inline">\(p / n\)</span>, so <span class="math inline">\((\mathbf{I} - \mathbf{H}^*) \approx \mathbf{I}\)</span> for large sample sizes.</p>
</div>
<div class="section level2">
<h2 id="sec:spmod">
<code>spautor()</code> and <code>splm()</code><a class="anchor" aria-label="anchor" href="#sec:spmod"></a>
</h2>
<p>Next we discuss technical details for the <code><a href="../reference/spautor.html">spautor()</a></code> and <code><a href="../reference/splm.html">splm()</a></code> functions. Many of the details for the two functions are the same, though occasional differences are noted in the following subsection headers. Specifically, <code><a href="../reference/spautor.html">spautor()</a></code> and <code><a href="../reference/splm.html">splm()</a></code> are for different data types and use different covariance functions. <code><a href="../reference/spautor.html">spautor()</a></code> is for spatial linear models with areal data (i.e., spatial autoregressive models) and <code><a href="../reference/splm.html">splm()</a></code> is for spatial linear models with point-referenced data (i.e., geostatistical models). There are also a few features <code><a href="../reference/splm.html">splm()</a></code> has that <code><a href="../reference/spautor.html">spautor()</a></code> does not: semivariogram-based estimation, random effects, anisotropy, and big data approximations.</p>
<div class="section level3">
<h3 id="spautor-spatial-covariance-functions">
<code>spautor()</code> Spatial Covariance Functions<a class="anchor" aria-label="anchor" href="#spautor-spatial-covariance-functions"></a>
</h3>
<p>For areal data, the covariance matrix depends on the specification of a neighborhood structure among the observations. Observations with at least one neighbor (not including itself) are called “connected” observations. Observations with no neighbors are called “unconnected” observations. The autoregressive spatial covariance matrix can be defined as <span class="math display">\[\begin{equation*}
  \boldsymbol{\Sigma} =
  \begin{bmatrix}
    \sigma^2_{de} \mathbf{R} &amp; \mathbf{0} \\
    \mathbf{0} &amp; \sigma^2_{\xi} \mathbf{I}
  \end{bmatrix}
  + \sigma^2_{ie} \mathbf{I},
\end{equation*}\]</span> where <span class="math inline">\(\sigma^2_{de}\)</span> <span class="math inline">\((\geq 0)\)</span> is the spatially dependent (correlated) variance for the connected observations, <span class="math inline">\(\mathbf{R}\)</span> is a matrix that describes the spatial dependence for the connected observations, <span class="math inline">\(\sigma^2_{\xi}\)</span> <span class="math inline">\((\geq 0)\)</span> is the independent (not correlated) variance for the unconnected observations, and <span class="math inline">\(\sigma^2_{ie}\)</span> <span class="math inline">\((\geq 0)\)</span> is the independent (not correlated) variance for all observations. As seen, the connected and unconnected observations are allowed different variances. The total variance for connected observations is then <span class="math inline">\(\sigma^2_{de} + \sigma^2_{ie}\)</span> and the total variance for unconnected observations is <span class="math inline">\(\sigma^2_{\xi} + \sigma^2_{ie}\)</span>. <code>spmodel</code> accommodates two spatial covariances: conditional autoregressive (CAR) and simultaneous autoregressive (SAR), both of which have their <span class="math inline">\(\mathbf{R}\)</span> forms provided in Table<span class="math inline">\(~\)</span>. For both CAR and SAR covariance functions, <span class="math inline">\(\mathbf{R}\)</span> depends on similar quantities: <span class="math inline">\(\mathbf{I}\)</span>, an identity matrix; <span class="math inline">\(\phi\)</span>, a range parameter, and <span class="math inline">\(\mathbf{W}\)</span>, a matrix that defines the neighborhood structure. Often <span class="math inline">\(\mathbf{W}\)</span> is symmetric but it need not be. Valid values for <span class="math inline">\(\phi\)</span> are in <span class="math inline">\((1 / \lambda_{max}, 1 / \lambda_{min})\)</span>, where <span class="math inline">\(\lambda_{min}\)</span> is the minimum eigenvalue of <span class="math inline">\(\mathbf{W}\)</span> and <span class="math inline">\(\lambda_{max}\)</span> is the maximum eigenvalue of <span class="math inline">\(\mathbf{W}\)</span>. For SAR covariance functions, <span class="math inline">\(\lambda_{min}\)</span> must be negative and <span class="math inline">\(\lambda_{max}\)</span> must be positive. For CAR covariances functions, a matrix <span class="math inline">\(\mathbf{M}\)</span> matrix must be provided that satisfies the CAR symmetry condition, which enforces the symmetry of the covariance matrix. The CAR symmetry condition states <span class="math display">\[\begin{equation*}
  \frac{\mathbf{W}_{ij}}{\mathbf{M}_{ii}} = \frac{\mathbf{W}_{ji}}{\mathbf{M}_{jj}}
\end{equation*}\]</span> for all <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>, where <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> index rows or columns. When <span class="math inline">\(\mathbf{W}\)</span> is symmetric, <span class="math inline">\(\mathbf{M}\)</span> is often taken to be the identity matrix.</p>
<p>The default in <code>spmodel</code> is to row-standardize <span class="math inline">\(\mathbf{W}\)</span> by dividing each element by its respective row sum, which decreases variance. If row-standardization is not used for a CAR model, the default in <code>spmodel</code> for <span class="math inline">\(\mathbf{M}\)</span> is the identity matrix.</p>

</div>
<div class="section level3">
<h3 id="splm-spatial-covariance-functions">
<code>splm()</code> Spatial Covariance Functions<a class="anchor" aria-label="anchor" href="#splm-spatial-covariance-functions"></a>
</h3>
For point-referenced data, the spatial covariance is given by <span class="math display">\[\begin{equation*}
\sigma^2_{de}\mathbf{R} + \sigma^2_{ie} \mathbf{I},
\end{equation*}\]</span> where <span class="math inline">\(\sigma^2_{de}\)</span> <span class="math inline">\((\geq 0)\)</span> is the spatially dependent (correlated) variance, <span class="math inline">\(\mathbf{R}\)</span> is a spatial correlation matrix, <span class="math inline">\(\sigma^2_{ie}\)</span> <span class="math inline">\((\geq 0)\)</span> is the spatially independent (not correlated) variance, and <span class="math inline">\(\mathbf{I}\)</span> is an identity matrix. The <span class="math inline">\(\mathbf{R}\)</span> matrix always depends on a range parameter, <span class="math inline">\(\phi\)</span> <span class="math inline">\((&gt; 0)\)</span>, that controls the behavior of the covariance function with distance. For some covariance functions, the <span class="math inline">\(\mathbf{R}\)</span> matrix depends on an additional parameter that we call the “extra” parameter. Table<span class="math inline">\(~\)</span> shows the parametric form for all <span class="math inline">\(\mathbf{R}\)</span> matrices available in <code><a href="../reference/splm.html">splm()</a></code>. In Table<span class="math inline">\(~\)</span>, the range parameter is denoted as <span class="math inline">\(\phi\)</span>, the distance divided by the range parameter (<span class="math inline">\(h / \phi\)</span>) is denoted as <span class="math inline">\(\eta\)</span>, <span class="math inline">\(\mathbbm{1}\{\cdot\}\)</span> is an indicator function equal to one when the argument occurs and zero otherwise, and the extra parameter is denoted as <span class="math inline">\(\xi\)</span> (when relevant).

</div>
<div class="section level3">
<h3 id="subsec:estimation">Model-fitting<a class="anchor" aria-label="anchor" href="#subsec:estimation"></a>
</h3>
<div class="section level4">
<h4 id="likelihood-based-estimation-estmethod-reml-or-estmethod-ml">Likelihood-based Estimation (<code>estmethod = "reml"</code> or <code>estmethod = "ml"</code>)<a class="anchor" aria-label="anchor" href="#likelihood-based-estimation-estmethod-reml-or-estmethod-ml"></a>
</h4>
<p>Minus twice a profiled (by <span class="math inline">\(\boldsymbol{\beta}\)</span>) Gaussian log-likelihood is given by <span class="math display">\[\begin{equation}\label{eq:ml-lik}
  -2\ell_p(\boldsymbol{\theta}) = \ln{|\boldsymbol{\Sigma}|} + (\mathbf{y} - \mathbf{X} \tilde{\boldsymbol{\beta}})^\top \boldsymbol{\Sigma}^{-1} (\mathbf{y} - \mathbf{X} \tilde{\boldsymbol{\beta}}) + n \ln{2\pi},
\end{equation}\]</span> where <span class="math inline">\(\tilde{\boldsymbol{\beta}} = (\mathbf{X}^\top \mathbf{\Sigma}^{-1} \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{\Sigma}^{-1} \mathbf{y}\)</span>. Minimizing Equation<span class="math inline">\(~\)</span> yields <span class="math inline">\(\boldsymbol{\hat{\theta}}_{ml}\)</span>, the maximum likelihood estimates for <span class="math inline">\(\boldsymbol{\theta}\)</span>. Then a closed form solution exists for <span class="math inline">\(\boldsymbol{\hat{\beta}}_{ml}\)</span>, the maximum likelihood estimates for <span class="math inline">\(\boldsymbol{\beta}\)</span>: <span class="math inline">\(\boldsymbol{\hat{\beta}}_{ml} = \tilde{\boldsymbol{\beta}}_{ml}\)</span>, where <span class="math inline">\(\tilde{\boldsymbol{\beta}}_{ml}\)</span> is <span class="math inline">\(\tilde{\boldsymbol{\beta}}\)</span> evaluated at <span class="math inline">\(\boldsymbol{\hat{\theta}}_{ml}\)</span>. Unfortunately <span class="math inline">\(\boldsymbol{\hat{\theta}}_{ml}\)</span> can be badly biased for <span class="math inline">\(\boldsymbol{\theta}\)</span> (especially for small sample sizes), which impacts the estimation of <span class="math inline">\(\boldsymbol{\beta}\)</span> <span class="citation">(Patterson and Thompson 1971)</span>. This bias occurs due to the simultaneous estimation of <span class="math inline">\(\boldsymbol{\beta}\)</span> and <span class="math inline">\(\boldsymbol{\theta}\)</span>. To reduce this bias, restricted maximum likelihood estimation (REML) emerged <span class="citation">(Patterson and Thompson 1971; Harville 1977; Wolfinger, Tobias, and Sall 1994)</span>. Integrating <span class="math inline">\(\boldsymbol{\beta}\)</span> out of a Gaussian likelihood yields the restricted Gaussian likelihood. Minus twice a restricted Gaussian log-likelihood is given by <span class="math display">\[\begin{equation}\label{eq:reml-lik}
  -2\ell_R(\boldsymbol{\theta}) = -2\ell_p(\boldsymbol{\theta})  + \ln{|\mathbf{X}^\top \boldsymbol{\Sigma}^{-1} \mathbf{X}|} - p \ln{2\pi} ,
\end{equation}\]</span> where <span class="math inline">\(p\)</span> equals the dimension of <span class="math inline">\(\boldsymbol{\beta}\)</span>. Minimizing Equation<span class="math inline">\(~\)</span> yields <span class="math inline">\(\boldsymbol{\hat{\theta}}_{reml}\)</span>, the restricted maximum likelihood estimates for <span class="math inline">\(\boldsymbol{\theta}\)</span>. Then a closed for solution exists for <span class="math inline">\(\boldsymbol{\hat{\beta}}_{reml}\)</span>, the restricted maximum likelihood estimates for <span class="math inline">\(\boldsymbol{\beta}\)</span>: <span class="math inline">\(\boldsymbol{\hat{\beta}}_{reml} = \tilde{\boldsymbol{\beta}}_{reml}\)</span>, where <span class="math inline">\(\tilde{\boldsymbol{\beta}}_{reml}\)</span> is <span class="math inline">\(\tilde{\boldsymbol{\beta}}\)</span> evaluated at <span class="math inline">\(\boldsymbol{\hat{\theta}}_{reml}\)</span>.</p>
<p>The covariance matrix can often be written as <span class="math inline">\(\boldsymbol{\Sigma} = \sigma^2 \boldsymbol{\Sigma}^*\)</span>, where <span class="math inline">\(\sigma^2\)</span> is the overall variance and <span class="math inline">\(\boldsymbol{\Sigma}^*\)</span> is a covariance matrix that depends on parameter vector <span class="math inline">\(\boldsymbol{\theta}^*\)</span> with one less dimension than <span class="math inline">\(\boldsymbol{\theta}\)</span>. Then the overall variance, <span class="math inline">\(\sigma^2\)</span>, can be profiled out of Equation<span class="math inline">\(~\)</span> and Equation<span class="math inline">\(~\)</span>. This reduces the number of parameters requiring optimization by one, which can dramatically reduce estimation time. Profiling <span class="math inline">\(\sigma^2\)</span> out of Equation<span class="math inline">\(~\)</span> yields <span class="math display">\[\begin{equation*}\label{eq:ml-plik}
  -2\ell_p^*(\boldsymbol{\theta}^*) = \ln{|\boldsymbol{\Sigma^*}|} + n\ln[(\mathbf{y} - \mathbf{X} \tilde{\boldsymbol{\beta}})^\top \boldsymbol{\Sigma}^{* -1} (\mathbf{y} - \mathbf{X} \tilde{\boldsymbol{\beta}})] + n + n\ln{2\pi / n}.
\end{equation*}\]</span> After finding <span class="math inline">\(\hat{\boldsymbol{\theta}}^*_{ml}\)</span>, a closed form solution for <span class="math inline">\(\hat{\sigma}^2_{ml}\)</span> exists: <span class="math inline">\(\hat{\sigma}^2_{ml} = [(\mathbf{y} - \mathbf{X} \boldsymbol{\tilde{\beta}})^\top \mathbf{\Sigma}^{* -1} (\mathbf{y} - \mathbf{X} \tilde{\boldsymbol{\beta}})] / n\)</span>. Then <span class="math inline">\(\boldsymbol{\hat{\theta}}^*_{ml}\)</span> is combined with <span class="math inline">\(\hat{\sigma}^2_{ml}\)</span> to yield <span class="math inline">\(\boldsymbol{\hat{\theta}}_{ml}\)</span> and subsequently <span class="math inline">\(\boldsymbol{\hat{\beta}}_{ml}\)</span>. A similar result holds for restricted maximum likelihood estimation. Profiling <span class="math inline">\(\sigma^2\)</span> out of Equation<span class="math inline">\(~\)</span> yields <span class="math display">\[\begin{equation*}\label{eq:reml-plik}
  -2\ell_R^*(\boldsymbol{\Theta}) = \ln{|\boldsymbol{\Sigma}^*|} + (n - p)\ln[(\mathbf{y} - \mathbf{X} \tilde{\boldsymbol{\beta}})^\top \boldsymbol{\Sigma}^{* -1} (\mathbf{y} - \mathbf{X} \tilde{\boldsymbol{\beta}})] + \ln{|\mathbf{X}^\top \boldsymbol{\Sigma}^{* -1} \mathbf{X}|} + (n - p) + (n - p)\ln2\pi / (n - p).
\end{equation*}\]</span> After finding <span class="math inline">\(\hat{\boldsymbol{\theta}}^*_{reml}\)</span>, a closed form solution for <span class="math inline">\(\hat{\sigma}^2_{reml}\)</span> exists: <span class="math inline">\(\hat{\sigma}^2_{reml} = [(\mathbf{y} - \mathbf{X} \boldsymbol{\tilde{\beta}})^\top \mathbf{\Sigma}^{* -1} (\mathbf{y} - \mathbf{X} \tilde{\boldsymbol{\beta}})] / (n - p)\)</span>. Then <span class="math inline">\(\boldsymbol{\hat{\theta}}^*_{reml}\)</span> is combined with <span class="math inline">\(\hat{\sigma}^2_{reml}\)</span> to yield <span class="math inline">\(\boldsymbol{\hat{\theta}}_{reml}\)</span> and subsequently <span class="math inline">\(\boldsymbol{\hat{\beta}}_{reml}\)</span>. For more on profiling Gaussian likelihoods, see <span class="citation">Wolfinger, Tobias, and Sall (1994)</span>.</p>
<p>Both maximum likelihood and restricted maximum likelihood estimation rely on the <span class="math inline">\(n \times n\)</span> covariance matrix inverse. Inverting an <span class="math inline">\(n \times n\)</span> matrix is an enormous computational demand that scales cubically with the sample size. For this reason, maximum likelihood and restricted maximum likelihood estimation have historically been infeasible to implement in their standard form with data larger than a few thousand observations. This motivates the use for the big data approaches outlined in Section<span class="math inline">\(~\)</span>.</p>
</div>
<div class="section level4">
<h4 id="semivariogram-based-estimation-splm-only">Semivariogram-based Estimation (<code>splm()</code> only)<a class="anchor" aria-label="anchor" href="#semivariogram-based-estimation-splm-only"></a>
</h4>
<p>An alternative approach to likelihood-based estimation is semivariogram-based estimation. The semivariogram of a constant-mean process <span class="math inline">\(\mathbf{y}\)</span> is the expectation of half of the squared difference between two observations <span class="math inline">\(h\)</span> distance units apart. More formally, the semivariogram is denoted <span class="math inline">\(\gamma(h)\)</span> and defined as <span class="math display">\[\begin{equation*}\label{eq:sv}
  \gamma(h) = \text{E}[(y_i - y_j)^2] / 2 ,
\end{equation*}\]</span> where <span class="math inline">\(h\)</span> is the Euclidean distance between the locations of <span class="math inline">\(y_i\)</span> and <span class="math inline">\(y_j\)</span>. When the process <span class="math inline">\(\mathbf{y}\)</span> is second-order stationary, the semivariogram and covariance function are intimately connected: <span class="math inline">\(\gamma(h) = \sigma^2 - \text{Cov}(h)\)</span>, where <span class="math inline">\(\sigma^2\)</span> is the overall variance and <span class="math inline">\(\text{Cov}(h)\)</span> is the covariance function evaluated at <span class="math inline">\(h\)</span>. As such, the semivariogram and covariance function rely on the same parameter vector <span class="math inline">\(\boldsymbol{\theta}\)</span>. Both of the semivariogram approaches described next are more computationally efficient than restricted maximum likelihood and maximum likelihood estimation because the major computational burden of the semivariogram approaches (calculations based on squared differences among pairs) scales quadratically with the sample size (i.e., not the cubed sample size like the likelihood-based approaches).</p>
<div class="section level5">
<h5 id="weighted-least-squares-estmethod-sv-wls">Weighted Least Squares (<code>estmethod = "sv-wls"</code>)<a class="anchor" aria-label="anchor" href="#weighted-least-squares-estmethod-sv-wls"></a>
</h5>
<p>The empirical semivariogram is a moment-based estimate of the semivariogram denoted by <span class="math inline">\(\hat{\gamma}(h)\)</span>. Recall it is defined in Equation<span class="math inline">\(~\)</span> as <span class="math display">\[\begin{equation*}
  \hat{\gamma}(h) = \frac{1}{2|N(h)|} \sum_{N(h)} (y_i - y_j)^2, 
\end{equation*}\]</span> where <span class="math inline">\(N(h)\)</span> is the set of observations in <span class="math inline">\(\mathbf{y}\)</span> that are <span class="math inline">\(h\)</span> distance units apart (distance classes) and <span class="math inline">\(|N(h)|\)</span> is the cardinality of <span class="math inline">\(N(h)\)</span> <span class="citation">(Cressie 1993)</span>. More computational details are provided in Section<span class="math inline">\(~\)</span>. One criticism of the empirical semivariogram is that distance bins and cutoffs tend to be arbitrarily chosen (i.e., not chosen according to some statistical criteria).</p>
<p><span class="citation">Cressie (1985)</span> proposed estimating <span class="math inline">\(\boldsymbol{\theta}\)</span> by minimizing an objective function that involves <span class="math inline">\(\gamma(h)\)</span> and <span class="math inline">\(\hat{\gamma}(h)\)</span> and is based on a weighted least squares criterion. This criterion is defined as <span class="math display">\[\begin{equation}\label{eq:svwls}
  \sum_i w_i [\hat{\gamma}(h)_i - \gamma(h)_i]^2,
\end{equation}\]</span> where <span class="math inline">\(w_i\)</span>, <span class="math inline">\(\hat{\gamma}(h)_i\)</span>, and <span class="math inline">\(\gamma(h)_i\)</span> are the weights, empirical semivariogram, and semivariogram for the <span class="math inline">\(i\)</span>th distance class, respectively. Minimizing Equation<span class="math inline">\(~\)</span> yields <span class="math inline">\(\boldsymbol{\hat{\theta}}_{wls}\)</span>, the semivariogram weighted least squares estimate of <span class="math inline">\(\boldsymbol{\theta}\)</span>. After estimating <span class="math inline">\(\boldsymbol{\theta}\)</span>, <span class="math inline">\(\boldsymbol{\beta}\)</span> estimates are constructed using (empirical) generalized least squares: <span class="math inline">\(\boldsymbol{\hat{\beta}}_{wls} = (\mathbf{X}^\top \hat{\mathbf{\Sigma}}^{-1} \mathbf{X})^{-1} \mathbf{X}^\top \hat{\mathbf{\Sigma}}^{-1} \mathbf{y}\)</span>.</p>
<span class="citation">Cressie (1985)</span> recommends setting the <span class="math inline">\(w_i\)</span> in Equation<span class="math inline">\(~\)</span> as <span class="math inline">\(w_i = |N(h)| / \gamma(h)_i^2\)</span>, which gives more weight to distance classes with more observations (<span class="math inline">\(|N(h)|\)</span>) and shorter distances (<span class="math inline">\(1 / \gamma(h)_i^2\)</span>). The default in <code>spmodel</code> is to use these <span class="math inline">\(w_i\)</span>, known as Cressie weights, though several other options for <span class="math inline">\(w_i\)</span> exist and are available via the <code>weights</code> argument. Table<span class="math inline">\(~\)</span> contains all <span class="math inline">\(w_i\)</span> available via the <code>weights</code> argument.

<p>The number of <span class="math inline">\(N(h)\)</span> classes and the maximum distance for <span class="math inline">\(h\)</span> are specified by passing the <code>bins</code> and <code>cutoff</code> arguments to <code><a href="../reference/splm.html">splm()</a></code> (these arguments are passed via <code>...</code> to <code><a href="../reference/esv.html">esv()</a></code>). The default value for <code>bins</code> is 15 and the default value for <code>cutoff</code> is half the maximum distance of the spatial domain’s bounding box.</p>
<p>Recall that the semivariogram is defined for a constant-mean process. Generally, <span class="math inline">\(\mathbf{y}\)</span> does not necessarily have a constant mean so the empirical semivariogram and <span class="math inline">\(\boldsymbol{\hat{\theta}}_{wls}\)</span> are typically constructed using the residuals from an ordinary least squares regression of <span class="math inline">\(\mathbf{y}\)</span> on <span class="math inline">\(\mathbf{X}\)</span>. These ordinary least squares residuals are assumed to have mean zero.</p>
</div>
<div class="section level5">
<h5 id="composite-likelihood-estmethod-sv-cl">Composite Likelihood (<code>estmethod = "sv-cl"</code>)<a class="anchor" aria-label="anchor" href="#composite-likelihood-estmethod-sv-cl"></a>
</h5>
<p>Composite likelihood approaches involve constructing likelihoods based on conditional or marginal events for which likelihoods are available and then adding together these individual components. Composite likelihoods are attractive because they behave very similar to likelihoods but are easier to handle, both from a theoretical and from a computational perspective. <span class="citation">Curriero and Lele (1999)</span> derive a particular composite likelihood for estimating semivariogram parameters. The negative log of this composite likelihood, denoted <span class="math inline">\(\text{CL}(h)\)</span>, is given by <span class="math display">\[\begin{equation}\label{eq:svcl}
  \text{CL}(h) = \sum_{i = 1}^{n - 1} \sum_{j &gt; i} \left( \frac{(y_i - y_j)^2}{2\gamma(h)} + \ln(\gamma(h)) \right)
\end{equation}\]</span> where <span class="math inline">\(\gamma(h)\)</span> is the semivariogram. Minimizing Equation<span class="math inline">\(~\)</span> yields <span class="math inline">\(\boldsymbol{\hat{\theta}}_{cl}\)</span>, the semivariogram composite likelihood estimates of <span class="math inline">\(\boldsymbol{\theta}\)</span>. After estimating <span class="math inline">\(\boldsymbol{\theta}\)</span>, <span class="math inline">\(\boldsymbol{\beta}\)</span> estimates are constructed using (empirical) generalized least squares: <span class="math inline">\(\boldsymbol{\hat{\beta}}_{cl} = (\mathbf{X}^\top \hat{\mathbf{\Sigma}}^{-1} \mathbf{X})^{-1} \mathbf{X}^\top \hat{\mathbf{\Sigma}}^{-1} \mathbf{y}\)</span>.</p>
<p>An advantage of the composite likelihood approach to semivariogram estimation is that it does not require arbitrarily specifying empirical semivariogram bins and cutoffs. It does tend to be more computationally demanding than weighted least squares, however. The composite likelihood is constructed from <span class="math inline">\(\binom{n}{2}\)</span> pairs for a sample size <span class="math inline">\(n\)</span>, whereas the weighted least squares approach only requires calculating <span class="math inline">\(\binom{|N(h)|}{2}\)</span> pairs for each distance bin <span class="math inline">\(N(h)\)</span>. As with the weighted least squares approach, Equation<span class="math inline">\(~\)</span> requires a constant-mean process, so typically the residuals from an ordinary least squares regression of <span class="math inline">\(\mathbf{y}\)</span> on <span class="math inline">\(\mathbf{X}\)</span> are used to estimate <span class="math inline">\(\boldsymbol{\theta}\)</span>.</p>
</div>
</div>
</div>
<div class="section level3">
<h3 id="optimization">Optimization<a class="anchor" aria-label="anchor" href="#optimization"></a>
</h3>
<p>Parameter estimation is performed using <code><a href="https://rdrr.io/r/stats/optim.html" class="external-link">stats::optim()</a></code>. The default estimation method is Nelder-Mead <span class="citation">(Nelder and Mead 1965)</span> and the stopping criterion is a relative convergence tolerance (<code>reltol</code>) of .0001. If only one parameter requires estimation (on the profiled scale if relevant), the Brent algorithm is instead used <span class="citation">(Brent 1971)</span>. Arguments to <code><a href="https://rdrr.io/r/stats/optim.html" class="external-link">optim()</a></code> are passed via <code>...</code> to <code><a href="../reference/splm.html">splm()</a></code> and <code><a href="../reference/spautor.html">spautor()</a></code>. For example, the default estimation method and convergence criteria are overridden by passing <code>method</code> and <code>control</code>, respectively, to <code><a href="../reference/splm.html">splm()</a></code> and <code><a href="../reference/spautor.html">spautor()</a></code>. If the <code>lower</code> and <code>upper</code> arguments to <code><a href="https://rdrr.io/r/stats/optim.html" class="external-link">optim()</a></code> are specified in <code><a href="../reference/splm.html">splm()</a></code> and <code><a href="../reference/spautor.html">spautor()</a></code> to be passed to <code><a href="https://rdrr.io/r/stats/optim.html" class="external-link">optim()</a></code>, they are ignored, as optimization for all parameters is generally unconstrained. Initial values for <code><a href="https://rdrr.io/r/stats/optim.html" class="external-link">optim()</a></code> are found using the grid search described next.</p>
<div class="section level4">
<h4 id="grid-search">Grid Search<a class="anchor" aria-label="anchor" href="#grid-search"></a>
</h4>
<p><code>spmodel</code> uses a grid search to find suitable initial values for use in optimization. For spatial linear models without random effects, the spatially dependent variance (<span class="math inline">\(\sigma^2_{de}\)</span>) and spatially independent variance (<span class="math inline">\(\sigma^2_{ie}\)</span>) parameters are given “low”, “medium”, and “high” values. The sample variance of a non-spatial linear model is slightly inflated by a factor of 1.2 (non-spatial models can underestimate the variance when there is spatial dependence) and these “low”, “medium”, and “high” values correspond to 10%, 50%, and 90% of the inflated sample variance. Only combinations of <span class="math inline">\(\sigma^2_{de}\)</span> and <span class="math inline">\(\sigma^2_{ie}\)</span> whose proportions sum to 100% are considered. The range (<span class="math inline">\(\phi\)</span>) and extra (<span class="math inline">\(\xi\)</span>) parameters are given “low” and “high” values that are unique to each spatial covariance function. The anisotropy (Section<span class="math inline">\(~\)</span>) rotation parameter (<span class="math inline">\(\alpha\)</span>) is given six values that correspond to 0, <span class="math inline">\(\pi/6\)</span>, <span class="math inline">\(2\pi/6\)</span>, <span class="math inline">\(4\pi/6\)</span>, <span class="math inline">\(5\pi/6\)</span>, and <span class="math inline">\(\pi\)</span> radians. The anisotropy scale parameter (<span class="math inline">\(S\)</span>) is given “low”, “medium”, and “high” values that correspond to scaling factors of 0.25, 0.75, and 1. Note that the anisotropy parameters are only used during grid searches for point-referenced data.</p>
The crossing of all appropriate parameter values is considered. If initial values are used for a parameter, the initial value replaces all values of the parameter in this crossing. Duplicate crossings are then omitted. The parameter configuration that yields the smallest value of the objective function is then used as an initial value for optimization. Suppose the inflated sample variance is 10 and the exponential covariance is used assuming isotropy. The parameter configurations evaluated are shown in Table<span class="math inline">\(~\)</span>.

<p>For spatial linear models with random effects, the same approach is used to create a crossing of spatial covariance parameters. A separate approach is used to create a set of random effect variances. The random effect variances are similarly first grouped by proportions. The first combination is such that the first random effect variance is given 90% of variance, and the remaining 10% is spread out evenly among the remaining random effect variances. The second combination is such that the second random effect variance is given 90% of the variance, and the remaining 10% is spread out evenly among the remaining random effect variances. And so on and so forth. These combinations ascertain whether one random effect dominates variability. A final grouping is lastly considered: all 100% of variance is spread out evenly among all random effects.</p>
<p>When finding parameter values <span class="math inline">\(\sigma^2_{de}\)</span>, <span class="math inline">\(\sigma^2_{ie}\)</span>, and the random effect variances (<span class="math inline">\(\sigma^2_{u_i}\)</span> for the <span class="math inline">\(i\)</span>th random effect), three scenarios are considered. In the first scenario, <span class="math inline">\(\sigma^2_{de}\)</span> and <span class="math inline">\(\sigma^2_{ie}\)</span> get 90% of the inflated sample variance and the random effect variances get 10%. In this scenario, only the random effect grouping where the variance is evenly spread out is considered. This is because the random effect variances are already contributing little to the overall variability, so performing additional objective function evaluations is unnecessary. In the second scenario, the random effects get 90% of the inflated sample variances and <span class="math inline">\(\sigma^2_{de}\)</span> and <span class="math inline">\(\sigma^2_{ie}\)</span> get 10%. Similarly in this scenario, only the <span class="math inline">\(\sigma^2_{de}\)</span> and <span class="math inline">\(\sigma^2_{ie}\)</span> grouping where the variance is evenly spread out is considered. Also in this scenario, only the lowest value for <code>range</code> and <code>extra</code> are used. In the third scenario, the 50% of the inflated sample variance is given to <span class="math inline">\(\sigma^2_{de}\)</span> and <span class="math inline">\(\sigma^2_{ie}\)</span> and 50% to the random effects. In this scenario, the only parameter combination considered is the case where variances are evenly spread out among <span class="math inline">\(\sigma^2_{de}\)</span>, <span class="math inline">\(\sigma^2_{ie}\)</span>, and the random effect variances. Together, there are parameter configurations where the spatial variability dominates (scenario 1), the random variability dominates (scenario 2), and where there is an even contribution from spatial and random variability. The parameter configuration that minimizes the objective function is then used as an initial value for optimization. Recall that random effects are only used with restricted maximum likelihood or maximum likelihood estimation, so the objective function is always a likelihood.</p>
Suppose the inflated sample variance is 10, the exponential covariance is used assuming isotropy, and there are two random effects. The parameter configurations evaluated are shown in Table<span class="math inline">\(~\)</span>.

<p>This grid search approach balances a thorough exploration of the parameter space with computational efficiency, as each objective function evaluation can be computationally expensive.</p>
</div>
</div>
<div class="section level3">
<h3 id="hypothesis-testing">Hypothesis Testing<a class="anchor" aria-label="anchor" href="#hypothesis-testing"></a>
</h3>
<p>The hypothesis tests for <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> returned by <code><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary()</a></code> or <code><a href="https://generics.r-lib.org/reference/tidy.html" class="external-link">tidy()</a></code> of an <code>splm</code> or <code>spautor</code> object are asymptotic z-tests based on the normal (Gaussian) distribution (Wald tests). The null hypothesis for the test associated with each <span class="math inline">\(\hat{\beta}_i\)</span> is that <span class="math inline">\(\beta_i = 0\)</span>. Then the test statistic is given by <span class="math display">\[\begin{equation*}
  \tilde{z} = \frac{\hat{\beta}_i}{\text{SE}(\hat{\beta}_i)},
\end{equation*}\]</span> where <span class="math inline">\(\text{SE}(\hat{\beta}_i)\)</span> is the standard error of <span class="math inline">\(\hat{\beta}_i\)</span>, which equals the square root of the <span class="math inline">\(i\)</span>th diagonal element of <span class="math inline">\((\mathbf{X}^\top \hat{\boldsymbol{\Sigma}}^{-1} \mathbf{X})^{-1}\)</span>. The p-value is given by <span class="math inline">\(2 * (1 - \Phi(|\tilde{z}|))\)</span>, which corresponds to an equal-tailed, two-sided hypothesis test of level <span class="math inline">\(\alpha\)</span> where <span class="math inline">\(\Phi(\cdot)\)</span> denotes the standard normal (Gaussian) cumulative distribution function and <span class="math inline">\(|\cdot|\)</span> denotes the absolute value.</p>
</div>
<div class="section level3">
<h3 id="random-effects-splm-only-and-reml-or-ml-estmethod-only">Random Effects (<code>splm()</code> only and <code>"reml"</code> or <code>"ml"</code> <code>estmethod</code> only)<a class="anchor" aria-label="anchor" href="#random-effects-splm-only-and-reml-or-ml-estmethod-only"></a>
</h3>
<p>The random effects contribute directly to the covariance through their design matrices. Let <span class="math inline">\(\mathbf{u}\)</span> be a mean-zero random effect column vector of length <span class="math inline">\(n_u\)</span>, where <span class="math inline">\(n_u\)</span> is the number of levels of the random effect, with design matrix <span class="math inline">\(\mathbf{Z}_u\)</span>. Then <span class="math inline">\(\text{Cov}(\mathbf{Z}_u\mathbf{u}) = \mathbf{Z}_u \text{Cov}(\mathbf{u})\mathbf{Z}_u^\top\)</span>. Because each element of <span class="math inline">\(\mathbf{u}\)</span> is independent of one another, this reduces to <span class="math inline">\(\text{Cov}(\mathbf{Z}_u\mathbf{u}) = \sigma^2_u \mathbf{Z}_u \mathbf{Z}_u^\top\)</span>, where <span class="math inline">\(\sigma^2_u\)</span> is the variance parameter corresponding to the random effect (i.e., the random effect variance parameter).</p>
<p>The <span class="math inline">\(\mathbf{Z}\)</span> matrices index the levels of the random effect. <span class="math inline">\(\mathbf{Z}\)</span> has dimension <span class="math inline">\(n \times n_u\)</span>, where <span class="math inline">\(n\)</span> is the sample size. Each row of <span class="math inline">\(\mathbf{Z}\)</span> corresponds to an observation and each column to a level of the random effect. For example, suppose we have <span class="math inline">\(n = 4\)</span> observations, so <span class="math inline">\(\mathbf{y} = \{y_1, y_2, y_3, y_4\}\)</span>. Also suppose that the random effect <span class="math inline">\(\mathbf{u}\)</span> has two levels and that <span class="math inline">\(y_1\)</span> and <span class="math inline">\(y_4\)</span> are in the first level and <span class="math inline">\(y_2\)</span> and <span class="math inline">\(y_3\)</span> are in the second level. For random intercepts, each element of <span class="math inline">\(\mathbf{Z}\)</span> is one if the observation is in the appropriate level of the random effect and zero otherwise. So it follows that <span class="math display">\[\begin{equation*}
\mathbf{Z}\mathbf{u} = 
\begin{bmatrix}
1 &amp; 0 \\
0 &amp; 1 \\
0 &amp; 1 \\
1 &amp; 0
\end{bmatrix}
\begin{bmatrix}
u_1 \\
u_2
\end{bmatrix},
\end{equation*}\]</span> where <span class="math inline">\(u_1\)</span> and <span class="math inline">\(u_2\)</span> are the random intercepts for the first and second levels of <span class="math inline">\(\mathbf{u}\)</span>, respectively. For random slopes, each element of <span class="math inline">\(\mathbf{Z}\)</span> equals the value of an auxiliary variable, <span class="math inline">\(\mathbf{k}\)</span>, if the observation is in the appropriate level of the random effect and zero otherwise. So if <span class="math inline">\(\mathbf{k} = \{2, 7, 5, 4 \}\)</span> it follows that <span class="math display">\[\begin{equation*}
\mathbf{Z}\mathbf{u} = 
\begin{bmatrix}
2 &amp; 0 \\
0 &amp; 7 \\
0 &amp; 5 \\
4 &amp; 0
\end{bmatrix}
\begin{bmatrix}
u_1 \\
u_2
\end{bmatrix},
\end{equation*}\]</span> where <span class="math inline">\(u_1\)</span> and <span class="math inline">\(u_2\)</span> are the random slopes for the first and second levels of <span class="math inline">\(\mathbf{u}\)</span>, respectively. If a random slope is included in the model, it is common for the auxiliary variable to be a column in <span class="math inline">\(\mathbf{X}\)</span>, the fixed effects design matrix (i.e., also a fixed effect). Denote this column as <span class="math inline">\(\mathbf{x}\)</span>. Here <span class="math inline">\(\boldsymbol{\beta}\)</span> captures the average effect of <span class="math inline">\(\mathbf{x}\)</span> on <span class="math inline">\(\mathbf{y}\)</span> (accounting for other explanatory variables) and <span class="math inline">\(\mathbf{u}\)</span> captures a subject-specific effect of <span class="math inline">\(\mathbf{x}\)</span> on <span class="math inline">\(\mathbf{y}\)</span>. So for a subject in the <span class="math inline">\(i\)</span>th level of <span class="math inline">\(\mathbf{u}\)</span>, the average increase in <span class="math inline">\(y\)</span> associated with a one-unit increase <span class="math inline">\(x\)</span> is <span class="math inline">\(\beta + u_i\)</span>.</p>
<p>The <code>sv-wls</code> and <code>sv-cl</code> estimation methods do not use a likelihood, and thus, they do not allow for the estimation of random effects in <code>spmodel</code>.</p>
</div>
<div class="section level3">
<h3 id="sec:anisotropy">Anisotropy (<code>splm()</code> only)<a class="anchor" aria-label="anchor" href="#sec:anisotropy"></a>
</h3>
An isotropic spatial covariance function behaves similarly in all directions (i.e., is independent of direction) as a function of distance. An anisotropic spatial covariance function does not behave similarly in all directions as a function of distance.
<div class="figure">
<img src="technical_files/figure-html/anisotropy-1.png" alt="In the left figure, the ellipse of an isotropic spatial covariance function centered at the origin is shown. In the right figure, the ellipse of an anisotropic spatial covariance function centered at the origin is shown. The black outline of each ellipse is a level curve of equal correlation. " width="50%"><img src="technical_files/figure-html/anisotropy-2.png" alt="In the left figure, the ellipse of an isotropic spatial covariance function centered at the origin is shown. In the right figure, the ellipse of an anisotropic spatial covariance function centered at the origin is shown. The black outline of each ellipse is a level curve of equal correlation. " width="50%"><p class="caption">
In the left figure, the ellipse of an isotropic spatial covariance function centered at the origin is shown. In the right figure, the ellipse of an anisotropic spatial covariance function centered at the origin is shown. The black outline of each ellipse is a level curve of equal correlation.
</p>
</div>
<p>Figure<span class="math inline">\(~\)</span> shows ellipses for an isotropic and anisotropic spatial covariance function centered at the origin (a distance of zero). The black outline of each ellipse is a level curve of equal correlation. The left ellipse (a circle) represents an isotropic covariance function. The distance at which the correlation between two observations lays on the level curve is the same in all directions. The right ellipse represents an anisotropic covariance function. The distance at which the correlation between two observations lays on the level curve is different in different directions.</p>
<p>To accommodate spatial anisotropy, the original coordinates must be transformed such that the transformed coordinates yield an isotropic spatial covariance. This transformation involves a rotation and a scaling. Consider a set of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> coordinates that should be transformed into <span class="math inline">\(x^*\)</span> and <span class="math inline">\(y^*\)</span> coordinates. This transformation is formally defined as <span class="math display">\[\begin{equation*}
  \begin{bmatrix}
    x^* \\
    y^*
  \end{bmatrix} = 
  \begin{bmatrix}
    1 &amp; 0 \\
    0 &amp; 1 / S
  \end{bmatrix}
  \begin{bmatrix}
    \cos(\alpha) &amp; \sin(\alpha) \\
    -\sin(\alpha) &amp; \cos(\alpha)
  \end{bmatrix}  
  \begin{bmatrix}
    x \\
    y
  \end{bmatrix}.
\end{equation*}\]</span> The original coordinates are first multiplied by the rotation matrix, which rotates the coordinates clockwise by angle <span class="math inline">\(\alpha\)</span>. They are then multiplied by the scaling matrix, which scales the minor axis of the spatial covariance ellipse by the reciprocal of <span class="math inline">\(S\)</span>. The transformed coordinates are then used to compute distances and the spatial covariances in Table<span class="math inline">\(~\)</span>. This type of anisotropy is more formally known as “geometric” anisotropy because it involves a geometric transformation of the coordinates. Figure<span class="math inline">\(~\)</span> shows this process step-by-step.</p>
<div class="figure">
<img src="technical_files/figure-html/anisotropy2-1.png" alt="In the left figure, the ellipse of an anisotropic spatial covariance function centered at the origin is shown. The blue lines represent the original axes and the red lines the transformed axes. The solid lines represent the x-axes and the dotted lines the y-axes. Note that the solid, red line is the major axis of the ellpise and the dashed, red line is the minor axis of the ellipse. In the center figure, the ellipse has been rotated clockwise by the rotate parameter so the major axis is the transformed x-axis and the minor axis is the transformed y-axis. In the right figure, the minor axis of the ellipse has been scaled by the reciprocal of the scale parameter so that the ellipse becomes a circle, which corresponds to an isotropic spatial covariance function. The transformed coordinates are then used to compute distances and spatial covariances." width="33%"><img src="technical_files/figure-html/anisotropy2-2.png" alt="In the left figure, the ellipse of an anisotropic spatial covariance function centered at the origin is shown. The blue lines represent the original axes and the red lines the transformed axes. The solid lines represent the x-axes and the dotted lines the y-axes. Note that the solid, red line is the major axis of the ellpise and the dashed, red line is the minor axis of the ellipse. In the center figure, the ellipse has been rotated clockwise by the rotate parameter so the major axis is the transformed x-axis and the minor axis is the transformed y-axis. In the right figure, the minor axis of the ellipse has been scaled by the reciprocal of the scale parameter so that the ellipse becomes a circle, which corresponds to an isotropic spatial covariance function. The transformed coordinates are then used to compute distances and spatial covariances." width="33%"><img src="technical_files/figure-html/anisotropy2-3.png" alt="In the left figure, the ellipse of an anisotropic spatial covariance function centered at the origin is shown. The blue lines represent the original axes and the red lines the transformed axes. The solid lines represent the x-axes and the dotted lines the y-axes. Note that the solid, red line is the major axis of the ellpise and the dashed, red line is the minor axis of the ellipse. In the center figure, the ellipse has been rotated clockwise by the rotate parameter so the major axis is the transformed x-axis and the minor axis is the transformed y-axis. In the right figure, the minor axis of the ellipse has been scaled by the reciprocal of the scale parameter so that the ellipse becomes a circle, which corresponds to an isotropic spatial covariance function. The transformed coordinates are then used to compute distances and spatial covariances." width="33%"><p class="caption">
In the left figure, the ellipse of an anisotropic spatial covariance function centered at the origin is shown. The blue lines represent the original axes and the red lines the transformed axes. The solid lines represent the x-axes and the dotted lines the y-axes. Note that the solid, red line is the major axis of the ellpise and the dashed, red line is the minor axis of the ellipse. In the center figure, the ellipse has been rotated clockwise by the rotate parameter so the major axis is the transformed x-axis and the minor axis is the transformed y-axis. In the right figure, the minor axis of the ellipse has been scaled by the reciprocal of the scale parameter so that the ellipse becomes a circle, which corresponds to an isotropic spatial covariance function. The transformed coordinates are then used to compute distances and spatial covariances.
</p>
</div>
<p>Anisotropy parameters (<span class="math inline">\(\alpha\)</span> and <span class="math inline">\(S\)</span>) can be estimated in <code>spmodel</code> using restricted maximum likelihood or maximum likelihood. Estimating anisotropy can be challenging. First, we need to restrict the parameter space so that the two parameters are identifiable (there is a unique parameter set for each possible outcome). We restricted <span class="math inline">\(\alpha\)</span> to <span class="math inline">\([0, \pi]\)</span> radians due to symmetry of the covariance ellipse at rotations <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\alpha + j \pi\)</span>, where <span class="math inline">\(j\)</span> is any integer. We also restricted <span class="math inline">\(S\)</span> to occur on <span class="math inline">\([0, 1]\)</span> because we have defined <span class="math inline">\(S\)</span> as the scaling factor for the length of the minor axis relative to the major axis – otherwise it would not be clear whether <span class="math inline">\(S\)</span> refers to the minor or major axis. Given this restricted parameter space, there is still an issue of local maxima, particularly at rotation parameters near zero, which have a rotation very close to rotation parameter <span class="math inline">\(\pi\)</span>, but zero is far from <span class="math inline">\(\pi\)</span> in the parameter space. To address the local maxima problem, each optimization iteration actually involves two likelihood evaluations – one for <span class="math inline">\(\alpha\)</span> and another for <span class="math inline">\(|\pi - \alpha|\)</span>, where <span class="math inline">\(|\cdot|\)</span> denotes absolute value. Thus one likelihood evaluation is always in <span class="math inline">\([0, \pi/2]\)</span> radians and another in <span class="math inline">\([\pi/2, \pi]\)</span> radians, exploring different quadrants of the parameter space and allowing optimization to test solutions near zero and <span class="math inline">\(\pi\)</span> simultaneously.</p>
<p>Anisotropy parameters cannot be estimated in <code>spmodel</code> when <code>estmethod</code> is <code>sv-wls</code> or <code>sv-cl</code>. However, known anisotropy parameters for these estimation methods can be specified via <code>spcov_initial</code> and incorporated into estimation of <span class="math inline">\(\boldsymbol{\theta}\)</span> and <span class="math inline">\(\boldsymbol{\beta}\)</span>. Anisotropy is not defined for areal data given its (binary) neighborhood structure.</p>
</div>
<div class="section level3">
<h3 id="partition-factors">Partition Factors<a class="anchor" aria-label="anchor" href="#partition-factors"></a>
</h3>
<p>A partition factor is a factor (or categorical) variable in which observations from different levels of the partition factor are assumed uncorrelated. A partition matrix <span class="math inline">\(\mathbf{P}\)</span> of dimension <span class="math inline">\(n \times n\)</span> can be constructed to represent the partition factor. The <span class="math inline">\(ij\)</span>th element of <span class="math inline">\(\mathbf{P}\)</span> equals one if the observation in the <span class="math inline">\(i\)</span>th row and <span class="math inline">\(j\)</span>th column are from the same level of the partition factor and zero otherwise. Then the initial covariance matrix (ignoring the partition factor) is updated by taking the Hadmard (element-wise) product with the partition matrix: <span class="math display">\[\begin{equation*}
 \boldsymbol{\Sigma}_{updated} = \boldsymbol{\Sigma}_{initial} \odot \mathbf{P},
\end{equation*}\]</span> where <span class="math inline">\(\odot\)</span> indicates the Hadmard product. Partition factors impose a block structure in <span class="math inline">\(\boldsymbol{\Sigma}\)</span>, which allows for efficient computation of <span class="math inline">\(\boldsymbol{\Sigma}^{-1}\)</span> used for estimation and prediction.</p>
<p>When computing the empirical semivariogram using <code><a href="../reference/esv.html">esv()</a></code>, semivariances are ignored when observations are from different levels of the partition factor. For the <code>sv-wls</code> and <code>sv-cl</code> estimation methods, semivariances are ignored when observations are from different levels of the partition factor.</p>
</div>
<div class="section level3">
<h3 id="subsec:bigdata">Big Data (<code>splm()</code> only)<a class="anchor" aria-label="anchor" href="#subsec:bigdata"></a>
</h3>
<p>Big data model-fitting is accommodated in <code>spmodel</code> using a “local indexing” approach. Suppose there are <span class="math inline">\(m\)</span> unique indexes, and each observation is in one index. Then <span class="math inline">\(\boldsymbol{\Sigma}\)</span> can be represented blockwise as <span class="math display">\[\begin{equation}\label{eq:full_cov}
  \boldsymbol{\Sigma} = 
  \begin{bmatrix}
  \boldsymbol{\Sigma}_{1,1} &amp; \boldsymbol{\Sigma}_{1,2} &amp; \hdots &amp; \hdots &amp; \boldsymbol{\Sigma}_{1,m} \\
  \boldsymbol{\Sigma}_{2,1} &amp; \boldsymbol{\Sigma}_{2,2} &amp; \boldsymbol{\Sigma}_{2,3} &amp; \hdots &amp; \boldsymbol{\Sigma}_{2,m} \\
  \vdots &amp; \boldsymbol{\Sigma}_{3,2} &amp; \ddots &amp; \boldsymbol{\Sigma}_{3,4} &amp; \vdots \\
  \vdots &amp; \vdots &amp; \boldsymbol{\Sigma}_{4,3} &amp; \ddots &amp; \vdots \\
  \boldsymbol{\Sigma}_{m,1} &amp; \hdots &amp; \hdots &amp; \hdots &amp; \boldsymbol{\Sigma}_{m, m}
  \end{bmatrix},
\end{equation}\]</span> To perform estimation for big data, observations with the same index value are assumed independent of observations with different index values, yielding a “big-data” covariance matrix given by <span class="math display">\[\begin{equation}\label{eq:bd_cov}
  \boldsymbol{\Sigma}_{bd} = 
  \begin{bmatrix}
  \boldsymbol{\Sigma}_{1,1} &amp; \boldsymbol{0} &amp; \hdots &amp; \hdots &amp; \boldsymbol{0} \\
  \boldsymbol{0} &amp; \boldsymbol{\Sigma}_{2,2} &amp; \boldsymbol{0} &amp; \hdots &amp; \boldsymbol{0} \\
  \vdots &amp; \boldsymbol{0} &amp; \ddots &amp; \boldsymbol{0} &amp; \vdots \\
  \vdots &amp; \vdots &amp; \boldsymbol{0} &amp; \ddots &amp; \vdots \\
  \boldsymbol{0} &amp; \hdots &amp; \hdots &amp; \hdots &amp; \boldsymbol{\Sigma}_{m, m}
  \end{bmatrix},
\end{equation}\]</span> Estimation then proceeds as described in Section<span class="math inline">\(~\)</span> using <span class="math inline">\(\boldsymbol{\Sigma}_{bd}\)</span> instead of <span class="math inline">\(\boldsymbol{\Sigma}\)</span>. When computing the empirical semivariogram, semivariances are ignored when observations have different local indexes. For the <code>sv-wls</code> and <code>sv-cl</code> estimation methods, semivariances are ignored when observations have different local indexes. Via Equation<span class="math inline">\(~\)</span>, it can be seen that the local index acts as a partition factor separate from the partition factor explicitly defined by <code>partition_factor</code>.</p>
<p>spmodel allows for custom local indexes to be passed to <code><a href="../reference/splm.html">splm()</a></code>. If a custom local index is not passed, the local index is determined using the <code>"random"</code> or <code>"kmeans"</code> method. The <code>"random"</code> method assigns observations to indexes randomly based on the number of groups desired. The <code>"kmeans"</code> method uses k-means clustering <span class="citation">(MacQueen and others 1967)</span> on the x-coordinates and y-coordinates to assign observations to indexes (based on the number of clusters (groups) desired).</p>
<p>The estimate of <span class="math inline">\(\boldsymbol{\beta}\)</span> when using Equation<span class="math inline">\(~\)</span> is given by <span class="math display">\[\begin{equation}\label{eq:beta_bd}
  \hat{\boldsymbol{\beta}}_{bd} = (\mathbf{X}^\top \boldsymbol{\hat{\Sigma}}^{-1}_{bd}\mathbf{X})^{-1}\mathbf{X}^\top \boldsymbol{\hat{\Sigma}}^{-1}_{bd} \mathbf{y} = \mathbf{T}^{-1}_{xx}\mathbf{t}_{xy},
\end{equation}\]</span> where <span class="math inline">\(\mathbf{T}_{xx} = \sum_{i = 1}^m \mathbf{X}_i^\top \boldsymbol{\hat{\Sigma}}^{-1}_{i, i}\mathbf{X}_i\)</span> and <span class="math inline">\(\mathbf{t}_{xy} = \sum_{i = 1}^m \mathbf{X}_i^\top \hat{\boldsymbol{\Sigma}}^{-1}_{i, i} \mathbf{y}_i\)</span>. Note that in <span class="math inline">\(\hat{\boldsymbol{\beta}}_{bd}\)</span>, <span class="math inline">\(\mathbf{X}_i\)</span> and <span class="math inline">\(\mathbf{y}_i\)</span> are the subsets of <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{y}\)</span>, respectively, for the <span class="math inline">\(i\)</span>th local index. Equation<span class="math inline">\(~\)</span> acts as a pooled estimator of <span class="math inline">\(\boldsymbol{\beta}\)</span> across the indexes.</p>
<p><code>spmodel</code> has four approaches for estimating the covariance matrix of <span class="math inline">\(\hat{\boldsymbol{\beta}}_{bd}\)</span>. The choice is determined by the <code>var_adjust</code> argument to <code>local</code>. The first approach is implements no adjustment (<code>var_adjust = "none"</code>) and simply uses <span class="math inline">\(\mathbf{T}_{xx}^{-1}\)</span>, which is the covariance matrix of <span class="math inline">\(\hat{\boldsymbol{\beta}}_{bd}\)</span> using <span class="math inline">\(\boldsymbol{\Sigma}_{bd}\)</span> (Equation<span class="math inline">\(~\)</span>). While computationally efficient, this approach ignores the covariance across indexes. It can be shown that the covariance of <span class="math inline">\(\hat{\boldsymbol{\beta}}_{bd}\)</span> using <span class="math inline">\(\boldsymbol{\Sigma}\)</span> (Equation<span class="math inline">\(~\)</span>) is given by <span class="math display">\[\begin{equation}\label{eq:var_theo}
  \mathbf{T}_{xx}^{-1} + \mathbf{T}_{xx}^{-1} \mathbf{W}_{xx}\mathbf{T}_{xx}^{-1},
\end{equation}\]</span> where <span class="math display">\[\begin{equation*}
\mathbf{W} = \sum_{i = 1}^{m - 1} \sum_{j = i + 1}^m (\mathbf{X}^\top \hat{\boldsymbol{\Sigma}}^{-1}_{i, i} \hat{\boldsymbol{\Sigma}}_{i, j} \hat{\boldsymbol{\Sigma}}^{-1}_{j, j} \mathbf{X}_j) + (\mathbf{X}^\top \hat{\boldsymbol{\Sigma}}^{-1}_{i, i} \hat{\boldsymbol{\Sigma}}_{i, j} \hat{\boldsymbol{\Sigma}}^{-1}_{j, j} \mathbf{X}_j)^\top
\end{equation*}\]</span> Equation<span class="math inline">\(~\)</span> can be viewed as the sum of the unadjusted covariance matrix of <span class="math inline">\(\hat{\boldsymbol{\beta}}_{bd}\)</span> (<span class="math inline">\(\mathbf{T}_{xx}^{-1}\)</span>) and a correction that incorporates the covariance across indexes (<span class="math inline">\(\mathbf{T}_{xx}^{-1} \mathbf{W}_{xx}\mathbf{T}_{xx}^{-1}\)</span>). This adjustment is known as the “theoretically-correct” (<code>var_adjust = "theoretical"</code>) adjustment because it uses <span class="math inline">\(\boldsymbol{\Sigma}\)</span>. The theoretical adjustment is the default adjustment in <code>spmodel</code> because it is theoretically correct, but it is the most computationally expensive adjustment. Two alternative adjustments are also provided, and while not equal to the theoretical adjustment, they are easier to compute. They are the empirical (<code>var_adjust = "empirical"</code>) and pooled (<code>var_adjust = "pooled"</code>) adjustments. The empirical adjustment is given by <span class="math display">\[\begin{equation*}
\frac{1}{m(m -1)} \sum_{i = 1}^m (\boldsymbol{\hat{\beta}}_i - \boldsymbol{\hat{\beta}}_{bd})(\boldsymbol{\hat{\beta}}_i - \boldsymbol{\hat{\beta}}_{bd})^\top,
\end{equation*}\]</span> where <span class="math inline">\(\boldsymbol{\hat{\beta}}_i = (\mathbf{X}^\top \hat{\boldsymbol{\Sigma}}^{-1} \mathbf{X})^{-1}\mathbf{X}_i^\top \hat{\boldsymbol{\Sigma}}^{-1}_{i, i} \mathbf{y}_i\)</span>. A similar adjustment could use <span class="math inline">\(\boldsymbol{\hat{\beta}}_i = (\mathbf{X}_i^\top \hat{\boldsymbol{\Sigma}}^{-1}_{i, i} \mathbf{X}_i)^{-1}\mathbf{X}_i \hat{\boldsymbol{\Sigma}}^{-1}_{i, i} \mathbf{y}_i\)</span>, which more closely resembles a composite likelihood approach. This approach is sensitive to the presence of at least one singularity in <span class="math inline">\(\mathbf{X}_i^\top \hat{\boldsymbol{\Sigma}}^{-1}_{i, i} \mathbf{X}_i\)</span>, in which case the variance adjustment cannot be computed. The <code>"pooled"</code> variance adjustment is given by <span class="math display">\[\begin{equation*}
\frac{1}{m^2} \sum_{i = 1}^m (\mathbf{X}^\top_i \hat{\boldsymbol{\Sigma}}^{-1}_{i, i} \mathbf{X}_i)^{-1}.
\end{equation*}\]</span> Note that the pooled variance adjustment cannot be computed if any <span class="math inline">\(\mathbf{X}_i^\top \hat{\boldsymbol{\Sigma}}^{-1}_{i, i} \mathbf{X}_i\)</span> are singular.</p>
</div>
</div>
<div class="section level2">
<h2 id="sec:rf">
<code>splmRF()</code> and <code>spautorRF()</code><a class="anchor" aria-label="anchor" href="#sec:rf"></a>
</h2>
<p><code><a href="../reference/splmRF.html">splmRF()</a></code> and <code><a href="../reference/spautorRF.html">spautorRF()</a></code> fit random forest spatial residual models designed for prediction. These models are fit by combining aspects of random forest and spatial linear modeling. First, a random forest model <span class="citation">(Breiman 2001; James et al. 2013)</span> is fit using the <code>ranger</code> <strong>R</strong> package <span class="citation">(Wright and Ziegler 2015)</span>. Then random forest fitted values are obtained for each data observation and used to compute a residual (by subtracting the fitted value from the observed value). Then an intercept-only spatial linear model is fit to these residuals: <span class="math display">\[\begin{equation*}
  \mathbf{e}_{rf} = \beta_0 + \mathbf{\tau} + \mathbf{\epsilon},
\end{equation*}\]</span> where <span class="math inline">\(\mathbf{e}_{rf}\)</span> are the random forest residuals. Random forest spatial residual models can significantly improve predictive accuracy for new data compared to standard random forest models by formally incorporating spatial covariance in the random forest residuals <span class="citation">(Fox, Ver Hoef, and Olsen 2020)</span>. Random forest spatial residual model prediction is discussed in Section<span class="math inline">\(~\)</span>.</p>
<p>Different estimation methods, different spatial covariance functions, fixing spatial covariance parameter values, random effects, anisotropy, partition factors, and big data are accommodated in the spatial linear model portion of the random forest spatial residual models by supplying their respective named arguments to <code><a href="../reference/splmRF.html">splmRF()</a></code> and <code><a href="../reference/spautorRF.html">spautorRF()</a></code>.</p>
</div>
<div class="section level2">
<h2 id="sec:sprnorm">
<code>sprnorm()</code><a class="anchor" aria-label="anchor" href="#sec:sprnorm"></a>
</h2>
<p>Spatial normal (Gaussian) random variables are simulated by taking the sum of a fixed mean and random errors. The random errors have mean zero and covariance matrix <span class="math inline">\(\boldsymbol{\Sigma}\)</span>. A realization of the random errors is obtained from <span class="math inline">\(\boldsymbol{\Sigma}^{1/2} \mathbf{e}\)</span>, where <span class="math inline">\(\mathbf{e}\)</span> is a normal random variable with mean zero and covariance matrix <span class="math inline">\(\mathbf{I}\)</span>. Then the spatial normal random variable equals <span class="math display">\[\begin{equation*}
 \mathbf{y} = \boldsymbol{\mu} + \boldsymbol{\Sigma}^{1/2} \mathbf{e},
\end{equation*}\]</span> where <span class="math inline">\(\boldsymbol{\mu}\)</span> is the fixed mean. It follows that <span class="math display">\[\begin{equation*}
  \begin{split}
  \text{E}(\mathbf{y}) &amp; = \boldsymbol{\mu} + \boldsymbol{\Sigma}^{1/2} \text{E}(\mathbf{e}) = \boldsymbol{\mu} \\
  \text{Cov}(\mathbf{y}) &amp; = \text{Cov}(\boldsymbol{\Sigma}^{1/2} \mathbf{e}) = \boldsymbol{\Sigma}^{1/2} \text{Cov}(\mathbf{e}) \boldsymbol{\Sigma}^{1/2} = \boldsymbol{\Sigma}^{1/2} \boldsymbol{\Sigma}^{1/2} = \boldsymbol{\Sigma}
  \end{split}
\end{equation*}\]</span></p>
</div>
<div class="section level2">
<h2 id="sec:vcov">
<code>vcov()</code><a class="anchor" aria-label="anchor" href="#sec:vcov"></a>
</h2>
<p><code><a href="https://rdrr.io/r/stats/vcov.html" class="external-link">vcov()</a></code> returns the variance-covariance matrix of estimated parameters. Currently, <code><a href="https://rdrr.io/r/stats/vcov.html" class="external-link">vcov()</a></code> only returns the variance-covariance matrix of <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>, the fixed effects. The variance-covariance matrix of the fixed effects is given by <span class="math inline">\((\mathbf{X}^\top \hat{\boldsymbol{\Sigma}}^{-1} \mathbf{X})^{-1}\)</span>.</p>
</div>
<div class="section level2">
<h2 id="sec:iprod">A Note on Covariance Square Roots and Inverse Products<a class="anchor" aria-label="anchor" href="#sec:iprod"></a>
</h2>
<p>Often <span class="math inline">\(\boldsymbol{\Sigma}^{-1}\)</span> is not strictly needed for estimation, prediction, or other purposes, but at least the product between <span class="math inline">\(\boldsymbol{\Sigma}^{-1}\)</span> and some other matrix is needed. Consider the example of the covariance matrix of <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> and observe <span class="math inline">\(\mathbf{X}^\top \boldsymbol{\Sigma}^{-1} \mathbf{X}\)</span> is needed. The most direct way to find this product is certainly to obtain <span class="math inline">\(\boldsymbol{\Sigma}^{-1}\)</span> and then multiply by <span class="math inline">\(\mathbf{X}^\top\)</span> on the left and <span class="math inline">\(\mathbf{X}\)</span> on the right. This is both computationally expensive and cannot be used to compute products that involve <span class="math inline">\(\boldsymbol{\Sigma}^{-1/2}\)</span>, which are often useful (Section<span class="math inline">\(~\)</span>). It is helpful to rewrite <span class="math inline">\(\mathbf{X}^\top \boldsymbol{\Sigma}^{-1} \mathbf{X}\)</span> as <span class="math inline">\(\mathbf{X}^\top (\mathbf{S}^\top)^{-1} \mathbf{S}^{-1} \mathbf{X} = (\mathbf{S}^{-1} \mathbf{X})^\top \mathbf{S}^{-1} \mathbf{X}\)</span>. Then one computes the inverse products by finding <span class="math inline">\(\mathbf{S}\)</span>.</p>
<p>One way to find <span class="math inline">\(\mathbf{S}\)</span> is to use an eigendecomposition. The eigendecomposition of <span class="math inline">\(\boldsymbol{\Sigma}\)</span> (which is real and symmetric) is given by <span class="math display">\[\begin{equation*}
\boldsymbol{\Sigma} = \mathbf{U} \mathbf{D} \mathbf{U}^\top,
\end{equation*}\]</span> where <span class="math inline">\(\mathbf{U}\)</span> is an orthogonal matrix of eigenvectors of <span class="math inline">\(\boldsymbol{\Sigma}\)</span> and <span class="math inline">\(\mathbf{D}\)</span> is a diagonal matrix with eigenvalues of <span class="math inline">\(\boldsymbol{\Sigma}\)</span> on the diagonal. Then <span class="math inline">\(\boldsymbol{\Sigma}^{1/2} = \mathbf{U} \mathbf{D}^{1/2} \mathbf{U}^\top\)</span>, where <span class="math inline">\(\mathbf{D}^{1/2}\)</span> is a diagonal matrix with square roots of eigenvalues of <span class="math inline">\(\boldsymbol{\Sigma}\)</span> on the diagonal. This result follows because <span class="math inline">\(\mathbf{U}\)</span> being orthogonal implies <span class="math inline">\(\mathbf{U}^\top = \mathbf{U}^{-1}\)</span> and <span class="math display">\[\begin{equation*}
\boldsymbol{\Sigma}^{1/2}\boldsymbol{\Sigma}^{1/2} = \mathbf{U} \mathbf{D}^{1/2} \mathbf{U}^\top \mathbf{U} \mathbf{D}^{1/2} \mathbf{U}^\top = \mathbf{U} \mathbf{D}^{1/2} (\mathbf{U}^\top \mathbf{U}) \mathbf{D}^{1/2} \mathbf{U}^\top = \mathbf{U} \mathbf{D} \mathbf{U}^\top = \boldsymbol{\Sigma}.
\end{equation*}\]</span> So then taking <span class="math inline">\(\mathbf{S} = \mathbf{D}^{1/2}\)</span> implies <span class="math inline">\(\mathbf{S}^{-1} = \mathbf{D}^{-1/2}\)</span>, which is straightforward to calculate as <span class="math inline">\(\mathbf{D}^{1/2}\)</span> is diagonal. So not only does the eigendecomposition approach give us the inverse products, it also gives us <span class="math inline">\(\boldsymbol{\Sigma}^{1/2}\)</span> and <span class="math inline">\(\boldsymbol{\Sigma}^{-1/2}\)</span>. While straightforward, this approach is less efficient than the Cholesky decomposition <span class="citation">(Golub and Van Loan 2013)</span>, which we discuss next.</p>
<p>The Cholesky decomposition decomposes <span class="math inline">\(\boldsymbol{\Sigma}\)</span> into the product between <span class="math inline">\(\mathbf{C}\)</span> and <span class="math inline">\(\mathbf{C}^\top\)</span> (<span class="math inline">\(\boldsymbol{\Sigma} = \mathbf{C}\mathbf{C}^\top\)</span>), where <span class="math inline">\(\mathbf{C}\)</span> is a lower triangular matrix. Note that <span class="math inline">\(\mathbf{C}\)</span> is generally not equal to <span class="math inline">\(\boldsymbol{\Sigma}^{1/2}\)</span>. Taking <span class="math inline">\(\mathbf{S}\)</span> to be <span class="math inline">\(\mathbf{C}\)</span>, we see that finding the inverse products requires solving <span class="math inline">\(\mathbf{C}^{-1}\mathbf{X}\)</span>. Observe that <span class="math inline">\(\mathbf{C}^{-1}\mathbf{X} = \mathbf{A}\)</span> for some matrix <span class="math inline">\(\mathbf{A}\)</span>. This implies <span class="math inline">\(\mathbf{X} = \mathbf{C}\mathbf{A}\)</span>, which for <span class="math inline">\(\mathbf{A}\)</span> can be efficiently solved using forward substitution because <span class="math inline">\(\mathbf{C}\)</span> is lower triangular.</p>
<p>The products in this document that involve <span class="math inline">\(\boldsymbol{\Sigma}^{1/2}\)</span> and <span class="math inline">\(\boldsymbol{\Sigma}^{-1/2}\)</span> are actually implemented in <code>spmodel</code> using <span class="math inline">\(\mathbf{C}\)</span> and <span class="math inline">\(\mathbf{C}^{-1}\)</span> (instead of <span class="math inline">\(\boldsymbol{\Sigma}^{1/2}\)</span> and <span class="math inline">\(\boldsymbol{\Sigma}^{-1/2}\)</span>). They are written in this document using <span class="math inline">\(\boldsymbol{\Sigma}^{1/2}\)</span> and <span class="math inline">\(\boldsymbol{\Sigma}^{-1/2}\)</span> because the underlying concepts are easier to communicate using square root notation.</p>
</div>
<div class="section level2">
<h2 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<div id="refs">
<div id="ref-breiman2001random">
<p>Breiman, Leo. 2001. “Random Forests.” <em>Machine Learning</em> 45 (1): 5–32.</p>
</div>
<div id="ref-brent1971algorithm">
<p>Brent, Richard P. 1971. “An Algorithm with Guaranteed Convergence for Finding a Zero of a Function.” <em>The Computer Journal</em> 14 (4): 422–25.</p>
</div>
<div id="ref-cook1979influential">
<p>Cook, R Dennis. 1979. “Influential Observations in Linear Regression.” <em>Journal of the American Statistical Association</em> 74 (365): 169–74.</p>
</div>
<div id="ref-cook1982residuals">
<p>Cook, R Dennis, and Sanford Weisberg. 1982. <em>Residuals and Influence in Regression</em>. New York: Chapman; Hall.</p>
</div>
<div id="ref-cressie1985fitting">
<p>Cressie, Noel. 1985. “Fitting Variogram Models by Weighted Least Squares.” <em>Journal of the International Association for Mathematical Geology</em> 17 (5): 563–86.</p>
</div>
<div id="ref-cressie1993statistics">
<p>———. 1993. <em>Statistics for Spatial Data</em>. John Wiley &amp; Sons.</p>
</div>
<div id="ref-curriero1999composite">
<p>Curriero, Frank C, and Subhash Lele. 1999. “A Composite Likelihood Approach to Semivariogram Estimation.” <em>Journal of Agricultural, Biological, and Environmental Statistics</em>, 9–28.</p>
</div>
<div id="ref-fox2020comparing">
<p>Fox, Eric W, Jay M Ver Hoef, and Anthony R Olsen. 2020. “Comparing Spatial Regression to Random Forests for Large Environmental Data Sets.” <em>PloS One</em> 15 (3): e0229509.</p>
</div>
<div id="ref-goldman2000statistical">
<p>Goldman, Nick, and Simon Whelan. 2000. “Statistical Tests of Gamma-Distributed Rate Heterogeneity in Models of Sequence Evolution in Phylogenetics.” <em>Molecular Biology and Evolution</em> 17 (6): 975–78.</p>
</div>
<div id="ref-golub2013matrix">
<p>Golub, Gene H, and Charles F Van Loan. 2013. <em>Matrix Computations</em>. JHU press.</p>
</div>
<div id="ref-harville1977maximum">
<p>Harville, David A. 1977. “Maximum Likelihood Approaches to Variance Component Estimation and to Related Problems.” <em>Journal of the American Statistical Association</em> 72 (358): 320–38.</p>
</div>
<div id="ref-harville1992mean">
<p>Harville, David A, and Daniel R Jeske. 1992. “Mean Squared Error of Estimation or Prediction Under a General Linear Model.” <em>Journal of the American Statistical Association</em> 87 (419): 724–31.</p>
</div>
<div id="ref-henderson1975best">
<p>Henderson, Charles R. 1975. “Best Linear Unbiased Estimation and Prediction Under a Selection Model.” <em>Biometrics</em>, 423–47.</p>
</div>
<div id="ref-hoeting2006model">
<p>Hoeting, Jennifer A, Richard A Davis, Andrew A Merton, and Sandra E Thompson. 2006. “Model Selection for Geostatistical Models.” <em>Ecological Applications</em> 16 (1): 87–98.</p>
</div>
<div id="ref-hrong1996approximate">
<p>Hrong-Tai Fai, Alex, and Paul L Cornelius. 1996. “Approximate F-Tests of Multiple Degree of Freedom Hypotheses in Generalized Least Squares Analyses of Unbalanced Split-Plot Experiments.” <em>Journal of Statistical Computation and Simulation</em> 54 (4): 363–78.</p>
</div>
<div id="ref-james2013introduction">
<p>James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. <em>An Introduction to Statistical Learning</em>. Vol. 112. Springer.</p>
</div>
<div id="ref-kackar1984approximations">
<p>Kackar, Raghu N, and David A Harville. 1984. “Approximations for Standard Errors of Estimators of Fixed and Random Effects in Mixed Linear Models.” <em>Journal of the American Statistical Association</em> 79 (388): 853–62.</p>
</div>
<div id="ref-kenward1997small">
<p>Kenward, Michael G, and James H Roger. 1997. “Small Sample Inference for Fixed Effects from Restricted Maximum Likelihood.” <em>Biometrics</em>, 983–97.</p>
</div>
<div id="ref-kenward2009improved">
<p>———. 2009. “An Improved Approximation to the Precision of Fixed Effects from Restricted Maximum Likelihood.” <em>Computational Statistics &amp; Data Analysis</em> 53 (7): 2583–95.</p>
</div>
<div id="ref-littell2006sas">
<p>Littell, Ramon C, George A Milliken, Walter W Stroup, Russell D Wolfinger, and Schabenberber Oliver. 2006. <em>SAS for Mixed Models</em>. SAS publishing.</p>
</div>
<div id="ref-macqueen1967some">
<p>MacQueen, James, and others. 1967. “Some Methods for Classification and Analysis of Multivariate Observations.” In <em>Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability</em>, 1:281–97. 14. Oakland, CA, USA.</p>
</div>
<div id="ref-meinshausen2006quantile">
<p>Meinshausen, Nicolai, and Greg Ridgeway. 2006. “Quantile Regression Forests.” <em>Journal of Machine Learning Research</em> 7 (6).</p>
</div>
<div id="ref-montgomery2021introduction">
<p>Montgomery, Douglas C, Elizabeth A Peck, and G Geoffrey Vining. 2021. <em>Introduction to Linear Regression Analysis</em>. John Wiley &amp; Sons.</p>
</div>
<div id="ref-myers2012generalized">
<p>Myers, Raymond H, Douglas C Montgomery, G Geoffrey Vining, and Timothy J Robinson. 2012. <em>Generalized Linear Models: With Applications in Engineering and the Sciences</em>. John Wiley &amp; Sons.</p>
</div>
<div id="ref-nelder1965simplex">
<p>Nelder, John A, and Roger Mead. 1965. “A Simplex Method for Function Minimization.” <em>The Computer Journal</em> 7 (4): 308–13.</p>
</div>
<div id="ref-patterson1971recovery">
<p>Patterson, Desmond, and Robin Thompson. 1971. “Recovery of Inter-Block Information When Block Sizes Are Unequal.” <em>Biometrika</em> 58 (3): 545–54.</p>
</div>
<div id="ref-pinheiro2006mixed">
<p>Pinheiro, José, and Douglas Bates. 2006. <em>Mixed-Effects Models in S and S-Plus</em>. Springer science &amp; business media.</p>
</div>
<div id="ref-prasad1990estimation">
<p>Prasad, NG Narasimha, and Jon NK Rao. 1990. “The Estimation of the Mean Squared Error of Small-Area Estimators.” <em>Journal of the American Statistical Association</em> 85 (409): 163–71.</p>
</div>
<div id="ref-satterthwaite1946approximate">
<p>Satterthwaite, Franklin E. 1946. “An Approximate Distribution of Estimates of Variance Components.” <em>Biometrics Bulletin</em> 2 (6): 110–14.</p>
</div>
<div id="ref-schluchter1990small">
<p>Schluchter, Mark D, and Janet T Elashoff. 1990. “Small-Sample Adjustments to Tests with Unbalanced Repeated Measures Assuming Several Covariance Structures.” <em>Journal of Statistical Computation and Simulation</em> 37 (1-2): 69–87.</p>
</div>
<div id="ref-searle2009variance">
<p>Searle, Shayle R, George Casella, and Charles E McCulloch. 2009. <em>Variance Components</em>. John Wiley &amp; Sons.</p>
</div>
<div id="ref-self1987asymptotic">
<p>Self, Steven G, and Kung-Yee Liang. 1987. “Asymptotic Properties of Maximum Likelihood Estimators and Likelihood Ratio Tests Under Nonstandard Conditions.” <em>Journal of the American Statistical Association</em> 82 (398): 605–10.</p>
</div>
<div id="ref-stram1994variance">
<p>Stram, Daniel O, and Jae Won Lee. 1994. “Variance Components Testing in the Longitudinal Mixed Effects Model.” <em>Biometrics</em>, 1171–7.</p>
</div>
<div id="ref-wolf1978helmert">
<p>Wolf, Helmut. 1978. “The Helmert Block Method-Its Origin and Development.” In <em>Proceedings of the Second International Symposium on Problems Related to the Redefinition of North American Geodetic Networks,(NOAA, Arlington-Va, 1978)</em>, 319–26.</p>
</div>
<div id="ref-wolfinger1994computing">
<p>Wolfinger, Russ, Randy Tobias, and John Sall. 1994. “Computing Gaussian Likelihoods and Their Derivatives for General Linear Mixed Models.” <em>SIAM Journal on Scientific Computing</em> 15 (6): 1294–1310.</p>
</div>
<div id="ref-wright2015ranger">
<p>Wright, Marvin N, and Andreas Ziegler. 2015. “Ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R.” <em>arXiv Preprint arXiv:1508.04409</em>.</p>
</div>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Michael Dumelle, Matt Higham, Jay M. Ver Hoef.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.6.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
