<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Technical Details • spmodel</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.1/jquery.min.js" integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Technical Details">
<meta property="og:description" content="spmodel">
<meta property="og:image" content="https://usepa.github.io/spmodel/logo.png">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    

    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">spmodel</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">0.10.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/introduction.html">An Introduction to spmodel</a>
    </li>
    <li>
      <a href="../articles/guide.html">A Detailed Guide to spmodel</a>
    </li>
    <li>
      <a href="../articles/SPGLMs.html">Spatial Generalized Linear Models in spmodel</a>
    </li>
    <li>
      <a href="../articles/emmeans.html">Using emmeans to Estimate Marginal Means of spmodel Objects</a>
    </li>
    <li>
      <a href="../articles/technical.html">Technical Details</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/USEPA/spmodel/" class="external-link">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Technical Details</h1>
                        <h4 data-toc-skip class="author">Michael
Dumelle, Matt Higham, and Jay M. Ver Hoef</h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/USEPA/spmodel/blob/HEAD/vignettes/articles/technical.Rmd" class="external-link"><code>vignettes/articles/technical.Rmd</code></a></small>
      <div class="hidden name"><code>technical.Rmd</code></div>

    </div>

    
    
<div class="section level2">
<h2 id="sec:introduction">Introduction<a class="anchor" aria-label="anchor" href="#sec:introduction"></a>
</h2>
<p>This vignette covers technical details regarding the functions in
that perform computations in <code>spmodel</code>. We first provide a
notation guide and then describe relevant details for each function.</p>
<p>If you use <code>spmodel</code> in a formal publication or report,
please cite it. Citing <code>spmodel</code> lets us devote more
resources to it in the future. To view the <code>spmodel</code>
citation, run</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/citation.html" class="external-link">citation</a></span><span class="op">(</span>package <span class="op">=</span> <span class="st">"spmodel"</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">#&gt; To cite spmodel in publications use:</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;   Dumelle M, Higham M, Ver Hoef JM (2023). spmodel: Spatial statistical</span></span>
<span><span class="co">#&gt;   modeling and prediction in R. PLOS ONE 18(3): e0282524.</span></span>
<span><span class="co">#&gt;   https://doi.org/10.1371/journal.pone.0282524</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; A BibTeX entry for LaTeX users is</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;   @Article{,</span></span>
<span><span class="co">#&gt;     title = {{spmodel}: Spatial statistical modeling and prediction in {R}},</span></span>
<span><span class="co">#&gt;     author = {Michael Dumelle and Matt Higham and Jay M. {Ver Hoef}},</span></span>
<span><span class="co">#&gt;     journal = {PLOS ONE},</span></span>
<span><span class="co">#&gt;     year = {2023},</span></span>
<span><span class="co">#&gt;     volume = {18},</span></span>
<span><span class="co">#&gt;     number = {3},</span></span>
<span><span class="co">#&gt;     pages = {1--32},</span></span>
<span><span class="co">#&gt;     doi = {10.1371/journal.pone.0282524},</span></span>
<span><span class="co">#&gt;     url = {https://doi.org/10.1371/journal.pone.0282524},</span></span>
<span><span class="co">#&gt;   }</span></span></code></pre>
</div>
<div class="section level2">
<h2 id="sec:notation">Notation Guide<a class="anchor" aria-label="anchor" href="#sec:notation"></a>
</h2>
<span class="math display">\[\begin{equation*}
  \begin{split}
   n &amp; = \text{Sample size} \\
   \mathbf{y} &amp; = \text{Response vector} \\
   \boldsymbol{\beta} &amp; = \text{Fixed effect parameter vector} \\
   \mathbf{X} &amp; = \text{Design matrix of known explanatory variables
(covariates)} \\
   p &amp; = \text{The number of linearly independent columns in }
\mathbf{X} \\
   \boldsymbol{\mu} &amp; = \text{Mean vector} \\
   \mathbf{w} &amp; = \text{Latent generalized linear model mean on the
link scale} \\
   \varphi &amp; = \text{Dispersion parameter} \\
   \mathbf{Z} &amp; = \text{Design matrix of known random effect
variables} \\
   \boldsymbol{\theta} &amp; = \text{Covariance parameter vector} \\   
   \boldsymbol{\Sigma} &amp; = \text{Covariance matrix evaluated at }
\boldsymbol{\theta} \\
   \boldsymbol{\Sigma}^{-1} &amp; = \text{The inverse of }
\boldsymbol{\Sigma} \\
   \boldsymbol{\Sigma}^{1/2} &amp; = \text{The square root of }
\boldsymbol{\Sigma} \\
   \boldsymbol{\Sigma}^{-1/2} &amp; = \text{The inverse of }
\boldsymbol{\Sigma}^{1/2} \\
   \boldsymbol{\Theta} &amp; = \text{General parameter vector} \\  
   \ell(\boldsymbol{\Theta}) &amp; = \text{Log-likelihood evaluated at }
\boldsymbol{\Theta} \\
   \boldsymbol{\tau} &amp; = \text{Spatial (dependent) random error} \\
   \boldsymbol{\epsilon} &amp; = \text{Independent (non-spatial) random
error} \\
   \mathbf{A}^* &amp; = \boldsymbol{\Sigma}^{-1/2}\mathbf{A} \text{ for
a general matrix } \mathbf{A} \text{ (this is known as whitening
$\mathbf{A}$)}
  \end{split}
\end{equation*}\]</span>
<p>A hat indicates the parameters are estimated (i.e., <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>) or evaluated at
a relevant estimated parameter vector (e.g., <span class="math inline">\(\hat{\boldsymbol{\Sigma}}\)</span> is evaluated at
<span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span>). When
<span class="math inline">\(\ell(\boldsymbol{\hat{\Theta}})\)</span> is
written, it means the log-likelihood evaluated at its maximum, <span class="math inline">\(\boldsymbol{\hat{\Theta}}\)</span>. When the
covariance matrix of <span class="math inline">\(\mathbf{A}\)</span> is
<span class="math inline">\(\boldsymbol{\Sigma}\)</span>, we say <span class="math inline">\(\mathbf{A}^*\)</span> “whitens” <span class="math inline">\(\mathbf{A}\)</span> because <span class="math display">\[\begin{equation*}
\text{Cov}(\mathbf{A}^*) =
\text{Cov}(\boldsymbol{\Sigma}^{-1/2}\mathbf{A}) =
\boldsymbol{\Sigma}^{-1/2}\text{Cov}(\mathbf{A})\boldsymbol{\Sigma}^{-1/2}
= \boldsymbol{\Sigma}^{-1/2}\boldsymbol{\Sigma}
\boldsymbol{\Sigma}^{-1/2} =
(\boldsymbol{\Sigma}^{-1/2}\boldsymbol{\Sigma}^{1/2})(\boldsymbol{\Sigma}^{1/2}\boldsymbol{\Sigma}^{-1/2})
= \mathbf{I}.
\end{equation*}\]</span> Later we discuss how to obtain <span class="math inline">\(\boldsymbol{\Sigma}^{1/2}\)</span>.</p>
<p>Additional notation is used in the <code><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict()</a></code> section:
<span class="math display">\[\begin{equation*}
  \begin{split}
   \mathbf{y}_o &amp; = \text{Observed response vector} \\
   \mathbf{y}_u &amp; = \text{Unobserved response vector} \\
   \mathbf{X}_o &amp; = \text{Design matrix of known explanatory
variables at observed response variable locations} \\
   \mathbf{X}_u &amp; = \text{Design matrix of known explanatory
variables at unobserved response variable locations} \\
   \boldsymbol{\Sigma}_o &amp; = \text{Covariance matrix of
$\mathbf{y}_o$ evaluated at } \boldsymbol{\theta} \\
   \boldsymbol{\Sigma}_u &amp; = \text{Covariance matrix of
$\mathbf{y}_u$ evaluated at } \boldsymbol{\theta} \\
   \boldsymbol{\Sigma}_{uo} &amp; = \text{A matrix of covariances
between $\mathbf{y}_u$ and $\mathbf{y}_o$ evaluated at }
\boldsymbol{\theta} \\
   \mathbf{w}_o &amp; = \text{Latent $\mathbf{w}$ for each observation
in $\mathbf{y}_o$} \\
   \mathbf{w}_u &amp; = \text{Latent $\mathbf{w}$ for each observation
in $\mathbf{y}_o$} \\
   \mathbf{G}_o &amp; = \text{Hessian for $\mathbf{w}_o$} \\
  \end{split}
\end{equation*}\]</span></p>
</div>
<div class="section level2">
<h2 id="sec:splms">Spatial Linear Models<a class="anchor" aria-label="anchor" href="#sec:splms"></a>
</h2>
<p>Statistical linear models are often parameterized as <span class="math display">\[\begin{equation}\label{eq:lm}
\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon},
\end{equation}\]</span> where for a sample size <span class="math inline">\(n\)</span>, <span class="math inline">\(\mathbf{y}\)</span> is an <span class="math inline">\(n \times 1\)</span> column vector of response
variables, <span class="math inline">\(\mathbf{X}\)</span> is an <span class="math inline">\(n \times p\)</span> design (model) matrix of
explanatory variables, <span class="math inline">\(\boldsymbol{\beta}\)</span> is a <span class="math inline">\(p \times 1\)</span> column vector of fixed effects
controlling the impact of <span class="math inline">\(\mathbf{X}\)</span> on <span class="math inline">\(\mathbf{y}\)</span>, and <span class="math inline">\(\boldsymbol{\epsilon}\)</span> is an <span class="math inline">\(n \times 1\)</span> column vector of random
errors. We typically assume that <span class="math inline">\(\text{E}(\boldsymbol{\epsilon}) =
\mathbf{0}\)</span> and <span class="math inline">\(\text{Cov}(\boldsymbol{\epsilon}) =
\sigma^2_\epsilon \mathbf{I}\)</span>, where <span class="math inline">\(\text{E}(\cdot)\)</span> denotes expectation,
<span class="math inline">\(\text{Cov}(\cdot)\)</span> denotes
covariance, <span class="math inline">\(\sigma^2_\epsilon\)</span>
denotes a variance parameter, and <span class="math inline">\(\mathbf{I}\)</span> denotes the identity
matrix.</p>
<p>The model <span class="math inline">\(\mathbf{y} = \mathbf{X}
\boldsymbol{\beta} + \boldsymbol{\epsilon}\)</span> assumes the elements
of <span class="math inline">\(\mathbf{y}\)</span> are uncorrelated.
Typically for spatial data, elements of <span class="math inline">\(\mathbf{y}\)</span> are correlated, as
observations close together in space tend to be more similar than
observations far apart <span class="citation">(Tobler 1970)</span>.
Failing to properly accommodate the spatial dependence in <span class="math inline">\(\mathbf{y}\)</span> can cause researchers to draw
incorrect conclusions about their data. To accommodate spatial
dependence in <span class="math inline">\(\mathbf{y}\)</span>, an <span class="math inline">\(n \times 1\)</span> spatial random effect, <span class="math inline">\(\boldsymbol{\tau}\)</span>, is added to the linear
model, yielding the model <span class="math display">\[\begin{equation}\label{eq:splm}
\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\tau} +
\boldsymbol{\epsilon},
\end{equation}\]</span> where <span class="math inline">\(\boldsymbol{\tau}\)</span> is independent of <span class="math inline">\(\boldsymbol{\epsilon}\)</span>, <span class="math inline">\(\text{E}(\boldsymbol{\tau}) = \mathbf{0}\)</span>,
<span class="math inline">\(\text{Cov}(\boldsymbol{\tau}) =
\sigma^2_\tau \mathbf{R}\)</span>, <span class="math inline">\(\mathbf{R}\)</span> is a matrix that determines
the spatial dependence structure in <span class="math inline">\(\mathbf{y}\)</span> and depends on a range
parameter, <span class="math inline">\(\phi\)</span>. We discuss <span class="math inline">\(\mathbf{R}\)</span> in more detail shortly. The
parameter <span class="math inline">\(\sigma^2_\tau\)</span> is called
the spatially dependent random error variance or partial sill. The
parameter <span class="math inline">\(\sigma^2_\epsilon\)</span> is
called the spatially independent random error variance or nugget. These
two variance parameters are henceforth more intuitively written as <span class="math inline">\(\sigma^2_{de}\)</span> and <span class="math inline">\(\sigma^2_{ie}\)</span>, respectively. The
covariance of <span class="math inline">\(\mathbf{y}\)</span> is denoted
<span class="math inline">\(\boldsymbol{\Sigma}\)</span> and given by
<span class="math inline">\(\sigma^2_{de} \mathbf{R} + \sigma^2_{ie}
\mathbf{I}\)</span>. The parameters that compose this covariance are
contained in the vector <span class="math inline">\(\boldsymbol{\theta}\)</span>, which is called the
covariance parameter vector.</p>
<p>The model <span class="math inline">\(\mathbf{y} = \mathbf{X}
\boldsymbol{\beta} + \boldsymbol{\tau} + \boldsymbol{\epsilon}\)</span>
is called the spatial linear model. The spatial linear model applies to
both point-referenced and areal (i.e., lattice) data. Spatial data are
point-referenced when the elements in <span class="math inline">\(\mathbf{y}\)</span> are observed at
point-locations indexed by x-coordinates and y-coordinates on a
spatially continuous surface with an infinite number of locations. The
<code><a href="../reference/splm.html">splm()</a></code> function is used to fit spatial linear models for
point-referenced data (these are sometimes called geostatistical
models). One spatial covariance function available in
<code><a href="../reference/splm.html">splm()</a></code> is the exponential spatial covariance function,
which has an <span class="math inline">\(\mathbf{R}\)</span> matrix
given by <span class="math display">\[\begin{equation*}
  \mathbf{R} = \exp(-\mathbf{M} / \phi),
\end{equation*}\]</span><br>
where <span class="math inline">\(\mathbf{M}\)</span> is a matrix of
Euclidean distances among observations. Recall that <span class="math inline">\(\phi\)</span> is the range parameter, controlling
the behavior of of the covariance function as a function of distance.
Parameterizations for <code><a href="../reference/splm.html">splm()</a></code> spatial covariance types and
their <span class="math inline">\(\mathbf{R}\)</span> matrices can be
seen by running <code><a href="../reference/splm.html">help("splm", "spmodel")</a></code> or
<code><a href="../articles/technical.html">vignette("technical", "spmodel")</a></code>. Some of these spatial
covariance types (e.g., Matérn) depend on an extra parameter beyond
<span class="math inline">\(\sigma^2_{de}\)</span>, <span class="math inline">\(\sigma^2_{ie}\)</span>, and <span class="math inline">\(\phi\)</span>.</p>
<p>Spatial data are areal when the elements in <span class="math inline">\(\mathbf{y}\)</span> are observed as part of a
finite network of polygons whose connections are indexed by a
neighborhood structure. For example, the polygons may represent counties
in a state that are neighbors if they share at least one boundary. Areal
data are often equivalently called lattice data <span class="citation">(Cressie 1993)</span>. The <code><a href="../reference/spautor.html">spautor()</a></code>
function is used to fit spatial linear models for areal data (these are
sometimes called spatial autoregressive models). One spatial
autoregressive covariance function available in <code><a href="../reference/spautor.html">spautor()</a></code>
is the simultaneous autoregressive spatial covariance function, which
has an <span class="math inline">\(\mathbf{R}\)</span> matrix given by
<span class="math display">\[\begin{equation*}
  \mathbf{R} = [(\mathbf{I} - \phi \mathbf{W})(\mathbf{I} - \phi
\mathbf{W})^\top]^{-1},
\end{equation*}\]</span> where <span class="math inline">\(\mathbf{W}\)</span> is a weight matrix describing
the neighborhood structure in <span class="math inline">\(\mathbf{y}\)</span>. Parameterizations for
<code><a href="../reference/spautor.html">spautor()</a></code> spatial covariance types and their <span class="math inline">\(\mathbf{R}\)</span> matrices can be seen by
running <code><a href="../reference/spautor.html">help("spautor", "spmodel")</a></code> or
<code><a href="../articles/technical.html">vignette("technical", "spmodel")</a></code>.</p>
<p>One way to define <span class="math inline">\(\mathbf{W}\)</span> is
through queen contiguity <span class="citation">(Anselin, Syabri, and
Kho 2010)</span>. Two observations are queen contiguous if they share a
boundary. The <span class="math inline">\(ij\)</span>th element of <span class="math inline">\(\mathbf{W}\)</span> is then one if observation
<span class="math inline">\(i\)</span> and observation <span class="math inline">\(j\)</span> are queen contiguous and zero
otherwise. Observations are not considered neighbors with themselves, so
each diagonal element of <span class="math inline">\(\mathbf{W}\)</span>
is zero.</p>
<p>Sometimes each element in the weight matrix <span class="math inline">\(\mathbf{W}\)</span> is divided by its respective
row sum. This is called row-standardization. Row-standardizing <span class="math inline">\(\mathbf{W}\)</span> has several benefits, which
are discussed in detail by <span class="citation">Ver Hoef et al.
(2018)</span>.</p>
<div class="section level3">
<h3 id="sec:aic-lm">
<code>AIC()</code> and <code>AICc()</code><a class="anchor" aria-label="anchor" href="#sec:aic-lm"></a>
</h3>
<p>The <code><a href="https://rdrr.io/r/stats/AIC.html" class="external-link">AIC()</a></code> and <code><a href="../reference/AICc.html">AICc()</a></code> functions in
<code>spmodel</code> are defined for restricted maximum likelihood and
maximum likelihood estimation, which maximize a likelihood. The AIC and
AICc as defined by <span class="citation">Hoeting et al. (2006)</span>
are given by <span class="math display">\[\begin{equation*}\label{eq:sp_aic}
  \begin{split}
    \text{AIC} &amp; = -2\ell(\hat{\boldsymbol{\Theta}}) +
2(|\hat{\boldsymbol{\Theta}}|) \\
    \text{AICc} &amp; = -2\ell(\hat{\boldsymbol{\Theta}}) +
2n(|\hat{\boldsymbol{\Theta}}|) / (n - |\hat{\boldsymbol{\Theta}}| - 1),
  \end{split}
\end{equation*}\]</span> where <span class="math inline">\(|\hat{\boldsymbol{\Theta}}|\)</span> is the
cardinality of <span class="math inline">\(\hat{\boldsymbol{\Theta}}\)</span>. For restricted
maximum likelihood, <span class="math inline">\(\hat{\boldsymbol{\Theta}} \equiv
\{\hat{\boldsymbol{\theta}}\}\)</span>. For maximum likelihood, <span class="math inline">\(\hat{\boldsymbol{\Theta}} \equiv
\{\hat{\boldsymbol{\theta}}, \hat{\boldsymbol{\beta}}\}\)</span> The
discrepancy arises because restricted maximum likelihood integrates the
fixed effects out of the likelihood, and so the likelihood does not
depend on <span class="math inline">\(\boldsymbol{\beta}\)</span>.</p>
<p>AIC comparisons between a model fit using restricted maximum
likelihood and a model fit using maximum likelihood are meaningless, as
the models are fit with different likelihoods. AIC comparisons between
models fit using restricted maximum likelihood are only valid when the
models have the same fixed effect structure. In contrast, AIC
comparisons between models fit using maximum likelihood are valid when
the models have different fixed effect structures.</p>
</div>
<div class="section level3">
<h3 id="sec:anova-lm">
<code>anova()</code><a class="anchor" aria-label="anchor" href="#sec:anova-lm"></a>
</h3>
<p>Test statistics from are formed using the general linear hypothesis
test. Let <span class="math inline">\(\mathbf{L}\)</span> be an <span class="math inline">\(l \times p\)</span> contrast matrix and <span class="math inline">\(l_0\)</span> be an <span class="math inline">\(l
\times 1\)</span> vector. The null hypothesis is that <span class="math inline">\(\mathbf{L} \boldsymbol{\hat{\beta}} = l_0\)</span>
and the alternative hypothesis is that <span class="math inline">\(\mathbf{L} \boldsymbol{\hat{\beta}} \neq
l_0\)</span>. Usually, <span class="math inline">\(l_0\)</span> is the
zero vector (in <code>spmodel</code>, this is assumed). The test
statistic is denoted <span class="math inline">\(Chi2\)</span> and is
given by <span class="math display">\[\begin{equation*}\label{eq:glht}
  Chi2 = [(\mathbf{L} \boldsymbol{\hat{\beta}} - l_0)^\top(\mathbf{L}
(\mathbf{X}^\top \mathbf{\hat{\Sigma}} \mathbf{X})^{-1}
\mathbf{L}^\top)^{-1}(\mathbf{L} \boldsymbol{\hat{\beta}} - l_0)]
\end{equation*}\]</span> By default, <span class="math inline">\(\mathbf{L}\)</span> is chosen such that each
variable in the data used to fit the model is tested marginally (i.e.,
controlling for the other variables) against <span class="math inline">\(l_0 = \mathbf{0}\)</span>. If this default is not
desired, the and arguments can be used to pass user-defined <span class="math inline">\(\mathbf{L}\)</span> matrices to . They must be
constructed in such a way that <span class="math inline">\(l_0 =
\mathbf{0}\)</span>.</p>
<p>It is notoriously difficult to determine appropriate p-values for
linear mixed models based on the general linear hypothesis test. lme4,
for example, does not report p-values by default. A few reasons why
obtaining p-values is so challenging:</p>
<ul>
<li>The first (and often most important) challenge is that when
estimating <span class="math inline">\(\boldsymbol{\theta}\)</span>
using a finite sample, it is usually not clear what the null
distribution of <span class="math inline">\(Chi2\)</span> is. In certain
cases such as ordinary least squares regression or some experimental
designs (e.g., blocked design, split plot design, etc.), <span class="math inline">\(Chi2 / rank(\mathbf{L})\)</span> is F-distributed
with known numerator and denominator degrees of freedom. But outside of
these well-studied cases, no general results exist.</li>
<li>The second challenge is that the standard error of <span class="math inline">\(Chi2\)</span> does not account for the uncertainty
in <span class="math inline">\(\boldsymbol{\hat{\theta}}\)</span>. For
some approaches to addressing this problem, see <span class="citation">Kackar and Harville (1984)</span>, <span class="citation">Prasad and Rao (1990)</span>, <span class="citation">Harville and Jeske (1992)</span>, and <span class="citation">Kenward and Roger (1997)</span>.</li>
<li>The third challenge is in determining denominator degrees of
freedom. Again, in some cases, these are known – but this is not true in
general. For some approaches to addressing this problem, see <span class="citation">Satterthwaite (1946)</span>, <span class="citation">Schluchter and Elashoff (1990)</span>, <span class="citation">Hrong-Tai Fai and Cornelius (1996)</span>, <span class="citation">Kenward and Roger (1997)</span>, <span class="citation">Littell et al. (2006)</span>, <span class="citation">Pinheiro and Bates (2006)</span>, and <span class="citation">Kenward and Roger (2009)</span>.</li>
</ul>
<p>For these reasons, <code>spmodel</code> uses an asymptotic (i.e.,
large sample) Chi-squared test when calculating p-values using
<code><a href="https://rdrr.io/r/stats/anova.html" class="external-link">anova()</a></code>. This approach addresses the three points above by
assuming that with a large enough sample size:</p>
<ul>
<li>
<span class="math inline">\(Chi2\)</span> is asymptotically
Chi-squared (under certain conditions) with <span class="math inline">\(rank(\mathbf{L})\)</span> degrees of freedom when
the null hypothesis is true.</li>
<li>The uncertainty from estimating <span class="math inline">\(\boldsymbol{\hat{\theta}}\)</span> is small enough
to be safely ignored.</li>
</ul>
<p>Because the approximation is asymptotic, degree of freedom
adjustments can be ignored (it is also worth noting that an F
distribution with infinite denominator degrees of freedom is a
Chi-squared distribution scaled by <span class="math inline">\(rank(\mathbf{L})\)</span>. This asymptotic
approximation implies these p-values are likely unreliable with small
samples.</p>
<p>Note that when comparing full and reduced models, the general linear
hypothesis test is analogous to an extra sum of (whitened) squares
approach <span class="citation">(Myers et al. 2012)</span>.</p>
<p>A second approach to determining p-values is a likelihood ratio test.
Let <span class="math inline">\(\ell(\boldsymbol{\hat{\Theta}})\)</span>
be the log-likelihood for some full model and <span class="math inline">\(\ell(\boldsymbol{\hat{\Theta}}_0)\)</span> be the
log-likelihood for some reduced model. For the likelihood ratio test to
be valid, the reduced model must be nested in the full model, which
means that <span class="math inline">\(\ell(\boldsymbol{\hat{\Theta}}_0)\)</span> is
obtained by fixing some parameters in <span class="math inline">\(\boldsymbol{\Theta}\)</span>. When the likelihood
ratio test is valid, <span class="math inline">\(X^2 =
2\ell(\boldsymbol{\hat{\Theta}}) -
2\ell(\boldsymbol{\hat{\Theta}}_0)\)</span> is asymptotically
Chi-squared with degrees of freedom equal to the difference in estimated
parameters between the full and reduced models.</p>
<p>For restricted maximum likelihood estimation, likelihood ratio tests
can only be used to compare nested models with the same explanatory
variables. To use likelihood ratio tests for comparing different
explanatory variable structures, parameters must be estimated using
maximum likelihood estimation. When using likelihood ratio tests to
assess the importance of parameters on the boundary of a parameter space
(e.g., a variance parameter being zero), p-values tend to be too large
<span class="citation">(Self and Liang 1987; Stram and Lee 1994; Goldman
and Whelan 2000; Pinheiro and Bates 2006)</span>.</p>
</div>
<div class="section level3">
<h3 id="sec:bic-lm">
<code>BIC()</code><a class="anchor" aria-label="anchor" href="#sec:bic-lm"></a>
</h3>
<p>The <code><a href="https://rdrr.io/r/stats/AIC.html" class="external-link">BIC()</a></code> function in <code>spmodel</code> is defined
for restricted maximum likelihood and maximum likelihood estimation,
which maximize a likelihood. The BIC as defined by <span class="citation">Schwarz (1978)</span> is given by <span class="math display">\[\begin{equation*}\label{eq:sp_bic}
    \text{BIC} = -2\ell(\hat{\boldsymbol{\Theta}}) +
\ln(n)(|\hat{\boldsymbol{\Theta}}|),
\end{equation*}\]</span> where <span class="math inline">\(n\)</span> is
the sample size and <span class="math inline">\(|\hat{\boldsymbol{\Theta}}|\)</span> is the
cardinality of <span class="math inline">\(\hat{\boldsymbol{\Theta}}\)</span>. For restricted
maximum likelihood, <span class="math inline">\(\hat{\boldsymbol{\Theta}} \equiv
\{\hat{\boldsymbol{\theta}}\}\)</span>. For maximum likelihood, <span class="math inline">\(\hat{\boldsymbol{\Theta}} \equiv
\{\hat{\boldsymbol{\theta}}, \hat{\boldsymbol{\beta}}\}\)</span> The
discrepancy arises because restricted maximum likelihood integrates the
fixed effects out of the likelihood, and so the likelihood does not
depend on <span class="math inline">\(\boldsymbol{\beta}\)</span>.</p>
<p>BIC comparisons between a model fit using restricted maximum
likelihood and a model fit using maximum likelihood are meaningless, as
the models are fit with different likelihoods. BIC comparisons between
models fit using restricted maximum likelihood are only valid when the
models have the same fixed effect structure. In contrast, BIC
comparisons between models fit using maximum likelihood are valid when
the models have different fixed effect structures. While BIC was derived
by <span class="citation">Schwarz (1978)</span> for independent data,
<span class="citation">Zimmerman and Ver Hoef (2024)</span> show it can
be useful for spatially-dependent data as well.</p>
</div>
<div class="section level3">
<h3 id="sec:coef-lm">
<code>coef()</code><a class="anchor" aria-label="anchor" href="#sec:coef-lm"></a>
</h3>
<p><code><a href="https://rdrr.io/r/stats/coef.html" class="external-link">coef()</a></code> returns relevant coefficients based on the
<code>type</code> argument. When <code>type = "fixed"</code> (the
default), <code><a href="https://rdrr.io/r/stats/coef.html" class="external-link">coef()</a></code> returns <span class="math display">\[\begin{equation*}
\hat{\boldsymbol{\beta}} = (\mathbf{X}^\top
\hat{\boldsymbol{\Sigma}}^{-1} \mathbf{X})^{-1}\mathbf{X}^\top
\hat{\boldsymbol{\Sigma}}^{-1} \mathbf{y} .
\end{equation*}\]</span> If the estimation method is restricted maximum
likelihood or maximum likelihood, <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> is known as the
restricted maximum likelihood or maximum likelihood estimator of <span class="math inline">\(\boldsymbol{\beta}\)</span>. If the estimation
method is semivariogram weighted least squares or semivariogram
composite likelihood, <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> is known as the
empirical generalized least squares estimator of <span class="math inline">\(\boldsymbol{\beta}\)</span>. When
<code>type = "spcov"</code>, the estimated spatial covariance parameters
are returned (available for all estimation methods). When
<code>type = "randcov"</code>, the estimated random effect variance
parameters are returned (available for restricted maximum likelihood and
maximum likelihood estimation).</p>
</div>
<div class="section level3">
<h3 id="sec:confint-lm">
<code>confint()</code><a class="anchor" aria-label="anchor" href="#sec:confint-lm"></a>
</h3>
<p><code><a href="https://rdrr.io/r/stats/confint.html" class="external-link">confint()</a></code> returns confidence intervals for estimated
parameters. Currently, <code><a href="https://rdrr.io/r/stats/confint.html" class="external-link">confint()</a></code> only returns confidence
intervals for <span class="math inline">\(\boldsymbol{\beta}\)</span>.
The <span class="math inline">\((1 - \alpha)\)</span>% confidence
interval for <span class="math inline">\(\beta_i\)</span> is <span class="math display">\[\begin{equation*}
\hat{\beta}_i \pm z^* \sqrt{(\mathbf{X}^\top
\hat{\boldsymbol{\Sigma}}^{-1} \mathbf{X})^{-1}_{i, i}},
\end{equation*}\]</span> where <span class="math inline">\((\mathbf{X}^\top \hat{\boldsymbol{\Sigma}}^{-1}
\mathbf{X})^{-1}_{i, i}\)</span> is the <span class="math inline">\(i\)</span>th diagonal element in <span class="math inline">\((\mathbf{X}^\top \hat{\boldsymbol{\Sigma}}^{-1}
\mathbf{X})^{-1}\)</span>, <span class="math inline">\(\Phi(z^*) = 1 -
\alpha / 2\)</span>, <span class="math inline">\(\Phi(\cdot)\)</span> is
the standard normal (Gaussian) cumulative distribution function, and
<span class="math inline">\(\alpha = 1 -\)</span> <code>level</code>,
where <code>level</code> is an argument to <code><a href="https://rdrr.io/r/stats/confint.html" class="external-link">confint()</a></code>. The
default for <code>level</code> is 0.95, which corresponds to a <span class="math inline">\(z^*\)</span> of approximately 1.96.</p>
</div>
<div class="section level3">
<h3 id="sec:cooks-lm">
<code>cooks.distance()</code><a class="anchor" aria-label="anchor" href="#sec:cooks-lm"></a>
</h3>
<p>Cook’s distance measures the influence of an observation <span class="citation">(Cook 1979; Cook and Weisberg 1982)</span>. An
influential observation has a large impact on the model fit. The vector
of Cook’s distances for the spatial linear model is given by <span class="math display">\[\begin{equation} \label{eq:cooksd}
\frac{\mathbf{e}_p^2}{p} \odot diag(\mathbf{H}_s) \odot \frac{1}{1 -
diag(\mathbf{H}_s)},
\end{equation}\]</span> where <span class="math inline">\(\mathbf{e}_p\)</span> are the Pearson residuals
and <span class="math inline">\(diag(\mathbf{H}_s)\)</span> is the
diagonal of the spatial hat matrix, <span class="math inline">\(\mathbf{H}_s \equiv \mathbf{X}^* (\mathbf{X}^{*
\top} \mathbf{X}^*)^{-1} \mathbf{X}^{* \top}\)</span> <span class="citation">(Montgomery, Peck, and Vining 2021)</span>, and <span class="math inline">\(\odot\)</span> denotes the Hadmard (element-wise)
product. The larger the Cook’s distance, the larger the influence.</p>
<p>To better understand the previous form, recall that the the
non-spatial linear model <span class="math inline">\(\mathbf{y} =
\mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}\)</span> assumes
elements of <span class="math inline">\(\boldsymbol{\epsilon}\)</span>
are independent and identically distributed (iid) with constant
variance. In this context the vector of non-spatial Cook’s distances is
given by <span class="math display">\[\begin{equation*}
\frac{\mathbf{e}_p^2}{p} \odot diag(\mathbf{H}) \odot \frac{1}{1 -
diag(\mathbf{H})},
\end{equation*}\]</span> where <span class="math inline">\(diag(\mathbf{H})\)</span> is the diagonal of the
non-spatial hat matrix, <span class="math inline">\(\mathbf{H} \equiv
\mathbf{X} (\mathbf{X}^{\top} \mathbf{X})^{-1}
\mathbf{X}^{\top}\)</span>. When the elements of <span class="math inline">\(\boldsymbol{\epsilon}\)</span> are not iid or do
not have constant variance or both, the spatial Cook’s distance cannot
be calculated using <span class="math inline">\(\mathbf{H}\)</span>.
First the linear model must be whitened according to <span class="math inline">\(\mathbf{y}^* = \mathbf{X}^* \boldsymbol{\beta} +
\boldsymbol{\epsilon}^*\)</span>, where <span class="math inline">\(\boldsymbol{\epsilon}^*\)</span> is the whitened
version of the sum of all random errors in the model. Then the spatial
Cook’s distance follows using <span class="math inline">\(\mathbf{X}^*\)</span>, the whitened version of
<span class="math inline">\(\mathbf{X}\)</span>.</p>
</div>
<div class="section level3">
<h3 id="sec:deviance-lm">
<code>deviance()</code><a class="anchor" aria-label="anchor" href="#sec:deviance-lm"></a>
</h3>
<p>The deviance of a fitted model is <span class="math display">\[\begin{equation*}
\mathcal{D}_{\boldsymbol{\Theta}} = 2\ell(\boldsymbol{\Theta}_s) -
2\ell(\boldsymbol{\hat{\Theta}}),
\end{equation*}\]</span> where <span class="math inline">\(\ell(\boldsymbol{\Theta}_s)\)</span> is the
log-likelihood of a “saturated” model that fits every observation
perfectly. For normal (Gaussian) random errors, <span class="math display">\[\begin{equation*}
\mathcal{D}_{\boldsymbol{\Theta}} = (\mathbf{y} - \mathbf{X}
\hat{\boldsymbol{\beta}})^\top \hat{\boldsymbol{\Sigma}}^{-1}
(\mathbf{y} - \mathbf{X} \hat{\boldsymbol{\beta}})
\end{equation*}\]</span></p>
</div>
<div class="section level3">
<h3 id="sec:fitted-lm">
<code>fitted()</code><a class="anchor" aria-label="anchor" href="#sec:fitted-lm"></a>
</h3>
<p>Fitted values can be obtained for the response, spatial random
errors, and random effects. The fitted values for the response
(<code>type = "response"</code>), denoted <span class="math inline">\(\mathbf{\hat{y}}\)</span>, are given by <span class="math display">\[\begin{equation*}\label{eq:fit_resp}
  \mathbf{\hat{y}} = \mathbf{X} \boldsymbol{\hat{\beta}} .
\end{equation*}\]</span> They are the estimated mean response given the
set of explanatory variables for each observation.</p>
<p>Fitted values for spatial random errors (<code>type = "spcov"</code>)
and random effects (<code>type = "randcov"</code>) are linked to best
linear unbiased predictors from linear mixed model theory. Consider the
standard random effects parameterization <span class="math display">\[\begin{equation*}
  \mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \mathbf{Z} \mathbf{u} +
\boldsymbol{\epsilon},
\end{equation*}\]</span> where <span class="math inline">\(\mathbf{Z}\)</span> denotes the random effects
design matrix, <span class="math inline">\(\mathbf{u}\)</span> denotes
the random effects, and <span class="math inline">\(\boldsymbol{\epsilon}\)</span> denotes independent
random error. <span class="citation">Henderson (1975)</span> states that
the best linear unbiased predictor (BLUP) of a single random effect
vector <span class="math inline">\(\mathbf{u}\)</span>, denoted <span class="math inline">\(\mathbf{\hat{u}}\)</span>, is given by <span class="math display">\[\begin{equation}\label{eq:blup_mm}
  \mathbf{\hat{u}} = \sigma^2_u \mathbf{Z}^\top
\mathbf{\Sigma}^{-1}(\mathbf{y} - \mathbf{X} \boldsymbol{\hat{\beta}}),
\end{equation}\]</span> where <span class="math inline">\(\sigma^2_u\)</span> is the variance of <span class="math inline">\(\mathbf{u}\)</span>.</p>
<p><span class="citation">Searle, Casella, and McCulloch (2009)</span>
generalize this idea by showing that for a random vector <span class="math inline">\(\boldsymbol{\alpha}\)</span> in a linear model,
the best linear unbiased predictor (based on the response, <span class="math inline">\(\mathbf{y}\)</span>) of <span class="math inline">\(\boldsymbol{\alpha}\)</span>, denoted <span class="math inline">\(\boldsymbol{\hat{\alpha}}\)</span>, is given by
<span class="math display">\[\begin{equation}\label{eq:blup_gen}
  \boldsymbol{\hat{\alpha}} = \text{E}(\boldsymbol{\alpha}) +
\boldsymbol{\Sigma}_\alpha \boldsymbol{\Sigma}^{-1}(\mathbf{y} -
\mathbf{X} \boldsymbol{\hat{\beta}}),
\end{equation}\]</span> where <span class="math inline">\(\boldsymbol{\Sigma}_\alpha =
\text{Cov}(\boldsymbol{\alpha}, \mathbf{y})\)</span>. Evaluating this
equation at the plug-in (empirical) estimates of the covariance
parameters yields the empirical best linear unbiased predictor (EBLUP)
of <span class="math inline">\(\boldsymbol{\alpha}\)</span>.</p>
<p>Recall that the spatial linear model with random effects is <span class="math display">\[\begin{equation*}
  \mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \mathbf{Z} \mathbf{u} +
\boldsymbol{\tau} + \boldsymbol{\epsilon},
\end{equation*}\]</span> Building from previous results, we can find
BLUPs for each random term in the spatial linear model (<span class="math inline">\(\mathbf{u}\)</span>, <span class="math inline">\(\boldsymbol{\tau}\)</span>, and <span class="math inline">\(\boldsymbol{\epsilon}\)</span>). For example, the
BLUP of <span class="math inline">\(\mathbf{u}\)</span> is found by
noting that <span class="math inline">\(\text{E}(\mathbf{u}) =
\mathbf{0}\)</span> and <span class="math display">\[\begin{equation*}
  \mathbf{\Sigma}_u = \text{Cov}(\mathbf{u}, \mathbf{y}) =
\text{Cov}(\mathbf{u}, \mathbf{X} \boldsymbol{\beta} + \mathbf{Z}
\mathbf{u} + \boldsymbol{\tau} + \boldsymbol{\epsilon}) =
\text{Cov}(\mathbf{u}, \mathbf{Z}\mathbf{u}) = \text{Cov}(\mathbf{u},
\mathbf{u})\mathbf{Z}^\top = \sigma^2_u \mathbf{Z}^\top,
\end{equation*}\]</span> where the result follows because the random
terms in <span class="math inline">\(\mathbf{y}\)</span> are independent
and <span class="math inline">\(\text{Cov}(\mathbf{u}, \mathbf{u}) =
\sigma^2_u \mathbf{I}\)</span>. Then it follows that <span class="math display">\[\begin{equation*}
  \hat{\mathbf{u}} = \text{E}(\mathbf{u}) + \boldsymbol{\Sigma}_u
\boldsymbol{\Sigma}^{-1}(\mathbf{y} - \mathbf{X}
\boldsymbol{\hat{\beta}}) = \sigma^2_u \mathbf{Z}^\top
\boldsymbol{\Sigma}^{-1}(\mathbf{y} - \mathbf{X}
\boldsymbol{\hat{\beta}}),
\end{equation*}\]</span> which matches the previous form of the BLUP.
Similarly, the BLUP of <span class="math inline">\(\boldsymbol{\tau}\)</span> is found by noting that
<span class="math inline">\(\text{E}(\boldsymbol{\tau}) =
\mathbf{0}\)</span> and <span class="math display">\[\begin{equation*}
  \mathbf{\Sigma}_{de} = \text{Cov}(\boldsymbol{\tau}, \mathbf{y}) =
\text{Cov}(\boldsymbol{\tau}, \mathbf{X} \boldsymbol{\beta} + \mathbf{Z}
\mathbf{u} + \boldsymbol{\tau} + \boldsymbol{\epsilon}) =
\text{Cov}(\boldsymbol{\tau}, \boldsymbol{\tau}) = \sigma^2_{de}
\mathbf{R},
\end{equation*}\]</span> where the result follows because the random
terms in <span class="math inline">\(\mathbf{y}\)</span> are independent
and <span class="math inline">\(\text{Cov}(\boldsymbol{\tau},
\boldsymbol{\tau}) = \sigma^2_{de} \mathbf{R}\)</span>, and <span class="math inline">\(\sigma^2_{de}\)</span> is the variance of <span class="math inline">\(\boldsymbol{\tau}\)</span>. Then it follows that
<span class="math display">\[\begin{equation}\label{eq:blup_sp}
  \hat{\boldsymbol{\tau}} = \text{E}(\boldsymbol{\tau}) +
\boldsymbol{\Sigma}_{de} \boldsymbol{\Sigma}^{-1}(\mathbf{y} -
\mathbf{X} \boldsymbol{\hat{\beta}}) = \sigma^2_{de} \mathbf{R}
\boldsymbol{\Sigma}^{-1}(\mathbf{y} - \mathbf{X}
\boldsymbol{\hat{\beta}}).
\end{equation}\]</span> Fitted values for <span class="math inline">\(\boldsymbol{\epsilon}\)</span> are obtained using
similar arguments. Evaluating these equations at the plug-in (empirical)
estimates of the covariance parameters yields EBLUPs.</p>
<p>When partition factors are used, the covariance matrix of all random
effects (spatial and non-spatial) can be viewed as the interaction
between the non-partitioned covariance matrix and the partition matrix,
<span class="math inline">\(\mathbf{P}\)</span>. The <span class="math inline">\(ij\)</span>th entry in <span class="math inline">\(\mathbf{P}\)</span> equals one if observation
<span class="math inline">\(i\)</span> and observation <span class="math inline">\(j\)</span> share the same level of the partition
factor and zero otherwise. For spatial random effects, an adjustment is
straightforward, as each column in <span class="math inline">\(\boldsymbol{\Sigma}_{de}\)</span> corresponds to a
distinct spatial random effect. Thus with partition factors, <span class="math inline">\(\boldsymbol{\Sigma}_{de}^* =
\boldsymbol{\Sigma}_{de} \odot \mathbf{P} = \sigma^2_{de} \mathbf{R}
\odot \mathbf{P}\)</span>, where <span class="math inline">\(\odot\)</span> denotes the Hadmart (element-wise)
product, is used instead of <span class="math inline">\(\boldsymbol{\Sigma}_{de}\)</span>. Note that <span class="math inline">\(\boldsymbol{\Sigma}_{ie}\)</span> is unchanged as
it is proportional to the identity matrix. For non-spatial random
effects, however, the situation is more complicated. Applying the BLUP
formula directly yields BLUPs of random effects corresponding to the
interaction between random effect levels and partition levels. Thus a
logical approach is to average the non-zero BLUPs for each random effect
level across partition levels, yielding a prediction for the random
effect level. This does not imply, however, that these estimates are
BLUPs of the random effect.</p>
<p>For big data without partition factors, the local indexes act as
partition factors. That is, the BLUPs correspond to random effects
interacted with each local index. For big data with partition factors,
an adjusted partition factor is created as the interaction between each
local index and the partition factor. Then this adjusted partition
factor is applied to yield <span class="math inline">\(\hat{\boldsymbol{\alpha}}\)</span>.</p>
</div>
<div class="section level3">
<h3 id="sec:hatvalues-lm">
<code>hatvalues()</code><a class="anchor" aria-label="anchor" href="#sec:hatvalues-lm"></a>
</h3>
<p>Hat values measure the leverage of an observation. An observation has
high leverage if its combination of explanatory variables is atypical
(far from the mean explanatory vector). The spatial leverage (hat)
matrix is given by <span class="math display">\[\begin{equation}
\label{eq:leverage}
\mathbf{H}_s = \mathbf{X}^* (\mathbf{X}^{* \top} \mathbf{X}^*)^{-1}
\mathbf{X}^{* \top}.
\end{equation}\]</span> The diagonal of this matrix yields the leverage
(hat) values for each observation <span class="citation">(Montgomery,
Peck, and Vining 2021)</span>. The larger the hat value, the larger the
leverage.</p>
<p>To better understand the previous form, recall that the the
non-spatial linear model <span class="math inline">\(\mathbf{y} =
\mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}\)</span> assumes
elements of <span class="math inline">\(\boldsymbol{\epsilon}\)</span>
are independent and identically distributed (iid) with constant
variance. In this context, the leverage (hat) matrix is given by <span class="math display">\[\begin{equation*}
\mathbf{H} \equiv \mathbf{X} (\mathbf{X}^{\top} \mathbf{X})^{-1}
\mathbf{X}^{\top},
\end{equation*}\]</span> When the elements of <span class="math inline">\(\boldsymbol{\epsilon}\)</span> are not iid or do
not have constant variance or both, the spatial leverage (hat) matrix is
not <span class="math inline">\(\mathbf{H}\)</span>. First the linear
model must be whitened according to <span class="math inline">\(\mathbf{y}^* = \mathbf{X}^* \boldsymbol{\beta} +
\boldsymbol{\epsilon}^*\)</span>, where <span class="math inline">\(\boldsymbol{\epsilon}^*\)</span> is the whitened
version of the sum of all random errors in the model. Then the spatial
leverage (hat) matrix follows using <span class="math inline">\(\mathbf{X}^*\)</span>, the whitened version of
<span class="math inline">\(\mathbf{X}\)</span>.</p>
</div>
<div class="section level3">
<h3 id="sec:loglik-lm">
<code>logLik()</code><a class="anchor" aria-label="anchor" href="#sec:loglik-lm"></a>
</h3>
<p>The log-likelihood is given by <span class="math inline">\(\ell(\boldsymbol{\hat{\Theta}})\)</span>.</p>
</div>
<div class="section level3">
<h3 id="sec:loocv-lm">
<code>loocv()</code><a class="anchor" aria-label="anchor" href="#sec:loocv-lm"></a>
</h3>
<p><span class="math inline">\(k\)</span>-fold cross validation is a
useful tool for evaluating model fits using “hold-out” data. The data
are split into <span class="math inline">\(k\)</span> sets. One-by-one,
one of the <span class="math inline">\(k\)</span> sets is held out, the
model is fit to the remaining <span class="math inline">\(k - 1\)</span>
sets, and predictions at each observation in the hold-out set are
compared to their true values. The closer the predictions are to the
true observations, the better the model fit. A special case where <span class="math inline">\(k = n\)</span> is known as leave-one-out cross
validation (loocv), as each observation is left out one-by-one.
Computationally efficient solutions exist for leave-one-out cross
validation in the non-spatial linear model (with iid, constant variance
errors). Outside of this case, however, fitting <span class="math inline">\(n\)</span> separate models can be computationally
infeasible. <code><a href="../reference/loocv.html">loocv()</a></code> makes a compromise that balances an
approximation to the true solution with computational feasibility. First
<span class="math inline">\(\boldsymbol{\theta}\)</span> is estimated
using all of the data. Then for each of the <span class="math inline">\(n\)</span> model fits, <code><a href="../reference/loocv.html">loocv()</a></code> does
not re-estimate <span class="math inline">\(\boldsymbol{\theta}\)</span>
but does re-estimate <span class="math inline">\(\boldsymbol{\beta}\)</span>. This approach relies
on the assumption that the covariance parameter estimates obtained using
<span class="math inline">\(n - 1\)</span> observations are
approximately the same as the covariance parameter estimates obtained
using all <span class="math inline">\(n\)</span> observations. For a
large enough sample size, this is a reasonable assumption.</p>
<p>First define <span class="math inline">\(\boldsymbol{\Sigma}_{-i,
-i}\)</span> as <span class="math inline">\(\boldsymbol{\Sigma}\)</span>
with the <span class="math inline">\(i\)</span>th row and column
deleted, <span class="math inline">\(\boldsymbol{\Sigma}_{i,
-i}\)</span> as the <span class="math inline">\(i\)</span>th row of
<span class="math inline">\(\boldsymbol{\Sigma}\)</span> with the <span class="math inline">\(i\)</span>th column deleted, <span class="math inline">\(\boldsymbol{\Sigma}_{i, i}\)</span> as the <span class="math inline">\(i\)</span>th diagonal element of <span class="math inline">\(\boldsymbol{\Sigma}\)</span>, <span class="math inline">\(\mathbf{X}_{-i}\)</span> as <span class="math inline">\(\mathbf{X}\)</span> with the <span class="math inline">\(i\)</span>th row deleted, <span class="math inline">\(\mathbf{X}_{i}\)</span> as the <span class="math inline">\(i\)</span>th row of <span class="math inline">\(\mathbf{X}\)</span>, <span class="math inline">\(y_{-i}\)</span> as <span class="math inline">\(\mathbf{y}\)</span> with the <span class="math inline">\(i\)</span>th element deleted, and <span class="math inline">\(\mathbf{y}_i\)</span> as the <span class="math inline">\(i\)</span>th element of <span class="math inline">\(\mathbf{y}\)</span>. <span class="citation">Wolf
(1978)</span> shows that given <span class="math inline">\(\boldsymbol{\Sigma}^{-1}\)</span>, a
computationally efficient form for <span class="math inline">\(\boldsymbol{\Sigma}^{-1}_{-i}\)</span> exists.
First observe that <span class="math inline">\(\boldsymbol{\Sigma}^{-1}\)</span> can be
represented blockwise as <span class="math display">\[\begin{equation*}
\boldsymbol{\Sigma}^{-1} =
\begin{bmatrix}
  \tilde{\boldsymbol{\Sigma}}_{-i, -i} &amp;
\tilde{\boldsymbol{\Sigma}}_{i,-i}^\top \\
  \tilde{\boldsymbol{\Sigma}}_{i,-i} &amp;
\tilde{\boldsymbol{\Sigma}}_{i, i}
\end{bmatrix},
\end{equation*}\]</span> where the dimensions of each <span class="math inline">\(\tilde{\boldsymbol{\Sigma}}\)</span> match the
respective dimensions of relevant blocks in <span class="math inline">\(\boldsymbol{\Sigma}\)</span>. Then it follows that
<span class="math display">\[\begin{equation*}
\boldsymbol{\Sigma}^{-1}_{-i, -i} = \tilde{\boldsymbol{\Sigma}}_{-i, -i}
- \tilde{\boldsymbol{\Sigma}}_{i,-i}^\top
\tilde{\boldsymbol{\Sigma}}_{i,
i}^{-1}\tilde{\boldsymbol{\Sigma}}_{i,-i}
\end{equation*}\]</span> and <span class="math display">\[\begin{equation*}
  \boldsymbol{\beta}_{-i} = (\mathbf{X}^\top_{-i}
\boldsymbol{\Sigma}^{-1}_{-i, -i} \mathbf{X}_{-i})^{-1}
\mathbf{X}^\top_{-i} \boldsymbol{\Sigma}^{-1}_{-i, -i} \mathbf{y}_{-i},
\end{equation*}\]</span> where <span class="math inline">\(\boldsymbol{\beta}_i\)</span> is the estimate of
<span class="math inline">\(\boldsymbol{\beta}\)</span> constructed
without the <span class="math inline">\(i\)</span>th observation.</p>
<p>The loocv prediction of <span class="math inline">\(y_i\)</span> is
then given by <span class="math display">\[\begin{equation*}
  \hat{y}_i = \mathbf{X}_i \hat{\boldsymbol{\beta}}_{-i} +
\hat{\boldsymbol{\Sigma}}_{i, -i}\hat{\boldsymbol{\Sigma}}_{-i,
-i}(\mathbf{y}_i - \mathbf{X}_{-i} \hat{\boldsymbol{\beta}}_{-i})
\end{equation*}\]</span> and the prediction variance of the loocv
prediction of <span class="math inline">\(y_i\)</span> is given by <span class="math display">\[\begin{equation*}
  \dot{\sigma}^2_i = \hat{\boldsymbol{\Sigma}}_{i, i} -
\hat{\boldsymbol{\Sigma}}_{i, - i} \hat{\boldsymbol{\Sigma}}^{-1}_{-i,
-i} \hat{\boldsymbol{\Sigma}}_{i, - i}^\top +
\mathbf{Q}_i(\mathbf{X}_{-i}^\top \hat{\boldsymbol{\Sigma}}_{-i,
-i}^{-1} \mathbf{X}_{-i})^{-1}\mathbf{Q}_i^\top ,
\end{equation*}\]</span> where <span class="math inline">\(\mathbf{Q}_i
= \mathbf{X}_i - \hat{\boldsymbol{\Sigma}}_{i, -i}
\hat{\boldsymbol{\Sigma}}^{-1}_{-i, -i} \mathbf{X}_{-i}\)</span>. These
formulas are analogous to the formulas used to obtain linear unbiased
predictions of unobserved data and prediction variances. Model fits are
evaluated using several statistics: bias, mean-squared-prediction error
(MSPE), root-mean-squared-prediction error (RMSPE), and the squared
correlation (cor2) between the observed data and leave-one-out
predictions (regarded as a prediction version of r-squared appropriate
for comparing across spatial and nonspatial models).</p>
<p>Bias is formally defined as <span class="math display">\[\begin{equation*}
bias = \frac{1}{n}\sum_{i = 1}^n(y_i - \hat{y}_i).
\end{equation*}\]</span></p>
<p>MSPE is formally defined as <span class="math display">\[\begin{equation*}
MSPE = \frac{1}{n}\sum_{i = 1}^n(y_i - \hat{y}_i)^2.
\end{equation*}\]</span></p>
<p>RMSPE is formally defined as <span class="math display">\[\begin{equation*}
RMSPE = \sqrt{\frac{1}{n}\sum_{i = 1}^n(y_i - \hat{y}_i)^2}.
\end{equation*}\]</span></p>
<p>cor2 is formally defined as <span class="math display">\[\begin{equation*}
cor2 = \text{Cor}(\mathbf{y}, \hat{\mathbf{y}})^2,
\end{equation*}\]</span> where Cor<span class="math inline">\((\cdot)\)</span> is the correlation function. cor2
is only returned for spatial linear models, as it is not applicable for
spatial generalized linear models (we are predicting a latent mean
parameter, which is unknown and not on the same scale as the original
data).</p>
<p>Generally, bias should be near zero for well-fitting models. The
lower the MSPE and RMSPE, the better the model fit. The higher the cor2,
the better the model fit.</p>
<div class="section level4">
<h4 id="sec:bigdata-loocv-lm">Big Data<a class="anchor" aria-label="anchor" href="#sec:bigdata-loocv-lm"></a>
</h4>
<p>Options for big data leave-one-out cross validation rely on the
<code>local</code> argument, which is passed to <code><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict()</a></code>.
The <code>local</code> list for <code><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict()</a></code> is explained in
detail in the <code><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict()</a></code> section, but we provide a short
summary of how <code>local</code> interacts with <code><a href="../reference/loocv.html">loocv()</a></code>
here.</p>
<p>For <code><a href="../reference/splm.html">splm()</a></code> and <code><a href="../reference/spautor.html">spautor()</a></code> objects, the
<code>local</code> method can be <code>"all"</code>. When the
<code>local</code> method is<code>"all"</code>, all of the data are used
for leave-one-out cross validation (i.e., it is implemented exactly as
previously described). Parallelization is implemented when setting
<code>parallel = TRUE</code> in <code>local</code>, and the number of
cores to use for parallelization is specified via
<code>ncores</code>.</p>
<p>For <code><a href="../reference/splm.html">splm()</a></code> objects, additional options for the
<code>local</code> method are <code>"covariance"</code> and
<code>"distance"</code>. When the <code>local</code> method is
<code>"covariance"</code>, then a number of observations (specified via
the <code>size</code> argument) having the highest covariance with the
held-out observation are used in the local neighborhood prediction
approach. When the <code>local</code> method is <code>"distance"</code>,
then a number of observations (specified via the <code>size</code>
argument) closest to the held-out observation are used in the local
neighborhood prediction approach. When no random effects are used, no
partition factor is used, and the spatial covariance function is
monotone decreasing, <code>"covariance"</code> and
<code>"distance"</code> are equivalent. The local neighborhood approach
only uses the observations in the local neighborhood of the held-out
observation to perform prediction, and is thus an approximation to the
true solution. Its computational efficiency derives from using <span class="math inline">\(\boldsymbol{\Sigma}_{l, l}\)</span> (the
covariance matrix of the observations in the local neighborhood) instead
of <span class="math inline">\(\boldsymbol{\Sigma}\)</span> (the
covariance matrix of all the observations). Parallelization is
implemented when setting <code>parallel = TRUE</code> in
<code>local</code>, and the number of cores to use for parallelization
is specified via <code>ncores</code>.</p>
</div>
</div>
<div class="section level3">
<h3 id="sec:predict-lm">
<code>predict()</code><a class="anchor" aria-label="anchor" href="#sec:predict-lm"></a>
</h3>
<div class="section level4">
<h4 id="sec:predict-none-lm">
<code>interval = "none"</code><a class="anchor" aria-label="anchor" href="#sec:predict-none-lm"></a>
</h4>
<p>The empirical best linear unbiased predictions (i.e., empirical
Kriging predictor) of <span class="math inline">\(\mathbf{y}_u\)</span>
are given by <span class="math display">\[\begin{equation}\label{eq:blup}
  \mathbf{\dot{y}}_u =  \mathbf{X}_u \hat{\boldsymbol{\beta}} +
\hat{\boldsymbol{\Sigma}}_{uo} \hat{\boldsymbol{\Sigma}}^{-1}_{o}
(\mathbf{y}_o - \mathbf{X}_o \hat{\boldsymbol{\beta}}) .
\end{equation}\]</span></p>
<p>This equation sometimes called an empirical universal Kriging
predictor, a Kriging with external drift predictor, or a regression
Kriging predictor.</p>
<p>The covariance matrix of <span class="math inline">\(\mathbf{\dot{y}}_u\)</span> <span class="math display">\[\begin{equation}\label{eq:blup_cov}
  \dot{\boldsymbol{\Sigma}}_u = \hat{\boldsymbol{\Sigma}}_u -
\hat{\boldsymbol{\Sigma}}_{uo} \hat{\boldsymbol{\Sigma}}^{-1}_o
\hat{\boldsymbol{\Sigma}}^\top_{uo} + \mathbf{Q}(\mathbf{X}_o^\top
\hat{\boldsymbol{\Sigma}}_o^{-1} \mathbf{X}_o)^{-1}\mathbf{Q}^\top ,
\end{equation}\]</span> where <span class="math inline">\(\mathbf{Q} =
\mathbf{X}_u - \hat{\boldsymbol{\Sigma}}_{uo}
\hat{\boldsymbol{\Sigma}}^{-1}_o \mathbf{X}_o\)</span>.</p>
<p>When <code>se.fit = TRUE</code>, standard errors are returned by
taking the square root of the diagonal of <span class="math inline">\(\dot{\boldsymbol{\Sigma}}_u\)</span>.</p>
</div>
<div class="section level4">
<h4 id="sec:predict-predict-lm">
<code>interval = "prediction"</code><a class="anchor" aria-label="anchor" href="#sec:predict-predict-lm"></a>
</h4>
<p>The empirical best linear unbiased predictions are returned as <span class="math inline">\(\mathbf{\dot{y}}_u\)</span>. The (100 <span class="math inline">\(\times\)</span> <code>level</code>)% prediction
interval for <span class="math inline">\((y_u)_i\)</span> is <span class="math inline">\((\dot{y}_u)_i \pm z^*
\sqrt{(\dot{\boldsymbol{\Sigma}}_u)_{i, i}}\)</span>, where <span class="math inline">\(\sqrt{(\dot{\boldsymbol{\Sigma}}_u)_{i,
i}}\)</span> is the standard error of <span class="math inline">\((\dot{y}_u)_i\)</span> obtained from
<code>se.fit = TRUE</code>, <span class="math inline">\(\Phi(z^*) = 1 -
\alpha / 2\)</span>, <span class="math inline">\(\Phi(\cdot)\)</span> is
the standard normal (Gaussian) cumulative distribution function, <span class="math inline">\(\alpha = 1 -\)</span> <code>level</code>, and
<code>level</code> is an argument to <code><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict()</a></code>. The default
for <code>level</code> is 0.95, which corresponds to a <span class="math inline">\(z^*\)</span> of approximately 1.96.</p>
</div>
<div class="section level4">
<h4 id="sec:predict-conf-lm">
<code>interval = "confidence"</code><a class="anchor" aria-label="anchor" href="#sec:predict-conf-lm"></a>
</h4>
<p>The best linear unbiased estimates of <span class="math inline">\(\text{E}[(y_u)_i]\)</span> (<span class="math inline">\(\text{E}(\cdot)\)</span> denotes expectation) are
returned by evaluating <span class="math inline">\((\mathbf{X}_u)_i
\hat{\boldsymbol{\beta}}\)</span>, where <span class="math inline">\((\mathbf{X}_u)_i\)</span> is the <span class="math inline">\(i\)</span>th row of <span class="math inline">\(\mathbf{X}_u\)</span> (i.e., fitted values
corresponding to <span class="math inline">\((\mathbf{X}_u)_i\)</span>
are returned). The (100 <span class="math inline">\(\times\)</span>
<code>level</code>)% confidence interval for <span class="math inline">\(\text{E}[(y_u)_i]\)</span> is <span class="math inline">\((\mathbf{X}_u)_i \hat{\boldsymbol{\beta}} \pm z^*
\sqrt{(\mathbf{X}_u)_i (\mathbf{X}^\top_o
\hat{\boldsymbol{\Sigma}}_o^{-1} \mathbf{X}_o)^{-1}
(\mathbf{X}_u)_i^\top}\)</span> where <span class="math inline">\(\sqrt{(\mathbf{X}_u)_i (\mathbf{X}^\top_o
\hat{\boldsymbol{\Sigma}}_o^{-1} \mathbf{X}_o)^{-1}
(\mathbf{X}_u)_i^\top}\)</span> is the standard error of <span class="math inline">\((\dot{y}_u)_i\)</span> obtained from
<code>se.fit = TRUE</code>, <span class="math inline">\(\Phi(z^*) = 1 -
\alpha / 2\)</span>, <span class="math inline">\(\Phi(\cdot)\)</span> is
the standard normal (Gaussian) cumulative distribution function, <span class="math inline">\(\alpha = 1 -\)</span> <code>level</code>, and
<code>level</code> is an argument to <code><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict()</a></code>. The default
for <code>level</code> is 0.95, which corresponds to a <span class="math inline">\(z^*\)</span> of approximately 1.96.</p>
</div>
<div class="section level4">
<h4 id="sec:predict-spautor-lm">
<code>spautor()</code> extra steps<a class="anchor" aria-label="anchor" href="#sec:predict-spautor-lm"></a>
</h4>
<p>For spatial autoregressive models, an extra step is required to
obtain <span class="math inline">\(\hat{\boldsymbol{\Sigma}}^{-1}_o\)</span>, <span class="math inline">\(\hat{\boldsymbol{\Sigma}}_u\)</span>, and <span class="math inline">\(\hat{\boldsymbol{\Sigma}}_{uo}\)</span> as they
depend on one another through the neighborhood structure of <span class="math inline">\(\mathbf{y}_o\)</span> and <span class="math inline">\(\mathbf{y}_u\)</span>. Recall that for
autoregressive models, it is <span class="math inline">\(\boldsymbol{\Sigma}^{-1}\)</span> that is
straightforward to obtain, not <span class="math inline">\(\boldsymbol{\Sigma}\)</span>.</p>
<p>Let <span class="math inline">\(\boldsymbol{\Sigma}^{-1}\)</span> be
the inverse covariance matrix of the observed and unobserved data, <span class="math inline">\(\mathbf{y}_o\)</span> and <span class="math inline">\(\mathbf{y}_u\)</span>. One approach to obtain
<span class="math inline">\(\boldsymbol{\Sigma}_o\)</span> and <span class="math inline">\(\boldsymbol{\Sigma}_{uo}\)</span> is to directly
invert <span class="math inline">\(\boldsymbol{\Sigma}^{-1}\)</span> and
then subset <span class="math inline">\(\boldsymbol{\Sigma}\)</span>
appropriately. This inversion can be prohibitive when <span class="math inline">\(n_o + n_u\)</span> is large. A faster way to
obtain <span class="math inline">\(\boldsymbol{\Sigma}_o\)</span> and
<span class="math inline">\(\boldsymbol{\Sigma}_{uo}\)</span> exists.
Represent <span class="math inline">\(\boldsymbol{\Sigma}^{-1}\)</span>
blockwise as <span class="math display">\[\begin{equation*}\label{eq:auto_hw}
  \boldsymbol{\Sigma}^{-1} =
  \begin{bmatrix}
    \tilde{\boldsymbol{\Sigma}}_{o} &amp;
\tilde{\boldsymbol{\Sigma}}^{\top}_{uo} \\
    \tilde{\boldsymbol{\Sigma}}_{uo} &amp;
\tilde{\boldsymbol{\Sigma}}_{u}
  \end{bmatrix},
\end{equation*}\]</span> where the dimensions of the blocks match the
relevant dimensions of <span class="math inline">\(\boldsymbol{\Sigma}\)</span>. All of the terms
required for prediction can be obtained from this block representation.
<span class="citation">Wolf (1978)</span> shows that <span class="math display">\[\begin{equation*}\label{eq:hw_forms}
  \begin{split}
    \boldsymbol{\Sigma}^{-1}_o &amp; = \tilde{\boldsymbol{\Sigma}}_{o} -
\tilde{\boldsymbol{\Sigma}}^{ \top}_{uo}
(\tilde{\boldsymbol{\Sigma}}_{u})^{-1} \tilde{\boldsymbol{\Sigma}}_{uo}
\\
    \boldsymbol{\Sigma}_u &amp; = (\tilde{\boldsymbol{\Sigma}}_{u} -
\tilde{\boldsymbol{\Sigma}}_{uo} (\tilde{\boldsymbol{\Sigma}}_{o})^{-1}
\tilde{\boldsymbol{\Sigma}}^\top_{uo})^{-1} \\
    \boldsymbol{\Sigma}_{uo} &amp; = - \boldsymbol{\Sigma}_u
\tilde{\boldsymbol{\Sigma}}_{uo} \tilde{\boldsymbol{\Sigma}}^{-1}_{o}
  \end{split}
\end{equation*}\]</span> Evaluating these expressions at <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span> yields <span class="math inline">\(\hat{\boldsymbol{\Sigma}}^{-1}_o\)</span>, and
<span class="math inline">\(\hat{\boldsymbol{\Sigma}}_u\)</span>, and
<span class="math inline">\(\hat{\boldsymbol{\Sigma}}_{uo}\)</span>.</p>
<p>A similar result exists for the log determinant of <span class="math inline">\(\boldsymbol{\Sigma}_o\)</span>, which is not
required for prediction but is required for restricted maximum
likelihood and maximum likelihood estimation.</p>
</div>
<div class="section level4">
<h4 id="sec:predict-bigdata-lm">Big Data<a class="anchor" aria-label="anchor" href="#sec:predict-bigdata-lm"></a>
</h4>
<p>When the number of observations in the fitted model (observed data)
are large or there are many locations to predict or both, it is often
necessary to implement computationally efficient big data
approximations. Big data approximations are implemented in
<code>spmodel</code> using the <code>local</code> argument to
<code><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict()</a></code>. When the <code>local</code> method is
<code>"all"</code>, all of the fitted model data are used to make
predictions. In this context, computational efficiency is only gained by
parallelizing each prediction. The only available <code>local</code>
method for <code><a href="../reference/spautor.html">spautor()</a></code> fitted models is <code>"all"</code>.
This is because the neighborhood structure of <code><a href="../reference/spautor.html">spautor()</a></code>
fitted models does not permit the subsetting used by the
<code>"covariance"</code> and <code>"distance"</code> methods that we
discuss next.</p>
<p>When the <code>local</code> method is <code>"covariance"</code>,
<span class="math inline">\(\hat{\boldsymbol{\Sigma}}_{uo}\)</span> is
computed between the observation being predicted (<span class="math inline">\(\mathbf{y}_u\)</span>) and the rest of the
observed data. This vector is then ordered and a number of observations
(specified via the <code>size</code> argument) having the highest
covariance with <span class="math inline">\(\mathbf{y}_u\)</span> are
subset, yielding <span class="math inline">\(\check{\boldsymbol{\Sigma}}_{uo}\)</span>, which
has dimension <span class="math inline">\(1 \times size\)</span>. Then
similarly <span class="math inline">\(\hat{\boldsymbol{\Sigma}}_o\)</span>, <span class="math inline">\(\mathbf{y}_o\)</span>, and <span class="math inline">\(\mathbf{X}_u\)</span> are also subset by these
<code>size</code> observations, yielding <span class="math inline">\(\check{\boldsymbol{\Sigma}}_{o}\)</span>, <span class="math inline">\(\check{\mathbf{y}}_o\)</span>, and <span class="math inline">\(\check{\mathbf{X}}_u\)</span>, respectively. The
previous prediction equations can be evaluated at <span class="math inline">\(\check{\boldsymbol{\Sigma}}_{uo}\)</span>, <span class="math inline">\(\check{\boldsymbol{\Sigma}}_{o}\)</span>, <span class="math inline">\(\check{\mathbf{y}}_o\)</span>, and <span class="math inline">\(\check{\mathbf{X}}_u\)</span> (except for the
quantity <span class="math inline">\((\mathbf{X}_o^\top
\hat{\boldsymbol{\Sigma}}_o^{-1} \mathbf{X}_o)^{-1}\)</span>, which is
evaluated using all the observed data) to yield predictions and standard
errors. When the <code>local</code> method is <code>"distance"</code>, a
similar approach is used except a number of observations (specified via
the <code>size</code> argument) closest (in terms of Euclidean distance)
to <span class="math inline">\(\mathbf{y}_u\)</span> are subset instead.
When random effects are not used, partition factors are not used, and
the spatial covariance function is monotone decreasing,
<code>"covariance"</code> and <code>"distance"</code> are equivalent.
This approach of subsetting the observed data by the set of locations
closest in covariance or proximity to <span class="math inline">\(\mathbf{y}_u\)</span> is known as the local
neighborhood approach. As long as <code>size</code> is relatively small
(the default is 100), the local neighborhood approach is very
computationally efficient, mainly because <span class="math inline">\(\check{\boldsymbol{\Sigma}}_{o}^{-1}\)</span> is
easy to compute. Additional computational efficiency is gained by
parallelizing each prediction.</p>
</div>
<div class="section level4">
<h4 id="sec:rf_pred-lm">
<code>splmRF()</code> and <code>spautorRF()</code><a class="anchor" aria-label="anchor" href="#sec:rf_pred-lm"></a>
</h4>
<p>Random forest spatial residual model predictions are obtained by
combining random forest predictions and spatial linear model predictions
(i.e., Kriging) of the random forest residuals. Formally, the random
forest spatial residual model predictions of <span class="math inline">\(\mathbf{y}_u\)</span> are given by <span class="math display">\[\begin{equation*}
  \mathbf{\dot{y}}_u = \mathbf{\dot{y}}_{u, rf} + \mathbf{\dot{e}}_{u,
slm},
\end{equation*}\]</span> where <span class="math inline">\(\mathbf{\dot{y}}_{u, rf}\)</span> are the random
forest predictions for <span class="math inline">\(\mathbf{y}_u\)</span>
and <span class="math inline">\(\mathbf{\dot{e}}_{u, slm}\)</span> are
the spatial linear model predictions of the random forest residuals for
<span class="math inline">\(\mathbf{y}_u\)</span>. This process of
obtaining predictions is sometimes analogously called random forest
regression Kriging <span class="citation">(Fox, Ver Hoef, and Olsen
2020)</span>.</p>
<p>Uncertainty quantification in a random forest context has been
studied <span class="citation">(Meinshausen and Ridgeway 2006)</span>
but is not currently available in <code>spmodel</code>. Big data are
accommodated by supplying the <code>local</code> argument to
<code><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict()</a></code>.</p>
</div>
</div>
<div class="section level3">
<h3 id="sec:pr2-lm">
<code>pseudoR2()</code><a class="anchor" aria-label="anchor" href="#sec:pr2-lm"></a>
</h3>
<p>The pseudo R-squared is a generalization of the classical R-squared
from non-spatial linear models. Like the classical R-squared, the pseudo
R-squared measures the proportion of variability in the response
explained by the fixed effects in the fitted model. Unlike the classical
R-squared, the pseudo R-squared can be applied to models whose errors do
not satisfy the iid and constant variance assumption. The pseudo
R-squared is given by <span class="math display">\[\begin{equation*}
PR2 = 1 -
\frac{\mathcal{D}(\boldsymbol{\hat{\Theta}})}{\mathcal{D}(\boldsymbol{\hat{\Theta}}_0)}.
\end{equation*}\]</span> For normal (Gaussian) random errors, the pseudo
R-squared is <span class="math display">\[\begin{equation*}
PR2 = 1 - \frac{(\mathbf{y} - \mathbf{X} \hat{\boldsymbol{\beta}})^\top
\hat{\boldsymbol{\Sigma}}^{-1}(\mathbf{y} - \mathbf{X}
\hat{\boldsymbol{\beta}})}{(\mathbf{y} - \hat{\mu})^\top
\hat{\boldsymbol{\Sigma}}^{-1}(\mathbf{y} - \hat{\mu})},
\end{equation*}\]</span> where <span class="math inline">\(\hat{\mu} =
(\boldsymbol{1}^\top \hat{\boldsymbol{\Sigma}}^{-1} \boldsymbol{1})^{-1}
\boldsymbol{1}^\top \hat{\boldsymbol{\Sigma}}^{-1} \mathbf{y}\)</span>.
For the non-spatial model, the pseudo R-squared reduces to the classical
R-squared, as <span class="math display">\[\begin{equation*}
PR2 = 1 - \frac{(\mathbf{y} - \mathbf{X} \hat{\boldsymbol{\beta}})^\top
\hat{\boldsymbol{\Sigma}}^{-1}(\mathbf{y} - \mathbf{X}
\hat{\boldsymbol{\beta}})}{(\mathbf{y} - \hat{\mu})^\top
\hat{\boldsymbol{\Sigma}}^{-1}(\mathbf{y} - \hat{\mu})}  = 1 -
\frac{(\mathbf{y} - \mathbf{X} \hat{\boldsymbol{\beta}})^\top
(\mathbf{y} - \mathbf{X} \hat{\boldsymbol{\beta}})}{(\mathbf{y} -
\hat{\mu})^\top (\mathbf{y} - \hat{\mu})} = 1 -
\frac{\text{SSE}}{\text{SST}} = R2,
\end{equation*}\]</span> where SSE denotes the error sum of squares and
SST denotes the total sum of squares. The result follows because for a
non-spatial model, <span class="math inline">\(\boldsymbol{\Sigma}\)</span> is proportional to
the identity matrix.</p>
<p>The adjusted pseudo r-squared adjusts for additional explanatory
variables and is given by <span class="math display">\[\begin{equation*}
  PR2adj = 1 - (1 - PR2)\frac{n - 1}{n - p}.
\end{equation*}\]</span> If the fitted model does not have an intercept,
the <span class="math inline">\(n - 1\)</span> term is instead <span class="math inline">\(n\)</span>.</p>
</div>
<div class="section level3">
<h3 id="sec:residuals-lm">
<code>residuals()</code><a class="anchor" aria-label="anchor" href="#sec:residuals-lm"></a>
</h3>
<p>Terminology regarding residual names is often conflicting and
confusing. Because of this, we explicitly define the residual options we
use in <code>spmodel</code>. These definitions may be different from
others you have seen in the literature.</p>
<p>When <code>type = "response"</code>, response residuals are returned:
<span class="math display">\[\begin{equation*}
\mathbf{e}_{r} = \mathbf{y} - \mathbf{X} \hat{\boldsymbol{\beta}}.
\end{equation*}\]</span></p>
<p>When <code>type = "pearson"</code>, Pearson residuals are returned:
<span class="math display">\[\begin{equation*}
\mathbf{e}_{p} = \hat{\boldsymbol{\Sigma}}^{-1/2}\mathbf{e}_{r},
\end{equation*}\]</span> If the errors are normal (Gaussian), the
Pearson residuals should be approximately normally distributed with mean
zero and variance one. The result follows when <span class="math inline">\(\hat{\boldsymbol{\Sigma}}^{-1/2} \approx
\boldsymbol{\Sigma}^{-1/2}\)</span> because <span class="math display">\[\begin{equation*}
  \text{E}(\boldsymbol{\Sigma}^{-1/2} \mathbf{e}_{r}) =
\boldsymbol{\Sigma}^{-1/2} \text{E}(\mathbf{e}_{r}) =
\boldsymbol{\Sigma}^{-1/2} \boldsymbol{0} = \boldsymbol{0}
\end{equation*}\]</span> and <span class="math display">\[\begin{equation*}
  \begin{split}
  \text{Cov}(\boldsymbol{\Sigma}^{-1/2} \mathbf{e}_{r}) &amp; =
\boldsymbol{\Sigma}^{-1/2} \text{Cov}(\mathbf{e}_{r})
\boldsymbol{\Sigma}^{-1/2} \\
  &amp; \approx \boldsymbol{\Sigma}^{-1/2} \boldsymbol{\Sigma}
\boldsymbol{\Sigma}^{-1/2} \\
  &amp; = (\boldsymbol{\Sigma}^{-1/2}
\boldsymbol{\Sigma}^{1/2})(\boldsymbol{\Sigma}^{1/2}
\boldsymbol{\Sigma}^{-1/2}) \\
  &amp; = \mathbf{I}
  \end{split}
\end{equation*}\]</span></p>
<p>When <code>type = "standardized"</code>, standardized residuals are
returned: <span class="math display">\[\begin{equation*}
\mathbf{e}_{s} = \mathbf{e}_{p} \odot \frac{1}{\sqrt{1 -
diag(\mathbf{H}^*)}},
\end{equation*}\]</span> where <span class="math inline">\(diag(\mathbf{H}^*)\)</span> is the diagonal of the
spatial hat matrix, <span class="math inline">\(\mathbf{H}_s \equiv
\mathbf{X}^* (\mathbf{X}^{* \top} \mathbf{X}^*)^{-1} \mathbf{X}^{*
\top}\)</span>, and <span class="math inline">\(\odot\)</span> denotes
the Hadmard (element-wise) product. This residual transformation
“standardizes” the Pearson residuals. As such, the standardized
residuals should also have mean zero and variance <span class="math display">\[\begin{equation*}
  \begin{split}
  \text{Cov}(\mathbf{e}_{s}) &amp; = \text{Cov}((\mathbf{I} -
\mathbf{H}^*) \hat{\boldsymbol{\Sigma}}^{-1/2}\mathbf{y}) \\
  &amp; \approx \text{Cov}((\mathbf{I} - \mathbf{H}^*)
\boldsymbol{\Sigma}^{-1/2}\mathbf{y}) \\
  &amp; = (\mathbf{I} - \mathbf{H}^*) \boldsymbol{\Sigma}^{-1/2}
\text{Cov}(\mathbf{y}) \boldsymbol{\Sigma}^{-1/2}(\mathbf{I} -
\mathbf{H}^*)^\top \\
  &amp; = (\mathbf{I} - \mathbf{H}^*) \boldsymbol{\Sigma}^{-1/2}
\boldsymbol{\Sigma} \boldsymbol{\Sigma}^{-1/2}(\mathbf{I} -
\mathbf{H}^*)^\top \\
  &amp; = (\mathbf{I} - \mathbf{H}^*) \mathbf{I} (\mathbf{I} -
\mathbf{H}^*)^\top \\
  &amp; = (\mathbf{I} - \mathbf{H}^*),
  \end{split}
\end{equation*}\]</span> because <span class="math inline">\((\mathbf{I}
- \mathbf{H}^*)\)</span> is symmetric and idempotent. Note that the
average value of <span class="math inline">\(diag(\mathbf{H}^*)\)</span>
is <span class="math inline">\(p / n\)</span>, so <span class="math inline">\((\mathbf{I} - \mathbf{H}^*) \approx
\mathbf{I}\)</span> for large sample sizes.</p>
</div>
<div class="section level3">
<h3 id="sec:spmod">
<code>spautor()</code> and <code>splm()</code><a class="anchor" aria-label="anchor" href="#sec:spmod"></a>
</h3>
<p>Next we discuss technical details for the <code><a href="../reference/spautor.html">spautor()</a></code> and
<code><a href="../reference/splm.html">splm()</a></code> functions. Many of the details for the two functions
are the same, though occasional differences are noted in the following
subsection headers. Specifically, <code><a href="../reference/spautor.html">spautor()</a></code> and
<code><a href="../reference/splm.html">splm()</a></code> are for different data types and use different
covariance functions. <code><a href="../reference/spautor.html">spautor()</a></code> is for spatial linear
models with areal data (i.e., spatial autoregressive models) and
<code><a href="../reference/splm.html">splm()</a></code> is for spatial linear models with point-referenced
data (i.e., geostatistical models). There are also a few features
<code><a href="../reference/splm.html">splm()</a></code> has that <code><a href="../reference/spautor.html">spautor()</a></code> does not:
semivariogram-based estimation, random effects, anisotropy, and big data
approximations.</p>
<div class="section level4">
<h4 id="sec:spautor-fn">
<code>spautor()</code> Spatial Covariance Functions<a class="anchor" aria-label="anchor" href="#sec:spautor-fn"></a>
</h4>
<p>For areal data, the covariance matrix depends on the specification of
a neighborhood structure among the observations. Observations with at
least one neighbor (not including itself) are called “connected”
observations. Observations with no neighbors are called “unconnected”
observations. The autoregressive spatial covariance matrix can be
defined as <span class="math display">\[\begin{equation*}
  \boldsymbol{\Sigma} =
  \begin{bmatrix}
    \sigma^2_{de} \mathbf{R} &amp; \mathbf{0} \\
    \mathbf{0} &amp; \sigma^2_{\xi} \mathbf{I}
  \end{bmatrix}
  + \sigma^2_{ie} \mathbf{I},
\end{equation*}\]</span> where <span class="math inline">\(\sigma^2_{de}\)</span> <span class="math inline">\((\geq 0)\)</span> is the spatially dependent
(correlated) variance for the connected observations, <span class="math inline">\(\mathbf{R}\)</span> is a matrix that describes the
spatial dependence for the connected observations, <span class="math inline">\(\sigma^2_{\xi}\)</span> <span class="math inline">\((\geq 0)\)</span> is the independent (not
correlated) variance for the unconnected observations, and <span class="math inline">\(\sigma^2_{ie}\)</span> <span class="math inline">\((\geq 0)\)</span> is the independent (not
correlated) variance for all observations. As seen, the connected and
unconnected observations are allowed different variances. The total
variance for connected observations is then <span class="math inline">\(\sigma^2_{de} + \sigma^2_{ie}\)</span> and the
total variance for unconnected observations is <span class="math inline">\(\sigma^2_{\xi} + \sigma^2_{ie}\)</span>.
<code>spmodel</code> accommodates two spatial covariances: conditional
autoregressive (CAR) and simultaneous autoregressive (SAR), both of
which have their <span class="math inline">\(\mathbf{R}\)</span> forms
provided in the following table.</p>
<table class="table">
<caption>The forms of R for each spatial covariance type available in
spautor()</caption>
<colgroup>
<col width="45%">
<col width="54%">
</colgroup>
<thead><tr class="header">
<th>Spatial covariance type</th>
<th>
<span class="math inline">\(\mathbf{R}\)</span> functional form</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\texttt{"car"}\)</span></td>
<td><span class="math inline">\((\mathbf{I} -
\phi\mathbf{W})^{-1}\mathbf{M}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\texttt{"sar"}\)</span></td>
<td><span class="math inline">\([(\mathbf{I} -
\phi\mathbf{W})(\mathbf{I} - \phi\mathbf{W})^\top]^{-1}\)</span></td>
</tr>
</tbody>
</table>
<p>For both CAR and SAR covariance functions, <span class="math inline">\(\mathbf{R}\)</span> depends on similar quantities:
<span class="math inline">\(\mathbf{I}\)</span>, an identity matrix;
<span class="math inline">\(\phi\)</span>, a range parameter, and <span class="math inline">\(\mathbf{W}\)</span>, a matrix that defines the
neighborhood structure. Often <span class="math inline">\(\mathbf{W}\)</span> is symmetric but it need not
be. Valid values for <span class="math inline">\(\phi\)</span> are in
<span class="math inline">\((1 / \lambda_{min}, 1 /
\lambda_{max})\)</span>, where <span class="math inline">\(\lambda_{min}\)</span> is the minimum eigenvalue
of <span class="math inline">\(\mathbf{W}\)</span> and <span class="math inline">\(\lambda_{max}\)</span> is the maximum eigenvalue
of <span class="math inline">\(\mathbf{W}\)</span> <span class="citation">(Ver Hoef et al. 2018)</span>. For SAR covariance
functions, <span class="math inline">\(\lambda_{min}\)</span> must be
negative and <span class="math inline">\(\lambda_{max}\)</span> must be
positive. For CAR covariances functions, a matrix <span class="math inline">\(\mathbf{M}\)</span> matrix must be provided that
satisfies the CAR symmetry condition, which enforces the symmetry of the
covariance matrix. The CAR symmetry condition states <span class="math display">\[\begin{equation*}
  \frac{\mathbf{W}_{ij}}{\mathbf{M}_{ii}} =
\frac{\mathbf{W}_{ji}}{\mathbf{M}_{jj}}
\end{equation*}\]</span> for all <span class="math inline">\(i\)</span>
and <span class="math inline">\(j\)</span>, where <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> index rows or columns. When <span class="math inline">\(\mathbf{W}\)</span> is symmetric, <span class="math inline">\(\mathbf{M}\)</span> is often taken to be the
identity matrix.</p>
<p>The default in <code>spmodel</code> is to row-standardize <span class="math inline">\(\mathbf{W}\)</span> by dividing each element by
its respective row sum, which decreases variance. If row-standardization
is not used for a CAR model, the default in <code>spmodel</code> for
<span class="math inline">\(\mathbf{M}\)</span> is the identity
matrix.</p>
</div>
<div class="section level4">
<h4 id="sec:splm-fn">
<code>splm()</code> Spatial Covariance Functions<a class="anchor" aria-label="anchor" href="#sec:splm-fn"></a>
</h4>
<p>For point-referenced data, the spatial covariance is given by <span class="math display">\[\begin{equation*}
\sigma^2_{de}\mathbf{R} + \sigma^2_{ie} \mathbf{I},
\end{equation*}\]</span> where <span class="math inline">\(\sigma^2_{de}\)</span> <span class="math inline">\((\geq 0)\)</span> is the spatially dependent
(correlated) variance, <span class="math inline">\(\mathbf{R}\)</span>
is a spatial correlation matrix, <span class="math inline">\(\sigma^2_{ie}\)</span> <span class="math inline">\((\geq 0)\)</span> is the spatially independent
(not correlated) variance, and <span class="math inline">\(\mathbf{I}\)</span> is an identity matrix. The
<span class="math inline">\(\mathbf{R}\)</span> matrix always depends on
a range parameter, <span class="math inline">\(\phi\)</span> <span class="math inline">\((&gt; 0)\)</span>, that controls the behavior of
the covariance function with distance. For some covariance functions,
the <span class="math inline">\(\mathbf{R}\)</span> matrix depends on an
additional parameter that we call the “extra” parameter. The following
table shows the parametric form for all <span class="math inline">\(\mathbf{R}\)</span> matrices available in
<code><a href="../reference/splm.html">splm()</a></code>. The range parameter is denoted as <span class="math inline">\(\phi\)</span>, the distance is denoted as <span class="math inline">\(h\)</span>, the distance divided by the range
parameter (<span class="math inline">\(h / \phi\)</span>) is denoted as
<span class="math inline">\(\eta\)</span>, <span class="math inline">\(\mathcal{I}\{\cdot\}\)</span> is an indicator
function equal to one when the argument occurs and zero otherwise, and
the extra parameter is denoted as <span class="math inline">\(\xi\)</span> (when relevant).</p>
<table class="table">
<caption>The forms of R for each spatial covariance type available in
splm(). All spatial covariance functions are valid in two dimensions
except the triangular and cosine functions, which are only valid in one
dimension. An alias for none is ie.</caption>
<colgroup>
<col width="56%">
<col width="43%">
</colgroup>
<thead><tr class="header">
<th>Spatial Covariance Type</th>
<th>R Functional Form</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\texttt{"exponential"}\)</span></td>
<td><span class="math inline">\(e^{-\eta}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\texttt{"spherical"}\)</span></td>
<td><span class="math inline">\((1 - 1.5\eta + 0.5\eta^3)\mathcal{I}\{h
\leq \phi \}\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\texttt{"gaussian"}\)</span></td>
<td><span class="math inline">\(e^{-\eta^2}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\texttt{"triangular"}\)</span></td>
<td><span class="math inline">\((1 - \eta)\mathcal{I}\{h \leq \phi
\}\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\texttt{"circular"}\)</span></td>
<td><span class="math inline">\((1 - \frac{2}{\pi}[m\sqrt{1 - m^2} +
sin^{-1}\{m\}])\mathcal{I}\{h \leq \phi \}, m = min(\eta,
1)\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\texttt{"cubic"}\)</span></td>
<td>
<span class="math inline">\((1 - 7\eta^2 + 8.75\eta^3 - 3.5\eta^5 +
0.75 \eta^7)\mathcal{I}\{h \leq \phi \}\)</span> \</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\texttt{"pentaspherical"}\)</span></td>
<td>
<span class="math inline">\((1 - 1.875\eta + 1.250\eta^3 -
0.375\eta^5)\mathcal{I}\{h \leq \phi \}\)</span> \</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\texttt{"cosine"}\)</span></td>
<td>$ cos() $</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\texttt{"wave"}\)</span></td>
<td><span class="math inline">\(\frac{sin(\eta)}{\eta}\mathcal{I}\{h
&gt; 0 \} + \mathcal{I}\{h = 0 \}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\texttt{"jbessel"}\)</span></td>
<td>
<span class="math inline">\(B_j(h\phi), B_j\)</span> is
Bessel-J</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\texttt{"gravity"}\)</span></td>
<td><span class="math inline">\((1 + \eta^2)^{-1/2}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\texttt{"rquad"}\)</span></td>
<td><span class="math inline">\((1 + \eta^2)^{-1}\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\texttt{"magnetic"}\)</span></td>
<td><span class="math inline">\((1 + \eta^2)^{-3/2}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\texttt{"matern"}\)</span></td>
<td>
<span class="math inline">\(\frac{2^{(1 - \xi)}}{\Gamma(\xi)}
\alpha^\xi B_k(\alpha, \xi), \alpha = \sqrt{2\xi \eta}, B_k\)</span> is
Bessel-K with order <span class="math inline">\(\xi\)</span>, <span class="math inline">\(\xi \in [1/5, 5]\)</span>
</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\texttt{"cauchy"}\)</span></td>
<td>
<span class="math inline">\((1 + \eta^2)^{-\xi}\)</span>, <span class="math inline">\(\xi &gt; 0\)</span>
</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\texttt{"pexponential"}\)</span></td>
<td>
<span class="math inline">\(exp(-h^\xi / \phi)\)</span>, <span class="math inline">\(\xi \in (0, 2]\)</span>
</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\texttt{"none"}\)</span></td>
<td><span class="math inline">\(0\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\texttt{"ie"}\)</span></td>
<td><span class="math inline">\(0\)</span></td>
</tr>
</tbody>
</table>
</div>
<div class="section level4">
<h4 id="sec:estimation-lm">Model-fitting<a class="anchor" aria-label="anchor" href="#sec:estimation-lm"></a>
</h4>
<div class="section level5">
<h5 id="sec:estimation-lik-lm">Likelihood-based Estimation (<code>estmethod = "reml"</code> or
<code>estmethod = "ml"</code>)<a class="anchor" aria-label="anchor" href="#sec:estimation-lik-lm"></a>
</h5>
<p>Minus twice a profiled (by <span class="math inline">\(\boldsymbol{\beta}\)</span>) Gaussian
log-likelihood is given by <span class="math display">\[\begin{equation}\label{eq:ml-lik}
  -2\ell_p(\boldsymbol{\theta}) = \ln{|\boldsymbol{\Sigma}|} +
(\mathbf{y} - \mathbf{X} \tilde{\boldsymbol{\beta}})^\top
\boldsymbol{\Sigma}^{-1} (\mathbf{y} - \mathbf{X}
\tilde{\boldsymbol{\beta}}) + n \ln{2\pi},
\end{equation}\]</span> where <span class="math inline">\(\tilde{\boldsymbol{\beta}} = (\mathbf{X}^\top
\mathbf{\Sigma}^{-1} \mathbf{X})^{-1} \mathbf{X}^\top
\mathbf{\Sigma}^{-1} \mathbf{y}\)</span>. Minimizing this equation
yields <span class="math inline">\(\boldsymbol{\hat{\theta}}_{ml}\)</span>, the
maximum likelihood estimates for <span class="math inline">\(\boldsymbol{\theta}\)</span>. Then a closed form
solution exists for <span class="math inline">\(\boldsymbol{\hat{\beta}}_{ml}\)</span>, the
maximum likelihood estimates for <span class="math inline">\(\boldsymbol{\beta}\)</span>: <span class="math inline">\(\boldsymbol{\hat{\beta}}_{ml} =
\tilde{\boldsymbol{\beta}}_{ml}\)</span>, where <span class="math inline">\(\tilde{\boldsymbol{\beta}}_{ml}\)</span> is <span class="math inline">\(\tilde{\boldsymbol{\beta}}\)</span> evaluated at
<span class="math inline">\(\boldsymbol{\hat{\theta}}_{ml}\)</span>. To
reduce bias in that variances of <span class="math inline">\(\boldsymbol{\hat{\beta}}_{ml}\)</span> that can
occur due to the simultaneous estimation of <span class="math inline">\(\boldsymbol{\beta}\)</span> and <span class="math inline">\(\boldsymbol{\theta}\)</span>, restricted maximum
likelihood estimation (REML) <span class="citation">(Patterson and
Thompson 1971; Harville 1977; Wolfinger, Tobias, and Sall 1994)</span>
has been shown to be better than maximum likelihood estimation.
Integrating <span class="math inline">\(\boldsymbol{\beta}\)</span> out
of a Gaussian likelihood yields the restricted Gaussian likelihood.
Minus twice a restricted Gaussian log-likelihood is given by <span class="math display">\[\begin{equation}\label{eq:reml-lik}
  -2\ell_R(\boldsymbol{\theta}) = -2\ell_p(\boldsymbol{\theta})  +
\ln{|\mathbf{X}^\top \boldsymbol{\Sigma}^{-1} \mathbf{X}|} - p \ln{2\pi}
,
\end{equation}\]</span> where <span class="math inline">\(p\)</span>
equals the dimension of <span class="math inline">\(\boldsymbol{\beta}\)</span>. Minimizing this
equation yields <span class="math inline">\(\boldsymbol{\hat{\theta}}_{reml}\)</span>, the
restricted maximum likelihood estimates for <span class="math inline">\(\boldsymbol{\theta}\)</span>. Then a closed for
solution exists for <span class="math inline">\(\boldsymbol{\hat{\beta}}_{reml}\)</span>, the
restricted maximum likelihood estimates for <span class="math inline">\(\boldsymbol{\beta}\)</span>: <span class="math inline">\(\boldsymbol{\hat{\beta}}_{reml} =
\tilde{\boldsymbol{\beta}}_{reml}\)</span>, where <span class="math inline">\(\tilde{\boldsymbol{\beta}}_{reml}\)</span> is
<span class="math inline">\(\tilde{\boldsymbol{\beta}}\)</span>
evaluated at <span class="math inline">\(\boldsymbol{\hat{\theta}}_{reml}\)</span>.</p>
<p>The covariance matrix can often be written as <span class="math inline">\(\boldsymbol{\Sigma} = \sigma^2
\boldsymbol{\Sigma}^*\)</span>, where <span class="math inline">\(\sigma^2\)</span> is the overall variance and
<span class="math inline">\(\boldsymbol{\Sigma}^*\)</span> is a
covariance matrix that depends on parameter vector <span class="math inline">\(\boldsymbol{\theta}^*\)</span> with one less
dimension than <span class="math inline">\(\boldsymbol{\theta}\)</span>.
Then the overall variance, <span class="math inline">\(\sigma^2\)</span>, can be profiled out of the
previous likelihood equation. This reduces the number of parameters
requiring optimization by one, which can dramatically reduce estimation
time. Further profiling out <span class="math inline">\(\sigma^2\)</span> yields <span class="math display">\[\begin{equation*}\label{eq:ml-plik}
  -2\ell_p^*(\boldsymbol{\theta}^*) = \ln{|\boldsymbol{\Sigma^*}|} +
n\ln[(\mathbf{y} - \mathbf{X} \tilde{\boldsymbol{\beta}})^\top
\boldsymbol{\Sigma}^{* -1} (\mathbf{y} - \mathbf{X}
\tilde{\boldsymbol{\beta}})] + n + n\ln{2\pi / n}.
\end{equation*}\]</span> After finding <span class="math inline">\(\hat{\boldsymbol{\theta}}^*_{ml}\)</span>, a
closed form solution for <span class="math inline">\(\hat{\sigma}^2_{ml}\)</span> exists: <span class="math inline">\(\hat{\sigma}^2_{ml} = [(\mathbf{y} - \mathbf{X}
\boldsymbol{\tilde{\beta}})^\top \mathbf{\Sigma}^{* -1} (\mathbf{y} -
\mathbf{X} \tilde{\boldsymbol{\beta}})] / n\)</span>. Then <span class="math inline">\(\boldsymbol{\hat{\theta}}^*_{ml}\)</span> is
combined with <span class="math inline">\(\hat{\sigma}^2_{ml}\)</span>
to yield <span class="math inline">\(\boldsymbol{\hat{\theta}}_{ml}\)</span> and
subsequently <span class="math inline">\(\boldsymbol{\hat{\beta}}_{ml}\)</span>. A similar
result holds for restricted maximum likelihood estimation. Further
profiling out <span class="math inline">\(\sigma^2\)</span> yields <span class="math display">\[\begin{equation*}\label{eq:reml-plik}
  -2\ell_R^*(\boldsymbol{\Theta}) = \ln{|\boldsymbol{\Sigma}^*|} + (n -
p)\ln[(\mathbf{y} - \mathbf{X} \tilde{\boldsymbol{\beta}})^\top
\boldsymbol{\Sigma}^{* -1} (\mathbf{y} - \mathbf{X}
\tilde{\boldsymbol{\beta}})] + \ln{|\mathbf{X}^\top
\boldsymbol{\Sigma}^{* -1} \mathbf{X}|} + (n - p) + (n - p)\ln2\pi / (n
- p).
\end{equation*}\]</span> After finding <span class="math inline">\(\hat{\boldsymbol{\theta}}^*_{reml}\)</span>, a
closed form solution for <span class="math inline">\(\hat{\sigma}^2_{reml}\)</span> exists: <span class="math inline">\(\hat{\sigma}^2_{reml} = [(\mathbf{y} - \mathbf{X}
\boldsymbol{\tilde{\beta}})^\top \mathbf{\Sigma}^{* -1} (\mathbf{y} -
\mathbf{X} \tilde{\boldsymbol{\beta}})] / (n - p)\)</span>. Then <span class="math inline">\(\boldsymbol{\hat{\theta}}^*_{reml}\)</span> is
combined with <span class="math inline">\(\hat{\sigma}^2_{reml}\)</span>
to yield <span class="math inline">\(\boldsymbol{\hat{\theta}}_{reml}\)</span> and
subsequently <span class="math inline">\(\boldsymbol{\hat{\beta}}_{reml}\)</span>. For more
on profiling Gaussian likelihoods, see <span class="citation">Wolfinger,
Tobias, and Sall (1994)</span>.</p>
<p>Both maximum likelihood and restricted maximum likelihood estimation
rely on the <span class="math inline">\(n \times n\)</span> covariance
matrix inverse. Inverting an <span class="math inline">\(n \times
n\)</span> matrix is an enormous computational demand that scales
cubically with the sample size. For this reason, maximum likelihood and
restricted maximum likelihood estimation have historically been
infeasible to implement in their standard form with data larger than a
few thousand observations. This motivates the use for big data
approaches.</p>
</div>
<div class="section level5">
<h5 id="sec:estimation-sv-lm">Semivariogram-based Estimation (<code>splm()</code> only)<a class="anchor" aria-label="anchor" href="#sec:estimation-sv-lm"></a>
</h5>
<p>An alternative approach to likelihood-based estimation is
semivariogram-based estimation. The semivariogram of a constant-mean
process <span class="math inline">\(\mathbf{y}\)</span> is the
expectation of half of the squared difference between two observations
<span class="math inline">\(h\)</span> distance apart. More formally,
the semivariogram is denoted <span class="math inline">\(\gamma(h)\)</span> and defined as <span class="math display">\[\begin{equation*}\label{eq:sv}
  \gamma(h) = \text{E}[(y_i - y_j)^2] / 2 ,
\end{equation*}\]</span> where <span class="math inline">\(h\)</span> is
the Euclidean distance between the locations of <span class="math inline">\(y_i\)</span> and <span class="math inline">\(y_j\)</span>. When the process <span class="math inline">\(\mathbf{y}\)</span> is second-order stationary,
the semivariogram and covariance function are intimately connected:
<span class="math inline">\(\gamma(h) = \sigma^2 -
\text{Cov}(h)\)</span>, where <span class="math inline">\(\sigma^2\)</span> is the overall variance and
<span class="math inline">\(\text{Cov}(h)\)</span> is the covariance
function evaluated at <span class="math inline">\(h\)</span>. As such,
the semivariogram and covariance function rely on the same parameter
vector <span class="math inline">\(\boldsymbol{\theta}\)</span>. Both of
the semivariogram approaches described next are more computationally
efficient than restricted maximum likelihood and maximum likelihood
estimation because the major computational burden of the semivariogram
approaches (calculations based on squared differences among pairs)
scales quadratically with the sample size (i.e., not the cubed sample
size like the likelihood-based approaches).</p>
<div class="section level6">
<h6 id="sec:estimation-sv-cwls-lm">Weighted Least Squares (<code>estmethod = "sv-wls"</code>)<a class="anchor" aria-label="anchor" href="#sec:estimation-sv-cwls-lm"></a>
</h6>
<p>The empirical semivariogram is a moment-based estimate of the
semivariogram denoted by <span class="math inline">\(\hat{\gamma}(h)\)</span>. It is defined as <span class="math display">\[\begin{equation*}
  \hat{\gamma}(h) = \frac{1}{2|N(h)|} \sum_{N(h)} (y_i - y_j)^2,
\end{equation*}\]</span> where <span class="math inline">\(N(h)\)</span>
is the set of observations in <span class="math inline">\(\mathbf{y}\)</span> that are <span class="math inline">\(h\)</span> distance units apart (distance classes)
and <span class="math inline">\(|N(h)|\)</span> is the cardinality of
<span class="math inline">\(N(h)\)</span> <span class="citation">(Cressie 1993)</span>. One criticism of the empirical
semivariogram is that distance bins and cutoffs tend to be arbitrarily
chosen (i.e., not chosen according to some statistical criteria).</p>
<p><span class="citation">Cressie (1985)</span> proposed estimating
<span class="math inline">\(\boldsymbol{\theta}\)</span> by minimizing
an objective function that involves <span class="math inline">\(\gamma(h)\)</span> and <span class="math inline">\(\hat{\gamma}(h)\)</span> and is based on a
weighted least squares criterion. This criterion is defined as <span class="math display">\[\begin{equation}\label{eq:svwls}
  \sum_i w_i [\hat{\gamma}(h)_i - \gamma(h)_i]^2,
\end{equation}\]</span> where <span class="math inline">\(w_i\)</span>,
<span class="math inline">\(\hat{\gamma}(h)_i\)</span>, and <span class="math inline">\(\gamma(h)_i\)</span> are the weights, empirical
semivariogram, and semivariogram for the <span class="math inline">\(i\)</span>th distance class, respectively.
Minimizing this loss function yields <span class="math inline">\(\boldsymbol{\hat{\theta}}_{wls}\)</span>, the
semivariogram weighted least squares estimate of <span class="math inline">\(\boldsymbol{\theta}\)</span>. After estimating
<span class="math inline">\(\boldsymbol{\theta}\)</span>, <span class="math inline">\(\boldsymbol{\beta}\)</span> estimates are
constructed using (empirical) generalized least squares: <span class="math inline">\(\boldsymbol{\hat{\beta}}_{wls} = (\mathbf{X}^\top
\hat{\mathbf{\Sigma}}^{-1} \mathbf{X})^{-1} \mathbf{X}^\top
\hat{\mathbf{\Sigma}}^{-1} \mathbf{y}\)</span>.</p>
<p><span class="citation">Cressie (1985)</span> recommends setting <span class="math inline">\(w_i = |N(h)| / \gamma(h)_i^2\)</span>, which gives
more weight to distance classes with more observations (<span class="math inline">\(|N(h)|\)</span>) and shorter distances (<span class="math inline">\(1 / \gamma(h)_i^2\)</span>). The default in
<code>spmodel</code> is to use these <span class="math inline">\(w_i\)</span>, known as Cressie weights, though
several other options for <span class="math inline">\(w_i\)</span> exist
and are available via the <code>weights</code> argument. The following
table contains all <span class="math inline">\(w_i\)</span> available
via the <code>weights</code> argument.</p>
<table class="table">
<caption>Table of values for the weights argument in splm() when
estmethod = “sv-wls”.</caption>
<colgroup>
<col width="36%">
<col width="31%">
<col width="31%">
</colgroup>
<thead><tr class="header">
<th>
<span class="math inline">\(w_i\)</span> Name</th>
<th>
<span class="math inline">\(w_i\)</span> Form</th>
<th><span class="math inline">\(\texttt{weight =}\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Cressie</td>
<td><span class="math inline">\(|N(h)| / \gamma(h)_i^2\)</span></td>
<td><span class="math inline">\(\texttt{"cressie"}\)</span></td>
</tr>
<tr class="even">
<td>Cressie (Denominator) Root</td>
<td><span class="math inline">\(|N(h)| / \gamma(h)_i\)</span></td>
<td><span class="math inline">\(\texttt{"cressie-dr"}\)</span></td>
</tr>
<tr class="odd">
<td>Cressie No Pairs</td>
<td><span class="math inline">\(1 / \gamma(h)_i^2\)</span></td>
<td><span class="math inline">\(\texttt{"cressie-nopairs"}\)</span></td>
</tr>
<tr class="even">
<td>Cressie (Denominator) Root No Pairs</td>
<td><span class="math inline">\(1 / \gamma(h)_i\)</span></td>
<td><span class="math inline">\(\texttt{"cressie-dr-nopairs"}\)</span></td>
</tr>
<tr class="odd">
<td>Pairs</td>
<td><span class="math inline">\(|N(h)|\)</span></td>
<td><span class="math inline">\(\texttt{pairs"}\)</span></td>
</tr>
<tr class="even">
<td>Pairs Inverse Distance</td>
<td><span class="math inline">\(|N(h)| / h^2\)</span></td>
<td><span class="math inline">\(\texttt{"pairs-invd"}\)</span></td>
</tr>
<tr class="odd">
<td>Pairs Inverse (Root) Distance</td>
<td><span class="math inline">\(|N(h)| / h\)</span></td>
<td><span class="math inline">\(\texttt{"pairs-invrd"}\)</span></td>
</tr>
<tr class="even">
<td>Ordinary Least Squares</td>
<td>1</td>
<td><span class="math inline">\(\texttt{"ols"}\)</span></td>
</tr>
</tbody>
</table>
<p>The number of <span class="math inline">\(N(h)\)</span> classes and
the maximum distance for <span class="math inline">\(h\)</span> are
specified by passing the <code>bins</code> and <code>cutoff</code>
arguments to <code><a href="../reference/splm.html">splm()</a></code> (these arguments are passed via
<code>...</code> to <code><a href="../reference/esv.html">esv()</a></code>). The default value for
<code>bins</code> is 15 and the default value for <code>cutoff</code> is
half the maximum distance of the spatial domain’s bounding box.</p>
<p>Recall that the semivariogram is defined for a constant-mean process.
Generally, <span class="math inline">\(\mathbf{y}\)</span> does not
necessarily have a constant mean so the empirical semivariogram and
<span class="math inline">\(\boldsymbol{\hat{\theta}}_{wls}\)</span> are
typically constructed using the residuals from an ordinary least squares
regression of <span class="math inline">\(\mathbf{y}\)</span> on <span class="math inline">\(\mathbf{X}\)</span>. These ordinary least squares
residuals are assumed to have mean zero.</p>
</div>
<div class="section level6">
<h6 id="sec:estimation-sv-cl-lm">Composite Likelihood (<code>estmethod = "sv-cl"</code>)<a class="anchor" aria-label="anchor" href="#sec:estimation-sv-cl-lm"></a>
</h6>
<p>Composite likelihood approaches involve constructing likelihoods
based on conditional or marginal events for which likelihoods are
available and then adding together these individual components.
Composite likelihoods are attractive because they behave very similar to
likelihoods but are easier to handle, both from a theoretical and from a
computational perspective. <span class="citation">Curriero and Lele
(1999)</span> derive a particular composite likelihood for estimating
semivariogram parameters. The negative log of this composite likelihood,
denoted <span class="math inline">\(\text{CL}(h)\)</span>, is given by
<span class="math display">\[\begin{equation}\label{eq:svcl}
  \text{CL}(h) = \sum_{i = 1}^{n - 1} \sum_{j &gt; i} \left( \frac{(y_i
- y_j)^2}{2\gamma(h)} + \ln(\gamma(h)) \right)
\end{equation}\]</span> where <span class="math inline">\(\gamma(h)\)</span> is the semivariogram.
Minimizing this loss function yields <span class="math inline">\(\boldsymbol{\hat{\theta}}_{cl}\)</span>, the
semivariogram composite likelihood estimates of <span class="math inline">\(\boldsymbol{\theta}\)</span>. After estimating
<span class="math inline">\(\boldsymbol{\theta}\)</span>, <span class="math inline">\(\boldsymbol{\beta}\)</span> estimates are
constructed using (empirical) generalized least squares: <span class="math inline">\(\boldsymbol{\hat{\beta}}_{cl} = (\mathbf{X}^\top
\hat{\mathbf{\Sigma}}^{-1} \mathbf{X})^{-1} \mathbf{X}^\top
\hat{\mathbf{\Sigma}}^{-1} \mathbf{y}\)</span>.</p>
<p>An advantage of the composite likelihood approach to semivariogram
estimation is that it does not require arbitrarily specifying empirical
semivariogram bins and cutoffs. It does tend to be more computationally
demanding than weighted least squares, however. The composite likelihood
is constructed from <span class="math inline">\(\binom{n}{2}\)</span>
pairs for a sample size <span class="math inline">\(n\)</span>, whereas
the weighted least squares approach only requires calculating <span class="math inline">\(\binom{|N(h)|}{2}\)</span> pairs for each distance
bin <span class="math inline">\(N(h)\)</span>. As with the weighted
least squares approach, the composite likelihood approach requires a
constant-mean process, so typically the residuals from an ordinary least
squares regression of <span class="math inline">\(\mathbf{y}\)</span> on
<span class="math inline">\(\mathbf{X}\)</span> are used to estimate
<span class="math inline">\(\boldsymbol{\theta}\)</span>.</p>
</div>
</div>
</div>
<div class="section level4">
<h4 id="sec:optim-lm">Optimization<a class="anchor" aria-label="anchor" href="#sec:optim-lm"></a>
</h4>
<p>Parameter estimation is performed using <code><a href="https://rdrr.io/r/stats/optim.html" class="external-link">stats::optim()</a></code>.
The default estimation method is Nelder-Mead <span class="citation">(Nelder and Mead 1965)</span> and the stopping
criterion is a relative convergence tolerance (<code>reltol</code>) of
.0001. If only one parameter requires estimation (on the profiled scale
if relevant), the Brent algorithm is instead used <span class="citation">(Brent 1971)</span>. Arguments to <code><a href="https://rdrr.io/r/stats/optim.html" class="external-link">optim()</a></code>
are passed via <code>...</code> to <code><a href="../reference/splm.html">splm()</a></code> and
<code><a href="../reference/spautor.html">spautor()</a></code>. For example, the default estimation method and
convergence criteria are overridden by passing <code>method</code> and
<code>control</code>, respectively, to <code><a href="../reference/splm.html">splm()</a></code> and
<code><a href="../reference/spautor.html">spautor()</a></code>. If the <code>lower</code> and <code>upper</code>
arguments to <code><a href="https://rdrr.io/r/stats/optim.html" class="external-link">optim()</a></code> are specified in <code><a href="../reference/splm.html">splm()</a></code>
and <code><a href="../reference/spautor.html">spautor()</a></code> to be passed to <code><a href="https://rdrr.io/r/stats/optim.html" class="external-link">optim()</a></code>, they
are ignored, as optimization for all parameters is generally
unconstrained. Initial values for <code><a href="https://rdrr.io/r/stats/optim.html" class="external-link">optim()</a></code> are found using
the grid search described next.</p>
<div class="section level5">
<h5 id="sec:grid-lm">Grid Search<a class="anchor" aria-label="anchor" href="#sec:grid-lm"></a>
</h5>
<p><code>spmodel</code> uses a grid search to find suitable initial
values for use in optimization. For spatial linear models without random
effects, the spatially dependent variance (<span class="math inline">\(\sigma^2_{de}\)</span>) and spatially independent
variance (<span class="math inline">\(\sigma^2_{ie}\)</span>) parameters
are given “low”, “medium”, and “high” values. The sample variance of a
non-spatial linear model is slightly inflated by a factor of 1.2
(non-spatial models can underestimate the variance when there is spatial
dependence) and these “low”, “medium”, and “high” values correspond to
10%, 50%, and 90% of the inflated sample variance. Only combinations of
<span class="math inline">\(\sigma^2_{de}\)</span> and <span class="math inline">\(\sigma^2_{ie}\)</span> whose proportions sum to
100% are considered. The range (<span class="math inline">\(\phi\)</span>) and extra (<span class="math inline">\(\xi\)</span>) parameters are given “low” and
“high” values that are unique to each spatial covariance function. For
example, when using an exponential covariance function, the “low” value
of <span class="math inline">\(\phi\)</span> is one-half the diagonal of
the domain’s bounding box divided by three. This particular value is
chosen so that the effective range (the distance at which the covariance
is approximately zero), which equals <span class="math inline">\(3\phi\)</span> for the exponential covariance
function, is is reached at one-half the diagonal of the domain’s
bounding box. Analogously, the “high” value of <span class="math inline">\(\phi\)</span> is three-halves the diagonal of the
domain’s bounding box divided by three. The anisotropy rotation
parameter (<span class="math inline">\(\alpha\)</span>) is given six
values that correspond to 0, <span class="math inline">\(\pi/6\)</span>,
<span class="math inline">\(2\pi/6\)</span>, <span class="math inline">\(4\pi/6\)</span>, <span class="math inline">\(5\pi/6\)</span>, and <span class="math inline">\(\pi\)</span> radians. The anisotropy scale
parameter (<span class="math inline">\(S\)</span>) is given “low”,
“medium”, and “high” values that correspond to scaling factors of 0.25,
0.75, and 1. Note that the anisotropy parameters are only used during
grid searches for point-referenced data.</p>
<p>The crossing of all appropriate parameter values is considered. If
initial values are used for a parameter, the initial value replaces all
values of the parameter in this crossing. Duplicate crossings are then
omitted. The parameter configuration that yields the smallest value of
the objective function is then used as an initial value for
optimization. Suppose the inflated sample variance is 10, the
exponential covariance is used assuming isotropy, and the diagonal of
the bounding box is 180 distance units. The parameter configurations
evaluated are shown in the following table.</p>
<table class="table">
<caption>Grid search parameter configurations for an isotropic
exponential spatial covariance with inflated sample variance equal to 10
and diagonal of the bounding box equal to 180 distance units.</caption>
<thead><tr class="header">
<th><span class="math inline">\(\sigma^2_{de}\)</span></th>
<th><span class="math inline">\(\sigma^2_{ie}\)</span></th>
<th><span class="math inline">\(\phi\)</span></th>
<th><span class="math inline">\(\alpha\)</span></th>
<th><span class="math inline">\(S\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>9</td>
<td>1</td>
<td>15</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="even">
<td>1</td>
<td>9</td>
<td>15</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="odd">
<td>5</td>
<td>5</td>
<td>15</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="even">
<td>9</td>
<td>1</td>
<td>45</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="odd">
<td>1</td>
<td>9</td>
<td>45</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="even">
<td>5</td>
<td>5</td>
<td>15</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>For spatial linear models with random effects, the same approach is
used to create a crossing of spatial covariance parameters. A separate
approach is used to create a set of random effect variances. The random
effect variances are similarly first grouped by proportions. The first
combination is such that the first random effect variance is given 90%
of variance, and the remaining 10% is spread out evenly among the
remaining random effect variances. The second combination is such that
the second random effect variance is given 90% of the variance, and the
remaining 10% is spread out evenly among the remaining random effect
variances. And so on and so forth. These combinations ascertain whether
one random effect dominates variability. A final grouping is lastly
considered: all 100% of variance is spread out evenly among all random
effects.</p>
<p>When finding parameter values <span class="math inline">\(\sigma^2_{de}\)</span>, <span class="math inline">\(\sigma^2_{ie}\)</span>, and the random effect
variances (<span class="math inline">\(\sigma^2_{u_i}\)</span> for the
<span class="math inline">\(i\)</span>th random effect), three scenarios
are considered. In the first scenario, <span class="math inline">\(\sigma^2_{de}\)</span> and <span class="math inline">\(\sigma^2_{ie}\)</span> get 90% of the inflated
sample variance and the random effect variances get 10%. In this
scenario, only the random effect grouping where the variance is evenly
spread out is considered. This is because the random effect variances
are already contributing little to the overall variability, so
performing additional objective function evaluations is unnecessary. In
the second scenario, the random effects get 90% of the inflated sample
variances and <span class="math inline">\(\sigma^2_{de}\)</span> and
<span class="math inline">\(\sigma^2_{ie}\)</span> get 10%. Similarly in
this scenario, only the <span class="math inline">\(\sigma^2_{de}\)</span> and <span class="math inline">\(\sigma^2_{ie}\)</span> grouping where the variance
is evenly spread out is considered. Also in this scenario, only the
lowest value for <code>range</code> and <code>extra</code> are used. In
the third scenario, the 50% of the inflated sample variance is given to
<span class="math inline">\(\sigma^2_{de}\)</span> and <span class="math inline">\(\sigma^2_{ie}\)</span> and 50% to the random
effects. In this scenario, the only parameter combination considered is
the case where variances are evenly spread out among <span class="math inline">\(\sigma^2_{de}\)</span>, <span class="math inline">\(\sigma^2_{ie}\)</span>, and the random effect
variances. Together, there are parameter configurations where the
spatial variability dominates (scenario 1), the random variability
dominates (scenario 2), and where there is an even contribution from
spatial and random variability. The parameter configuration that
minimizes the objective function is then used as an initial value for
optimization. Recall that random effects are only used with restricted
maximum likelihood or maximum likelihood estimation, so the objective
function is always a likelihood.</p>
<p>Suppose the inflated sample variance is 10, the exponential
covariance is used assuming isotropy, the diagonal of the bounding box
is 180 distance units, and there are two random effects. The parameter
configurations evaluated are shown in the following table.</p>
<table style="width:100%;" class="table">
<caption>Grid search parameter configurations for an isotropic
exponential spatial covariance with two random effects, inflated sample
variance equal to 10, and diagonal of the bounding box equal to 180
distance units.</caption>
<colgroup>
<col width="14%">
<col width="14%">
<col width="14%">
<col width="14%">
<col width="14%">
<col width="14%">
<col width="14%">
</colgroup>
<thead><tr class="header">
<th><span class="math inline">\(\sigma^2_{de}\)</span></th>
<th><span class="math inline">\(\sigma^2_{ie}\)</span></th>
<th><span class="math inline">\(\phi\)</span></th>
<th><span class="math inline">\(\alpha\)</span></th>
<th><span class="math inline">\(S\)</span></th>
<th><span class="math inline">\(\sigma^2_{u1}\)</span></th>
<th><span class="math inline">\(\sigma^2_{u2}\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>8.1</td>
<td>0.9</td>
<td>15</td>
<td>0</td>
<td>1</td>
<td>0.5</td>
<td>0.5</td>
</tr>
<tr class="even">
<td>0.9</td>
<td>8.1</td>
<td>15</td>
<td>0</td>
<td>1</td>
<td>0.5</td>
<td>0.5</td>
</tr>
<tr class="odd">
<td>4.5</td>
<td>4.5</td>
<td>15</td>
<td>0</td>
<td>1</td>
<td>0.5</td>
<td>0.5</td>
</tr>
<tr class="even">
<td>8.1</td>
<td>0.9</td>
<td>45</td>
<td>0</td>
<td>1</td>
<td>0.5</td>
<td>0.5</td>
</tr>
<tr class="odd">
<td>0.9</td>
<td>8.1</td>
<td>45</td>
<td>0</td>
<td>1</td>
<td>0.5</td>
<td>0.5</td>
</tr>
<tr class="even">
<td>4.5</td>
<td>4.5</td>
<td>45</td>
<td>0</td>
<td>1</td>
<td>0.5</td>
<td>0.5</td>
</tr>
<tr class="odd">
<td>0.5</td>
<td>0.5</td>
<td>15</td>
<td>0</td>
<td>1</td>
<td>8.1</td>
<td>0.9</td>
</tr>
<tr class="even">
<td>0.5</td>
<td>0.5</td>
<td>15</td>
<td>0</td>
<td>1</td>
<td>0.9</td>
<td>8.1</td>
</tr>
<tr class="odd">
<td>0.5</td>
<td>0.5</td>
<td>15</td>
<td>0</td>
<td>1</td>
<td>4.5</td>
<td>4.5</td>
</tr>
<tr class="even">
<td>2.5</td>
<td>2.5</td>
<td>15</td>
<td>0</td>
<td>1</td>
<td>2.5</td>
<td>2.5</td>
</tr>
<tr class="odd">
<td>2.5</td>
<td>2.5</td>
<td>45</td>
<td>0</td>
<td>1</td>
<td>2.5</td>
<td>2.5</td>
</tr>
</tbody>
</table>
<p>This grid search approach balances a thorough exploration of the
parameter space with computational efficiency, as each objective
function evaluation can be computationally expensive.</p>
</div>
</div>
<div class="section level4">
<h4 id="sec:testing-lm">Hypothesis Testing<a class="anchor" aria-label="anchor" href="#sec:testing-lm"></a>
</h4>
<p>The hypothesis tests for <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> returned by
<code><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary()</a></code> or <code><a href="https://generics.r-lib.org/reference/tidy.html" class="external-link">tidy()</a></code> of an <code>splm</code> or
<code>spautor</code> object are asymptotic z-tests based on the normal
(Gaussian) distribution (Wald tests). The null hypothesis for the test
associated with each <span class="math inline">\(\hat{\beta}_i\)</span>
is that <span class="math inline">\(\beta_i = 0\)</span>. Then the test
statistic is given by <span class="math display">\[\begin{equation*}
  \tilde{z} = \frac{\hat{\beta}_i}{\text{SE}(\hat{\beta}_i)},
\end{equation*}\]</span> where <span class="math inline">\(\text{SE}(\hat{\beta}_i)\)</span> is the standard
error of <span class="math inline">\(\hat{\beta}_i\)</span>, which
equals the square root of the <span class="math inline">\(i\)</span>th
diagonal element of <span class="math inline">\((\mathbf{X}^\top
\hat{\boldsymbol{\Sigma}}^{-1} \mathbf{X})^{-1}\)</span>. The p-value is
given by <span class="math inline">\(2 * (1 -
\Phi(|\tilde{z}|))\)</span>, which corresponds to an equal-tailed,
two-sided hypothesis test of level <span class="math inline">\(\alpha\)</span> where <span class="math inline">\(\Phi(\cdot)\)</span> denotes the standard normal
(Gaussian) cumulative distribution function and <span class="math inline">\(|\cdot|\)</span> denotes the absolute value.</p>
</div>
<div class="section level4">
<h4 id="sec:random-lm">Random Effects (<code>splm()</code> only and <code>"reml"</code> or
<code>"ml"</code> <code>estmethod</code> only)<a class="anchor" aria-label="anchor" href="#sec:random-lm"></a>
</h4>
<p>The random effects contribute directly to the covariance through
their design matrices. Let <span class="math inline">\(\mathbf{u}\)</span> be a mean-zero random effect
column vector of length <span class="math inline">\(n_u\)</span>, where
<span class="math inline">\(n_u\)</span> is the number of levels of the
random effect, with design matrix <span class="math inline">\(\mathbf{Z}_u\)</span>. Then <span class="math inline">\(\text{Cov}(\mathbf{Z}_u\mathbf{u}) = \mathbf{Z}_u
\text{Cov}(\mathbf{u})\mathbf{Z}_u^\top\)</span>. Because each element
of <span class="math inline">\(\mathbf{u}\)</span> is independent of one
another, this reduces to <span class="math inline">\(\text{Cov}(\mathbf{Z}_u\mathbf{u}) = \sigma^2_u
\mathbf{Z}_u \mathbf{Z}_u^\top\)</span>, where <span class="math inline">\(\sigma^2_u\)</span> is the variance parameter
corresponding to the random effect (i.e., the random effect variance
parameter).</p>
<p>The <span class="math inline">\(\mathbf{Z}\)</span> matrices index
the levels of the random effect. <span class="math inline">\(\mathbf{Z}\)</span> has dimension <span class="math inline">\(n \times n_u\)</span>, where <span class="math inline">\(n\)</span> is the sample size. Each row of <span class="math inline">\(\mathbf{Z}\)</span> corresponds to an observation
and each column to a level of the random effect. For example, suppose we
have <span class="math inline">\(n = 4\)</span> observations, so <span class="math inline">\(\mathbf{y} = \{y_1, y_2, y_3, y_4\}\)</span>. Also
suppose that the random effect <span class="math inline">\(\mathbf{u}\)</span> has two levels and that <span class="math inline">\(y_1\)</span> and <span class="math inline">\(y_4\)</span> are in the first level and <span class="math inline">\(y_2\)</span> and <span class="math inline">\(y_3\)</span> are in the second level. For random
intercepts, each element of <span class="math inline">\(\mathbf{Z}\)</span> is one if the observation is
in the appropriate level of the random effect and zero otherwise. So it
follows that <span class="math display">\[\begin{equation*}
\mathbf{Z}\mathbf{u} =
\begin{bmatrix}
1 &amp; 0 \\
0 &amp; 1 \\
0 &amp; 1 \\
1 &amp; 0
\end{bmatrix}
\begin{bmatrix}
u_1 \\
u_2
\end{bmatrix},
\end{equation*}\]</span> where <span class="math inline">\(u_1\)</span>
and <span class="math inline">\(u_2\)</span> are the random intercepts
for the first and second levels of <span class="math inline">\(\mathbf{u}\)</span>, respectively. For random
slopes, each element of <span class="math inline">\(\mathbf{Z}\)</span>
equals the value of an auxiliary variable, <span class="math inline">\(\mathbf{k}\)</span>, if the observation is in the
appropriate level of the random effect and zero otherwise. So if <span class="math inline">\(\mathbf{k} = \{2, 7, 5, 4 \}\)</span> it follows
that <span class="math display">\[\begin{equation*}
\mathbf{Z}\mathbf{u} =
\begin{bmatrix}
2 &amp; 0 \\
0 &amp; 7 \\
0 &amp; 5 \\
4 &amp; 0
\end{bmatrix}
\begin{bmatrix}
u_1 \\
u_2
\end{bmatrix},
\end{equation*}\]</span> where <span class="math inline">\(u_1\)</span>
and <span class="math inline">\(u_2\)</span> are the random slopes for
the first and second levels of <span class="math inline">\(\mathbf{u}\)</span>, respectively. If a random
slope is included in the model, it is common for the auxiliary variable
to be a column in <span class="math inline">\(\mathbf{X}\)</span>, the
fixed effects design matrix (i.e., also a fixed effect). Denote this
column as <span class="math inline">\(\mathbf{x}\)</span>. Here <span class="math inline">\(\boldsymbol{\beta}\)</span> captures the average
effect of <span class="math inline">\(\mathbf{x}\)</span> on <span class="math inline">\(\mathbf{y}\)</span> (accounting for other
explanatory variables) and <span class="math inline">\(\mathbf{u}\)</span> captures a subject-specific
effect of <span class="math inline">\(\mathbf{x}\)</span> on <span class="math inline">\(\mathbf{y}\)</span>. So for a subject in the <span class="math inline">\(i\)</span>th level of <span class="math inline">\(\mathbf{u}\)</span>, the average increase in <span class="math inline">\(y\)</span> associated with a one-unit increase
<span class="math inline">\(x\)</span> is <span class="math inline">\(\beta + u_i\)</span>.</p>
<p>The <code>sv-wls</code> and <code>sv-cl</code> estimation methods do
not use a likelihood, and thus, they do not allow for the estimation of
random effects in <code>spmodel</code>.</p>
</div>
<div class="section level4">
<h4 id="sec:anisotropy-lm">Anisotropy (<code>splm()</code> only)<a class="anchor" aria-label="anchor" href="#sec:anisotropy-lm"></a>
</h4>
<p>An isotropic spatial covariance function behaves similarly in all
directions (i.e., is independent of direction) as a function of
distance. An anisotropic spatial covariance function does not behave
similarly in all directions as a function of distance. The following
figure shows ellipses for an isotropic and anisotropic spatial
covariance function centered at the origin (a distance of zero). The
black outline of each ellipse is a level curve of equal correlation. The
left ellipse (a circle) represents an isotropic covariance function. The
distance at which the correlation between two observations lays on the
level curve is the same in all directions. The right ellipse represents
an anisotropic covariance function. The distance at which the
correlation between two observations lays on the level curve is
different in different directions.</p>
<div class="figure">
<img src="technical_files/figure-html/anisotropy-1.png" alt="In the left figure, the ellipse of an isotropic spatial covariance function centered at the origin is shown. In the right figure, the ellipse of an anisotropic spatial covariance function centered at the origin is shown. The black outline of each ellipse is a level curve of equal correlation. " width="50%"><img src="technical_files/figure-html/anisotropy-2.png" alt="In the left figure, the ellipse of an isotropic spatial covariance function centered at the origin is shown. In the right figure, the ellipse of an anisotropic spatial covariance function centered at the origin is shown. The black outline of each ellipse is a level curve of equal correlation. " width="50%"><p class="caption">
In the left figure, the ellipse of an isotropic spatial covariance
function centered at the origin is shown. In the right figure, the
ellipse of an anisotropic spatial covariance function centered at the
origin is shown. The black outline of each ellipse is a level curve of
equal correlation.
</p>
</div>
<p>To accommodate spatial anisotropy, the original coordinates must be
transformed such that the transformed coordinates yield an isotropic
spatial covariance. This transformation involves a rotation and a
scaling. Consider a set of <span class="math inline">\(x\)</span> and
<span class="math inline">\(y\)</span> coordinates that should be
transformed into <span class="math inline">\(x^*\)</span> and <span class="math inline">\(y^*\)</span> coordinates. This transformation is
formally defined as <span class="math display">\[\begin{equation*}
  \begin{bmatrix}
    x^* \\
    y^*
  \end{bmatrix} =
  \begin{bmatrix}
    1 &amp; 0 \\
    0 &amp; 1 / S
  \end{bmatrix}
  \begin{bmatrix}
    \cos(\alpha) &amp; \sin(\alpha) \\
    -\sin(\alpha) &amp; \cos(\alpha)
  \end{bmatrix}  
  \begin{bmatrix}
    x \\
    y
  \end{bmatrix}.
\end{equation*}\]</span> The original coordinates are first multiplied
by the rotation matrix, which rotates the coordinates clockwise by angle
<span class="math inline">\(\alpha\)</span>. They are then multiplied by
the scaling matrix, which scales the minor axis of the spatial
covariance ellipse by the reciprocal of <span class="math inline">\(S\)</span>. The transformed coordinates are then
used to compute distances and the resulting spatial covariances. This
type of anisotropy is more formally known as “geometric” anisotropy
because it involves a geometric transformation of the coordinates. The
following figure shows this process step-by-step.</p>
<div class="figure">
<img src="technical_files/figure-html/anisotropy2-1.png" alt="In the left figure, the ellipse of an anisotropic spatial covariance function centered at the origin is shown. The blue lines represent the original axes and the red lines the transformed axes. The solid lines represent the x-axes and the dotted lines the y-axes. Note that the solid, red line is the major axis of the ellpise and the dashed, red line is the minor axis of the ellipse. In the center figure, the ellipse has been rotated clockwise by the rotate parameter so the major axis is the transformed x-axis and the minor axis is the transformed y-axis. In the right figure, the minor axis of the ellipse has been scaled by the reciprocal of the scale parameter so that the ellipse becomes a circle, which corresponds to an isotropic spatial covariance function. The transformed coordinates are then used to compute distances and spatial covariances." width="33%"><img src="technical_files/figure-html/anisotropy2-2.png" alt="In the left figure, the ellipse of an anisotropic spatial covariance function centered at the origin is shown. The blue lines represent the original axes and the red lines the transformed axes. The solid lines represent the x-axes and the dotted lines the y-axes. Note that the solid, red line is the major axis of the ellpise and the dashed, red line is the minor axis of the ellipse. In the center figure, the ellipse has been rotated clockwise by the rotate parameter so the major axis is the transformed x-axis and the minor axis is the transformed y-axis. In the right figure, the minor axis of the ellipse has been scaled by the reciprocal of the scale parameter so that the ellipse becomes a circle, which corresponds to an isotropic spatial covariance function. The transformed coordinates are then used to compute distances and spatial covariances." width="33%"><img src="technical_files/figure-html/anisotropy2-3.png" alt="In the left figure, the ellipse of an anisotropic spatial covariance function centered at the origin is shown. The blue lines represent the original axes and the red lines the transformed axes. The solid lines represent the x-axes and the dotted lines the y-axes. Note that the solid, red line is the major axis of the ellpise and the dashed, red line is the minor axis of the ellipse. In the center figure, the ellipse has been rotated clockwise by the rotate parameter so the major axis is the transformed x-axis and the minor axis is the transformed y-axis. In the right figure, the minor axis of the ellipse has been scaled by the reciprocal of the scale parameter so that the ellipse becomes a circle, which corresponds to an isotropic spatial covariance function. The transformed coordinates are then used to compute distances and spatial covariances." width="33%"><p class="caption">
In the left figure, the ellipse of an anisotropic spatial covariance
function centered at the origin is shown. The blue lines represent the
original axes and the red lines the transformed axes. The solid lines
represent the x-axes and the dotted lines the y-axes. Note that the
solid, red line is the major axis of the ellpise and the dashed, red
line is the minor axis of the ellipse. In the center figure, the ellipse
has been rotated clockwise by the rotate parameter so the major axis is
the transformed x-axis and the minor axis is the transformed y-axis. In
the right figure, the minor axis of the ellipse has been scaled by the
reciprocal of the scale parameter so that the ellipse becomes a circle,
which corresponds to an isotropic spatial covariance function. The
transformed coordinates are then used to compute distances and spatial
covariances.
</p>
</div>
<p>Anisotropy parameters (<span class="math inline">\(\alpha\)</span>
and <span class="math inline">\(S\)</span>) can be estimated in
<code>spmodel</code> using restricted maximum likelihood or maximum
likelihood. Estimating anisotropy can be challenging. First, we need to
restrict the parameter space so that the two parameters are identifiable
(there is a unique parameter set for each possible outcome). We
restricted <span class="math inline">\(\alpha\)</span> to <span class="math inline">\([0, \pi]\)</span> radians due to symmetry of the
covariance ellipse at rotations <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\alpha + j \pi\)</span>, where <span class="math inline">\(j\)</span> is any integer. We also restricted
<span class="math inline">\(S\)</span> to <span class="math inline">\([0, 1]\)</span> because we have defined <span class="math inline">\(S\)</span> as the scaling factor for the length of
the minor axis relative to the major axis – otherwise it would not be
clear whether <span class="math inline">\(S\)</span> refers to the minor
or major axis. Given this restricted parameter space, there is still an
issue of local maxima, particularly at rotation parameters near zero,
which have a rotation very close to rotation parameter <span class="math inline">\(\pi\)</span>, but zero is far from <span class="math inline">\(\pi\)</span> in the parameter space. To address
the local maxima problem, each optimization iteration actually involves
two likelihood evaluations – one for <span class="math inline">\(\alpha\)</span> and another for <span class="math inline">\(|\pi - \alpha|\)</span>, where <span class="math inline">\(|\cdot|\)</span> denotes absolute value. Thus one
likelihood evaluation is always in <span class="math inline">\([0,
\pi/2]\)</span> radians and another in <span class="math inline">\([\pi/2, \pi]\)</span> radians, exploring different
quadrants of the parameter space and allowing optimization to test
solutions near zero and <span class="math inline">\(\pi\)</span>
simultaneously.</p>
<p>Anisotropy parameters cannot be estimated in <code>spmodel</code>
when <code>estmethod</code> is <code>sv-wls</code> or
<code>sv-cl</code>. However, known anisotropy parameters for these
estimation methods can be specified via <code>spcov_initial</code> and
incorporated into estimation of <span class="math inline">\(\boldsymbol{\theta}\)</span> and <span class="math inline">\(\boldsymbol{\beta}\)</span>. Anisotropy is not
defined for areal data given its (binary) neighborhood structure.</p>
</div>
<div class="section level4">
<h4 id="sec:partition-lm">Partition Factors<a class="anchor" aria-label="anchor" href="#sec:partition-lm"></a>
</h4>
<p>A partition factor is a factor (or categorical) variable in which
observations from different levels of the partition factor are assumed
uncorrelated. A partition matrix <span class="math inline">\(\mathbf{P}\)</span> of dimension <span class="math inline">\(n \times n\)</span> can be constructed to
represent the partition factor. The <span class="math inline">\(ij\)</span>th element of <span class="math inline">\(\mathbf{P}\)</span> equals one if the observation
in the <span class="math inline">\(i\)</span>th row and <span class="math inline">\(j\)</span>th column are from the same level of the
partition factor and zero otherwise. Then the initial covariance matrix
(ignoring the partition factor) is updated by taking the Hadmard
(element-wise) product with the partition matrix: <span class="math display">\[\begin{equation*}
\boldsymbol{\Sigma}_{updated} = \boldsymbol{\Sigma}_{initial} \odot
\mathbf{P},
\end{equation*}\]</span> where <span class="math inline">\(\odot\)</span> indicates the Hadmard product.
Partition factors impose a block structure in <span class="math inline">\(\boldsymbol{\Sigma}\)</span>, which allows for
efficient computation of <span class="math inline">\(\boldsymbol{\Sigma}^{-1}\)</span> used for
estimation and prediction.</p>
<p>When computing the empirical semivariogram using <code><a href="../reference/esv.html">esv()</a></code>,
semivariances are ignored when observations are from different levels of
the partition factor. For the <code>sv-wls</code> and <code>sv-cl</code>
estimation methods, semivariances are ignored when observations are from
different levels of the partition factor.</p>
</div>
<div class="section level4">
<h4 id="sec:bigdata-lm">Big Data (<code>splm()</code> only)<a class="anchor" aria-label="anchor" href="#sec:bigdata-lm"></a>
</h4>
<p>Big data model-fitting is accommodated in <code>spmodel</code> using
a “local” spatial indexing (SPIN) approach <span class="citation">(Ver
Hoef et al. 2023)</span>. Suppose there are <span class="math inline">\(m\)</span> unique indexes, and each observation
has one of the indexes. Then <span class="math inline">\(\boldsymbol{\Sigma}\)</span> can be represented
blockwise as <span class="math display">\[\begin{equation}\label{eq:full_cov}
  \boldsymbol{\Sigma} =
  \begin{bmatrix}
  \boldsymbol{\Sigma}_{1,1} &amp; \boldsymbol{\Sigma}_{1,2} &amp; \ldots
&amp; \ldots &amp; \boldsymbol{\Sigma}_{1,m} \\
  \boldsymbol{\Sigma}_{2,1} &amp; \boldsymbol{\Sigma}_{2,2} &amp;
\boldsymbol{\Sigma}_{2,3} &amp; \ldots &amp; \boldsymbol{\Sigma}_{2,m}
\\
  \vdots &amp; \boldsymbol{\Sigma}_{3,2} &amp; \ddots &amp;
\boldsymbol{\Sigma}_{3,4} &amp; \vdots \\
  \vdots &amp; \vdots &amp; \boldsymbol{\Sigma}_{4,3} &amp; \ddots &amp;
\vdots \\
  \boldsymbol{\Sigma}_{m,1} &amp; \ldots &amp; \ldots &amp; \ldots &amp;
\boldsymbol{\Sigma}_{m, m}
  \end{bmatrix},
\end{equation}\]</span> To perform estimation for big data, observations
with the same index value are assumed independent of observations with
different index values, yielding a “big-data” covariance matrix given by
<span class="math display">\[\begin{equation}\label{eq:bd_cov}
  \boldsymbol{\Sigma}_{bd} =
  \begin{bmatrix}
  \boldsymbol{\Sigma}_{1,1} &amp; \boldsymbol{0} &amp; \ldots &amp;
\ldots &amp; \boldsymbol{0} \\
  \boldsymbol{0} &amp; \boldsymbol{\Sigma}_{2,2} &amp; \boldsymbol{0}
&amp; \ldots &amp; \boldsymbol{0} \\
  \vdots &amp; \boldsymbol{0} &amp; \ddots &amp; \boldsymbol{0} &amp;
\vdots \\
  \vdots &amp; \vdots &amp; \boldsymbol{0} &amp; \ddots &amp; \vdots \\
  \boldsymbol{0} &amp; \ldots &amp; \ldots &amp; \ldots &amp;
\boldsymbol{\Sigma}_{m, m}
  \end{bmatrix},
\end{equation}\]</span> Estimation then proceeds using <span class="math inline">\(\boldsymbol{\Sigma}_{bd}\)</span> instead of <span class="math inline">\(\boldsymbol{\Sigma}\)</span>. When computing the
empirical semivariogram, semivariances are ignored when observations
have different local indexes. For the <code>sv-wls</code> and
<code>sv-cl</code> estimation methods, semivariances are ignored when
observations have different local indexes. Via this equation, it can be
seen that the local index acts as a partition factor separate from the
partition factor explicitly defined by
<code>partition_factor</code>.</p>
<p>spmodel allows for custom local indexes to be passed to
<code><a href="../reference/splm.html">splm()</a></code>. If a custom local index is not passed, the local
index is determined using the <code>"random"</code> or
<code>"kmeans"</code> method. The <code>"random"</code> method assigns
observations to indexes randomly based on the number of groups desired.
The <code>"kmeans"</code> method uses k-means clustering <span class="citation">(MacQueen 1967)</span> on the x-coordinates and
y-coordinates to assign observations to indexes (based on the number of
clusters (groups) desired).</p>
<p>The estimate of <span class="math inline">\(\boldsymbol{\beta}\)</span> when using <span class="math inline">\(\boldsymbol{\Sigma}_{bd}\)</span> is given by
<span class="math display">\[\begin{equation}\label{eq:beta_bd}
  \hat{\boldsymbol{\beta}}_{bd} = (\mathbf{X}^\top
\boldsymbol{\hat{\Sigma}}^{-1}_{bd}\mathbf{X})^{-1}\mathbf{X}^\top
\boldsymbol{\hat{\Sigma}}^{-1}_{bd} \mathbf{y} =
\mathbf{T}^{-1}_{xx}\mathbf{t}_{xy},
\end{equation}\]</span> where <span class="math inline">\(\mathbf{T}_{xx} = \sum_{i = 1}^m \mathbf{X}_i^\top
\boldsymbol{\hat{\Sigma}}^{-1}_{i, i}\mathbf{X}_i\)</span> and <span class="math inline">\(\mathbf{t}_{xy} = \sum_{i = 1}^m \mathbf{X}_i^\top
\hat{\boldsymbol{\Sigma}}^{-1}_{i, i} \mathbf{y}_i\)</span>. Note that
in <span class="math inline">\(\hat{\boldsymbol{\beta}}_{bd}\)</span>,
<span class="math inline">\(\mathbf{X}_i\)</span> and <span class="math inline">\(\mathbf{y}_i\)</span> are the subsets of <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{y}\)</span>, respectively, for the <span class="math inline">\(i\)</span>th local index. This estimator acts as a
pooled estimator of <span class="math inline">\(\boldsymbol{\beta}\)</span> across the
indexes.</p>
<p><code>spmodel</code> has four approaches for estimating the
covariance matrix of <span class="math inline">\(\hat{\boldsymbol{\beta}}_{bd}\)</span>. The choice
is determined by the <code>var_adjust</code> argument to
<code>local</code>. The first approach is implements no adjustment
(<code>var_adjust = "none"</code>) and simply uses <span class="math inline">\(\mathbf{T}_{xx}^{-1}\)</span>, which is the
covariance matrix of <span class="math inline">\(\hat{\boldsymbol{\beta}}_{bd}\)</span> using <span class="math inline">\(\boldsymbol{\Sigma}_{bd}\)</span>. While
computationally efficient, this approach ignores the covariance across
indexes. It can be shown that the covariance of <span class="math inline">\(\hat{\boldsymbol{\beta}}_{bd}\)</span> using <span class="math inline">\(\boldsymbol{\Sigma}\)</span>, the full covariance
matrix, is given by <span class="math display">\[\begin{equation}\label{eq:var_theo}
  \mathbf{T}_{xx}^{-1} + \mathbf{T}_{xx}^{-1}
\mathbf{W}_{xx}\mathbf{T}_{xx}^{-1},
\end{equation}\]</span> where <span class="math display">\[\begin{equation*}
\mathbf{W} = \sum_{i = 1}^{m - 1} \sum_{j = i + 1}^m (\mathbf{X}^\top
\hat{\boldsymbol{\Sigma}}^{-1}_{i, i} \hat{\boldsymbol{\Sigma}}_{i, j}
\hat{\boldsymbol{\Sigma}}^{-1}_{j, j} \mathbf{X}_j) + (\mathbf{X}^\top
\hat{\boldsymbol{\Sigma}}^{-1}_{i, i} \hat{\boldsymbol{\Sigma}}_{i, j}
\hat{\boldsymbol{\Sigma}}^{-1}_{j, j} \mathbf{X}_j)^\top
\end{equation*}\]</span> This equation can be viewed as the sum of the
unadjusted covariance matrix of <span class="math inline">\(\hat{\boldsymbol{\beta}}_{bd}\)</span> (<span class="math inline">\(\mathbf{T}_{xx}^{-1}\)</span>) and a correction
that incorporates the covariance across indexes (<span class="math inline">\(\mathbf{T}_{xx}^{-1}
\mathbf{W}_{xx}\mathbf{T}_{xx}^{-1}\)</span>). This adjustment is known
as the “theoretically-correct” (<code>var_adjust = "theoretical"</code>)
adjustment because it uses <span class="math inline">\(\boldsymbol{\Sigma}\)</span>. The theoretical
adjustment is the default adjustment in <code>spmodel</code> because it
is theoretically correct, but it is the most computationally expensive
adjustment. Two alternative adjustments are also provided, and while not
equal to the theoretical adjustment, they are easier to compute. They
are the empirical (<code>var_adjust = "empirical"</code>) and pooled
(<code>var_adjust = "pooled"</code>) adjustments. The empirical
adjustment is given by <span class="math display">\[\begin{equation*}
\frac{1}{m(m -1)} \sum_{i = 1}^m (\boldsymbol{\hat{\beta}}_i -
\boldsymbol{\hat{\beta}}_{bd})(\boldsymbol{\hat{\beta}}_i -
\boldsymbol{\hat{\beta}}_{bd})^\top,
\end{equation*}\]</span> where <span class="math inline">\(\boldsymbol{\hat{\beta}}_i = (\mathbf{X}^\top
\hat{\boldsymbol{\Sigma}}^{-1} \mathbf{X})^{-1}\mathbf{X}_i^\top
\hat{\boldsymbol{\Sigma}}^{-1}_{i, i} \mathbf{y}_i\)</span>. A similar
adjustment could use <span class="math inline">\(\boldsymbol{\hat{\beta}}_i = (\mathbf{X}_i^\top
\hat{\boldsymbol{\Sigma}}^{-1}_{i, i} \mathbf{X}_i)^{-1}\mathbf{X}_i
\hat{\boldsymbol{\Sigma}}^{-1}_{i, i} \mathbf{y}_i\)</span>, which more
closely resembles a composite likelihood approach. This approach is
sensitive to the presence of at least one singularity in <span class="math inline">\(\mathbf{X}_i^\top
\hat{\boldsymbol{\Sigma}}^{-1}_{i, i} \mathbf{X}_i\)</span>, in which
case the variance adjustment cannot be computed. The
<code>"pooled"</code> variance adjustment is given by <span class="math display">\[\begin{equation*}
\frac{1}{m^2} \sum_{i = 1}^m (\mathbf{X}^\top_i
\hat{\boldsymbol{\Sigma}}^{-1}_{i, i} \mathbf{X}_i)^{-1}.
\end{equation*}\]</span> Note that the pooled variance adjustment cannot
be computed if any <span class="math inline">\(\mathbf{X}_i^\top
\hat{\boldsymbol{\Sigma}}^{-1}_{i, i} \mathbf{X}_i\)</span> are
singular.</p>
</div>
</div>
<div class="section level3">
<h3 id="sec:rf-lm">
<code>splmRF()</code> and <code>spautorRF()</code><a class="anchor" aria-label="anchor" href="#sec:rf-lm"></a>
</h3>
<p><code><a href="../reference/splmRF.html">splmRF()</a></code> and <code><a href="../reference/spautorRF.html">spautorRF()</a></code> fit random forest
spatial residual models designed for prediction. These models are fit by
combining aspects of random forest and spatial linear modeling. First, a
random forest model <span class="citation">(Breiman 2001; James et al.
2013)</span> is fit using the <code>ranger</code> <strong>R</strong>
package <span class="citation">(Wright and Ziegler 2015)</span>. Then
random forest fitted values are obtained for each data observation and
used to compute a residual (by subtracting the fitted value from the
observed value). Then an intercept-only spatial linear model is fit to
these residuals: <span class="math display">\[\begin{equation*}
  \mathbf{e}_{rf} = \beta_0 + \mathbf{\tau} + \mathbf{\epsilon},
\end{equation*}\]</span> where <span class="math inline">\(\mathbf{e}_{rf}\)</span> are the random forest
residuals. Random forest spatial residual models can significantly
improve predictive accuracy for new data compared to standard random
forest models by formally incorporating spatial covariance in the random
forest residuals <span class="citation">(Fox, Ver Hoef, and Olsen
2020)</span>.</p>
<p>Different estimation methods, different spatial covariance functions,
fixing spatial covariance parameter values, random effects, anisotropy,
partition factors, and big data are accommodated in the spatial linear
model portion of the random forest spatial residual models by supplying
their respective named arguments to <code><a href="../reference/splmRF.html">splmRF()</a></code> and
<code><a href="../reference/spautorRF.html">spautorRF()</a></code>.</p>
</div>
<div class="section level3">
<h3 id="sec:sprnorm">
<code>sprnorm()</code><a class="anchor" aria-label="anchor" href="#sec:sprnorm"></a>
</h3>
<p>Spatial normal (Gaussian) random variables are simulated by taking
the sum of a fixed mean and random errors. The random errors have mean
zero and covariance matrix <span class="math inline">\(\boldsymbol{\Sigma}\)</span>. A realization of the
random errors is obtained from <span class="math inline">\(\boldsymbol{\Sigma}^{1/2} \mathbf{e}\)</span>,
where <span class="math inline">\(\mathbf{e}\)</span> is a normal random
variable with mean zero and covariance matrix <span class="math inline">\(\mathbf{I}\)</span>. Then the spatial normal
random variable equals <span class="math display">\[\begin{equation*}
\mathbf{y} = \boldsymbol{\mu} + \boldsymbol{\Sigma}^{1/2} \mathbf{e},
\end{equation*}\]</span> where <span class="math inline">\(\boldsymbol{\mu}\)</span> is the fixed mean. It
follows that <span class="math display">\[\begin{equation*}
  \begin{split}
  \text{E}(\mathbf{y}) &amp; = \boldsymbol{\mu} +
\boldsymbol{\Sigma}^{1/2} \text{E}(\mathbf{e}) = \boldsymbol{\mu} \\
  \text{Cov}(\mathbf{y}) &amp; = \text{Cov}(\boldsymbol{\Sigma}^{1/2}
\mathbf{e}) = \boldsymbol{\Sigma}^{1/2} \text{Cov}(\mathbf{e})
\boldsymbol{\Sigma}^{1/2} = \boldsymbol{\Sigma}^{1/2}
\boldsymbol{\Sigma}^{1/2} = \boldsymbol{\Sigma}
  \end{split}
\end{equation*}\]</span></p>
</div>
<div class="section level3">
<h3 id="sec:vcov-lm">
<code>vcov()</code><a class="anchor" aria-label="anchor" href="#sec:vcov-lm"></a>
</h3>
<p><code><a href="https://rdrr.io/r/stats/vcov.html" class="external-link">vcov()</a></code> returns the variance-covariance matrix of
estimated parameters. Currently, <code><a href="https://rdrr.io/r/stats/vcov.html" class="external-link">vcov()</a></code> only returns the
variance-covariance matrix of <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>, the fixed
effects. The variance-covariance matrix of the fixed effects is given by
<span class="math inline">\((\mathbf{X}^\top
\hat{\boldsymbol{\Sigma}}^{-1} \mathbf{X})^{-1}\)</span>.</p>
</div>
</div>
<div class="section level2">
<h2 id="sec:spglms">Spatial Generalized Linear Models<a class="anchor" aria-label="anchor" href="#sec:spglms"></a>
</h2>
<p>When building spatial linear models, the response vector <span class="math inline">\(\mathbf{y}\)</span> is typically assumed Gaussian
(given <span class="math inline">\(\mathbf{X}\)</span>). Relaxing this
assumption on the distribution of <span class="math inline">\(\mathbf{y}\)</span> yields a rich class of spatial
generalized linear models that can describe binary data, proportion
data, count data, and skewed data that is parameterized as <span class="math display">\[\begin{equation}\label{eq:spglm}
g(\boldsymbol{\mu}) = \boldsymbol{\eta} = \mathbf{X} \boldsymbol{\beta}
+ \boldsymbol{\tau} + \boldsymbol{\epsilon},
\end{equation}\]</span> where <span class="math inline">\(g(\cdot)\)</span> is called a link function, <span class="math inline">\(\boldsymbol{\mu}\)</span> is the mean of <span class="math inline">\(\mathbf{y}\)</span>, and the remaining terms <span class="math inline">\(\mathbf{X}\)</span>, <span class="math inline">\(\boldsymbol{\beta}\)</span>, <span class="math inline">\(\boldsymbol{\tau}\)</span>, <span class="math inline">\(\boldsymbol{\epsilon}\)</span> represent the same
quantities as for the spatial linear models. The link function, <span class="math inline">\(g(\cdot)\)</span>, “links” a function of <span class="math inline">\(\boldsymbol{\mu}\)</span> to the linear term <span class="math inline">\(\boldsymbol{\eta}\)</span> , denoted here as <span class="math inline">\(\mathbf{X} \boldsymbol{\beta} + \boldsymbol{\tau}
+ \boldsymbol{\epsilon}\)</span>, which is familiar from spatial linear
models. Note that the linking of <span class="math inline">\(\boldsymbol{\mu}\)</span> to <span class="math inline">\(\boldsymbol{\eta}\)</span> applies element-wise to
each vector. Each link function <span class="math inline">\(g(\cdot)\)</span> has a corresponding inverse link
function, <span class="math inline">\(g^{-1}(\cdot)\)</span>. The
inverse link function “links” a function of <span class="math inline">\(\boldsymbol{\eta}\)</span> to <span class="math inline">\(\boldsymbol{\mu}\)</span>. Notice that for spatial
generalized linear models, we are not modeling <span class="math inline">\(\mathbf{y}\)</span> directly as we do for spatial
linear models, but rather we are modeling a function of the mean of
<span class="math inline">\(\mathbf{y}\)</span>. Also notice that <span class="math inline">\(\boldsymbol{\eta}\)</span> is unconstrained but
<span class="math inline">\(\boldsymbol{\mu}\)</span> is usually
constrained in some way (e.g., positive).</p>
<p>The model <span class="math inline">\(g(\boldsymbol{\mu}) =
\boldsymbol{\eta} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\tau} +
\boldsymbol{\epsilon}\)</span> is called the spatial generalized linear
model. <code>spmodel</code> allows fitting of spatial generalized linear
models when <span class="math inline">\(\mathbf{y}\)</span> is a
binomial (or Bernoulli), beta, Poisson, negative binomial, gamma, or
inverse Gaussian random vector via the Laplace approximation and
restricted maximum likelihood estimation or maximum likelihood
estimation – <span class="citation">Ver Hoef et al. (2024)</span>
provide further details. For binomial and beta <span class="math inline">\(\mathbf{y}\)</span>, the logit link function is
defined as <span class="math inline">\(g(\boldsymbol{\mu}) =
\ln(\frac{\boldsymbol{\mu}}{1 - \boldsymbol{\mu}}) =
\boldsymbol{\eta}\)</span>, and the inverse logit link function is
defined as <span class="math inline">\(g^{-1}(\boldsymbol{\eta}) =
\frac{\exp(\boldsymbol{\eta})}{1 + \exp(\boldsymbol{\eta})} =
\boldsymbol{\mu}\)</span>. For Poisson, negative binomial, gamma, and
inverse Gaussian <span class="math inline">\(\mathbf{y}\)</span>, the
log link function is defined as <span class="math inline">\(g(\boldsymbol{\mu}) = log(\boldsymbol{\mu}) =
\boldsymbol{\eta}\)</span>, and the inverse log link function is defined
as <span class="math inline">\(g^{-1}(\boldsymbol{\eta}) =
\exp(\boldsymbol{\eta}) = \boldsymbol{\mu}\)</span>. Full
parameterizations of these distributions are given later.</p>
<p>For spatial linear models, one can marginalize over <span class="math inline">\(\boldsymbol{\beta}\)</span> and the random
components to obtain an explicit distribution of only the data (<span class="math inline">\(\mathbf{y}\)</span>) and covariance parameters
(<span class="math inline">\(\boldsymbol{\theta}\)</span>) – this is the
REML likelihood. For spatial generalized linear models, this
marginalization is more challenging. First define <span class="math inline">\(\mathbf{w} = \mathbf{X} \boldsymbol{\beta} +
\boldsymbol{\tau} + \boldsymbol{\epsilon}\)</span>. Our goal is to
marginalize the joint distribution of <span class="math inline">\(\mathbf{y}\)</span> and <span class="math inline">\(\mathbf{w}\)</span> over <span class="math inline">\(\mathbf{w}\)</span> (and <span class="math inline">\(\boldsymbol{\beta}\)</span>) to obtain a
distribution of only the data (<span class="math inline">\(\mathbf{y}\)</span>), a dispersion parameter
(<span class="math inline">\(\varphi\)</span>), and covariance
parameters (<span class="math inline">\(\boldsymbol{\theta}\)</span>).
To accomplish this feat, we use a hierarchical construction that treats
the <span class="math inline">\(\mathbf{w}\)</span> as latent (i.e.,
unobserved) variables and then use the Laplace approximation to perform
integration. We briefly describe this approach next, but it is described
in full detail in <span class="citation">Ver Hoef et al.
(2024)</span>.</p>
<p>The marginal distribution of interest can be written hierarchically
as <span class="math display">\[\begin{equation*}
  [\mathbf{y}|\mathbf{X}, \varphi, \boldsymbol{\theta}] =
\int_\mathbf{w} [\mathbf{y} | g^{-1}(\mathbf{w}), \varphi] [\mathbf{w} |
\mathbf{X}, \boldsymbol{\theta}] d\mathbf{w} .
\end{equation*}\]</span> The term <span class="math inline">\([\mathbf{y}|g^{-1}(\mathbf{w}), \varphi]\)</span>
is likelihood of the generalized linear model with mean function <span class="math inline">\(g^{-1}(\mathbf{w})\)</span>, and the term <span class="math inline">\([\mathbf{w}|\mathbf{X},
\boldsymbol{\theta}]\)</span> is the restricted log likelihood for <span class="math inline">\(\mathbf{w}\)</span> given the covariance
parameters.</p>
<p>Next define <span class="math inline">\(\ell_\mathbf{w} =
\ln([\mathbf{y} | g^{-1}(\mathbf{w}), \varphi] [\mathbf{w} | \mathbf{X},
\boldsymbol{\theta}])\)</span>. Let <span class="math inline">\(\mathbf{g}\)</span> be the gradient vector where
<span class="math inline">\(g_i = \frac{\partial
\ell_\mathbf{w}}{\partial w_i}\)</span> and <span class="math inline">\(\mathbf{G}\)</span> be the Hessian matrix with
<span class="math inline">\(ij\)</span>th element <span class="math inline">\(G_{i, j} = \frac{\partial^2
\ell_\mathbf{w}}{\partial w_i \partial w_j}\)</span>. Using a
multivariate Taylor series expansion around some point <span class="math inline">\(\mathbf{a}\)</span>, <span class="math display">\[\begin{equation*}
  \int_\mathbf{w} \exp(\ell_\mathbf{w}) d\mathbf{w} \approx
\int_\mathbf{w} \exp(\ell_\mathbf{a} + \mathbf{g}^\top(\mathbf{w} -
\mathbf{a}) + \frac{1}{2}(\mathbf{w} - \mathbf{a})^\top \mathbf{G}
(\mathbf{w} - \mathbf{a})) d\mathbf{w}.
\end{equation*}\]</span> If <span class="math inline">\(\mathbf{a}\)</span> is the value at which <span class="math inline">\(\mathbf{g} = \mathbf{0}\)</span>, then <span class="math display">\[\begin{equation*}
  \int_\mathbf{w} \exp(\ell_\mathbf{w}) d\mathbf{w} \approx
\exp(\ell_\mathbf{a}) \int_\mathbf{w} \exp \left(-\frac{1}{2}
(\mathbf{w} - \mathbf{a})^\top (- \mathbf{G})(\mathbf{w} - \mathbf{a})
\right) d\mathbf{w} = \exp(\ell_\mathbf{a}) (2 \pi)^{n/2}
|-\mathbf{G}_a|^{-1/2},
\end{equation*}\]</span> where <span class="math inline">\(\mathbf{G}_a\)</span> is the Hessian evaluated at
<span class="math inline">\(\mathbf{a}\)</span> and <span class="math inline">\(|\cdot|\)</span> is the determinant operator. The
previous result follows from the normalizing constant of a Gaussian
distribution with kernel <span class="math inline">\(\exp(\frac{1}{2}
(\mathbf{w} - \mathbf{a})^\top [(- \mathbf{G})^{-1}]^{-1}(\mathbf{w} -
\mathbf{a}))\)</span>. Finally, we arrive at <span class="math display">\[\begin{equation*}
  \int_\mathbf{w} \exp(\ell_\mathbf{w}) d\mathbf{w} \approx [\mathbf{y}
| g^{-1}(\mathbf{a}), \varphi] [\mathbf{a} | \mathbf{X},
\boldsymbol{\theta}] (2 \pi)^{n/2} |-\mathbf{G}_a|^{-1/2}|,
\end{equation*}\]</span> which is a distribution that has been
marginalized over the latent <span class="math inline">\(\mathbf{w}\)</span> and depends only on the data,
a dispersion parameter, and the covariance parameters. The approach we
outlined to solve this integral is known as the Laplace
approximation.</p>
<div class="section level3">
<h3 id="sec:aic-glm">
<code>AIC()</code> and <code>AICc()</code><a class="anchor" aria-label="anchor" href="#sec:aic-glm"></a>
</h3>
<p><code><a href="https://rdrr.io/r/stats/AIC.html" class="external-link">AIC()</a></code> and <code><a href="../reference/AICc.html">AICc()</a></code> for spatial generalized
linear models is defined the same as for spatial linear models.</p>
</div>
<div class="section level3">
<h3 id="sec:anova-glm">
<code>anova()</code><a class="anchor" aria-label="anchor" href="#sec:anova-glm"></a>
</h3>
<p><code><a href="https://rdrr.io/r/stats/anova.html" class="external-link">anova()</a></code> for spatial generalized linear models is defined
the same as for spatial linear models.</p>
</div>
<div class="section level3">
<h3 id="sec:auroc">
<code>AUROC()</code><a class="anchor" aria-label="anchor" href="#sec:auroc"></a>
</h3>
<p><code><a href="../reference/AUROC.html">AUROC()</a></code> is the area under the receiver operating
characteristic curve and is relevant for binomial (i.e., logistic)
regression models where each response represents a single success or
failure (i.e.., is binary). The AUROC ranges from zero to one and is a
measure of the model’s classification accuracy averaged over all
possible threshold values. More formally, it represents the probability
that a randomly chosen success (datum value of one) has a larger fitted
value than the fitted value of a randomly chosen failure (datum value of
zero), with an adjustment for ties in the fitted values <span class="citation">(Muschelli III 2020)</span>. <code><a href="../reference/AUROC.html">AUROC()</a></code> in
<code>spmodel</code> leverages the <code>auc()</code> function in
<code>pROC</code> <span class="citation">Robin et al. (2011)</span>. For
more on the AUROC, see <span class="citation">Hanley and McNeil
(1982)</span> and <span class="citation">Fawcett (2006)</span>.</p>
</div>
<div class="section level3">
<h3 id="sec:bic-glm">
<code>BIC()</code><a class="anchor" aria-label="anchor" href="#sec:bic-glm"></a>
</h3>
<p><code><a href="https://rdrr.io/r/stats/AIC.html" class="external-link">BIC()</a></code> for spatial generalized linear models is defined
the same as for spatial linear models.</p>
</div>
<div class="section level3">
<h3 id="sec:coef-glm">
<code>coef()</code><a class="anchor" aria-label="anchor" href="#sec:coef-glm"></a>
</h3>
<p><code><a href="https://rdrr.io/r/stats/coef.html" class="external-link">coef()</a></code> for spatial generalized linear models is defined
the same as for spatial linear models.</p>
</div>
<div class="section level3">
<h3 id="sec:confint-glm">
<code>confint()</code><a class="anchor" aria-label="anchor" href="#sec:confint-glm"></a>
</h3>
<p><code><a href="https://rdrr.io/r/stats/confint.html" class="external-link">confint()</a></code> for spatial generalized linear models is
defined the same as for spatial linear models.</p>
</div>
<div class="section level3">
<h3 id="sec:cooks-glm">
<code>cooks.distance()</code><a class="anchor" aria-label="anchor" href="#sec:cooks-glm"></a>
</h3>
<p>The Cook’s distance is defined as the standard generalized linear
model Cook’s distance after conditioning on <span class="math inline">\(\mathbf{w}\)</span>. That is, after conditioning
on <span class="math inline">\(\mathbf{w}\)</span>, the Cook’s distance
is <span class="math display">\[\begin{equation}
\frac{\mathbf{e}_p^2}{p} \odot diag(\mathbf{H}_c) \odot \frac{1}{1 -
diag(\mathbf{H}_c)},
\end{equation}\]</span> where <span class="math inline">\(\mathbf{e}_p^2\)</span> are the Pearson residuals,
<span class="math inline">\(diag(\mathbf{H}_c)\)</span> is the diagonal
of the leverage (hat) matrix conditional on <span class="math inline">\(\mathbf{w}\)</span> and given by <span class="math inline">\(\mathbf{H}_c = \mathbf{X}_v
(\mathbf{X}_v^\top\mathbf{X}_v)^{-1} \mathbf{X}_v^\top\)</span>, where
<span class="math inline">\(\mathbf{X}_v =
\mathbf{V}^{1/2}\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{V}\)</span> is a diagonal matrix with
<span class="math inline">\(i\)</span>th diagonal element equal to <span class="math inline">\(\text{Var}(w_i)\)</span>, and <span class="math inline">\(\odot\)</span> denotes the Hadmard (element-wise)
product.</p>
</div>
<div class="section level3">
<h3 id="sec:deviance-glm">
<code>deviance()</code><a class="anchor" aria-label="anchor" href="#sec:deviance-glm"></a>
</h3>
<p>The deviance is defined conditional on <span class="math inline">\(\mathbf{w}\)</span> (i.e., we find the deviance of
the conditional model). It is derived by taking the deviance definitions
from each generalized linear models distribution (defined later) and
evaluating <span class="math inline">\(\mu_i\)</span> at <span class="math inline">\(w_i\)</span>.</p>
</div>
<div class="section level3">
<h3 id="sec:fitted-glm">
<code>fitted()</code><a class="anchor" aria-label="anchor" href="#sec:fitted-glm"></a>
</h3>
<p>The fitted values on the link scale (<code>type = "link"</code>) are
given by <span class="math inline">\(\mathbf{w}\)</span>. The fitted
values on the response scale (<code>type = "response"</code>) are given
by <span class="math inline">\(g^{-1}(\mathbf{w})\)</span>. The fitted
values for the spatial random errors (<code>type = "spcov"</code>) and
random effects (<code>type = "randcov"</code>) are derived similarly as
for spatial linear models but treat <span class="math inline">\(\mathbf{w}\)</span> as the response instead of
<span class="math inline">\(\mathbf{y}\)</span> (and are on the link
scale).</p>
</div>
<div class="section level3">
<h3 id="sec:hatvalues-glm">
<code>hatvalues()</code><a class="anchor" aria-label="anchor" href="#sec:hatvalues-glm"></a>
</h3>
<p>The leverage (hat) matrix is obtained by finding the standard
generalized linear model leverage (hat) matrix after conditioning on
<span class="math inline">\(\mathbf{w}\)</span>. That is, after
conditioning on <span class="math inline">\(\mathbf{w}\)</span> the
leverage (hat) matrix, <span class="math inline">\(\mathbf{H}_c\)</span>, is <span class="math display">\[\begin{equation}
  \mathbf{H}_c = \mathbf{X}_v (\mathbf{X}_v^\top\mathbf{X}_v)^{-1}
\mathbf{X}_v^\top,
\end{equation}\]</span> where <span class="math inline">\(\mathbf{X}_v =
\mathbf{V}^{1/2}\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{V}\)</span> is a diagonal matrix with
<span class="math inline">\(i\)</span>th diagonal element equal to <span class="math inline">\(\text{Var}(\text{w}_i)\)</span>.</p>
</div>
<div class="section level3">
<h3 id="sec:loglik-glm">
<code>logLik()</code><a class="anchor" aria-label="anchor" href="#sec:loglik-glm"></a>
</h3>
<p><code><a href="https://rdrr.io/r/stats/logLik.html" class="external-link">logLik()</a></code> for spatial generalized linear models is
defined the same as for spatial linear models, in that the
log-likelihood is returned. The log-likelihood for spatial generalized
linear models is given by <span class="math display">\[\begin{equation*}
  \ell_p(\varphi, \boldsymbol{\theta}) = \ln([\mathbf{y} |
g^{-1}(\mathbf{a}), \varphi]) + \ln([\mathbf{a} | \mathbf{X},
\boldsymbol{\theta}]) + \frac{n}{2} \ln(2 \pi) - \frac{1}{2} \ln(|-
\mathbf{G}_{\mathbf{a}}|).
\end{equation*}\]</span></p>
</div>
<div class="section level3">
<h3 id="sec:loocv-glm">
<code>loocv()</code><a class="anchor" aria-label="anchor" href="#sec:loocv-glm"></a>
</h3>
<p><code><a href="../reference/loocv.html">loocv()</a></code> for spatial generalized linear models is defined
similarly as for spatial linear models, except that <span class="math inline">\(\mathbf{w}\)</span> is predicted instead of <span class="math inline">\(\mathbf{y}\)</span>. Then <span class="math inline">\(g^{-1}(\mathbf{w})\)</span> is compared to <span class="math inline">\(\mathbf{y}\)</span> to compute
mean-squared-prediction-error. That is, <span class="math display">\[\begin{equation*}
mspe = \frac{1}{n}\sum_{i = 1}^n(y_i - g^{-1}(w_i))^2.
\end{equation*}\]</span></p>
<p>When <code>cv_fitted = TRUE</code>, the predictions of held-out <span class="math inline">\(\mathbf{w}\)</span> are returned. The standard
errors of these predictions (of held-out <span class="math inline">\(\mathbf{w}\)</span>) are returned when
<code>se.fit = TRUE</code>. Note that <span class="math inline">\(\mathbf{G}_{-i}\)</span> is determined from <span class="math inline">\(\mathbf{G}\)</span> using Helmert-Wolf blocking as
is done for <span class="math inline">\(\boldsymbol{\Sigma}_{-i}\)</span> and <span class="math inline">\(\boldsymbol{\Sigma}\)</span>.</p>
<div class="section level4">
<h4 id="sec:bigdata-loocv-glm">Big Data<a class="anchor" aria-label="anchor" href="#sec:bigdata-loocv-glm"></a>
</h4>
<p><code><a href="../reference/loocv.html">loocv()</a></code> for big data spatial generalized linear models
are defined similarly as for big data spatial linear models, except that
<span class="math inline">\(\mathbf{w}\)</span> is predicted instead of
<span class="math inline">\(\mathbf{y}\)</span>. Additionally, <span class="math inline">\(\mathbf{G}_{l, l}\)</span> is determined from each
local neighborhood as is done for <span class="math inline">\(\boldsymbol{\Sigma}_{l, l}\)</span>.</p>
</div>
</div>
<div class="section level3">
<h3 id="sec:predict-glm">
<code>predict()</code><a class="anchor" aria-label="anchor" href="#sec:predict-glm"></a>
</h3>
<div class="section level4">
<h4 id="sec:predict-none-glm">
<code>interval = "none"</code><a class="anchor" aria-label="anchor" href="#sec:predict-none-glm"></a>
</h4>
<p>Building from the previously defined empirical best linear unbiased
predictions (i.e., empirical Kriging predictions), predictions of <span class="math inline">\(\mathbf{w}_u\)</span> are given on the link scale
(<code>type = "link"</code>) by <span class="math display">\[\begin{equation}\label{eq:glm_blup}
  \mathbf{\dot{w}}_u = \mathbf{X}_u \hat{\boldsymbol{\beta}} +
\hat{\boldsymbol{\Sigma}}_{uo} \hat{\boldsymbol{\Sigma}}^{-1}_{o}
(\hat{\mathbf{w}}_o - \mathbf{X}_o \hat{\boldsymbol{\beta}}) .
\end{equation}\]</span> These predictions are given on the response
(inverse link) scale (<code>type = "response"</code>) as <span class="math inline">\(g^{-1}(\mathbf{\dot{w}}_u)\)</span>.</p>
<p>Similar to the covariance matrix of <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>, the covariance
matrix of <span class="math inline">\(\mathbf{\dot{w}}_u\)</span>
requires an adjustment to account for the fact that the <span class="math inline">\(\mathbf{w}\)</span> are not actually observed.
First let <span class="math inline">\(\boldsymbol{\Lambda} =
\mathbf{X}_u \mathbf{B} + \hat{\boldsymbol{\Sigma}}_{uo}
\hat{\boldsymbol{\Sigma}}_o^{-1} + \hat{\boldsymbol{\Sigma}}_{uo}
\hat{\boldsymbol{\Sigma}}_o^{-1} \mathbf{X}_o \mathbf{B}\)</span> and
<span class="math inline">\(\mathbf{B} = (\mathbf{X}^\top
\hat{\boldsymbol{\Sigma}}_o^{-1} \mathbf{X})^{-1} \mathbf{X}^\top
\hat{\boldsymbol{\Sigma}}_o^{-1}\)</span>, and note that <span class="math inline">\(\mathbf{\dot{w}}_u = \Lambda
\hat{\mathbf{w}}_o\)</span>. Using the law of conditional variance and
conditioning on <span class="math inline">\(\mathbf{w}_o\)</span> as if
we had observed them, it follows that <span class="math display">\[\begin{equation*}
  \text{Cov}(\hat{\mathbf{w}}_u - \mathbf{w}_u) = \text{Cov}(\Lambda
\hat{\mathbf{w}}_o - \mathbf{w}_u) =
\text{E}_{\mathbf{w}_o}[\text{Cov}(\Lambda \hat{\mathbf{w}}_o -
\mathbf{w}_u | \mathbf{w}_o)] +
\text{Cov}_{\mathbf{w}_o}[\text{E}(\Lambda \hat{\mathbf{w}}_o -
\mathbf{w}_u | \mathbf{w}_o)],
\end{equation*}\]</span> We assume <span class="math inline">\(\hat{\mathbf{w}}_o\)</span> is unbiased for <span class="math inline">\(\mathbf{w}_o\)</span> (i.e., <span class="math inline">\(\text{E}(\hat{\mathbf{w}}_o | \mathbf{w}_o) =
\mathbf{w}_o\)</span>). Then <span class="math inline">\(\text{Cov}_{\mathbf{w}_o}[\text{E}(\Lambda
\hat{\mathbf{w}}_o - \mathbf{w}_u | \mathbf{w}_o)] =
\text{Cov}_{\mathbf{w}_o}(\Lambda \mathbf{w}_o - \mathbf{w}_u) =
\hat{\boldsymbol{\Sigma}}_u - \hat{\boldsymbol{\Sigma}}_{uo}
\hat{\boldsymbol{\Sigma}}^{-1}_o \hat{\boldsymbol{\Sigma}}^\top_{uo} +
\mathbf{Q}(\mathbf{X}_o^\top \hat{\boldsymbol{\Sigma}}_o^{-1}
\mathbf{X}_o)^{-1}\mathbf{Q}^\top\)</span>, where <span class="math inline">\(\mathbf{Q} = \mathbf{X}_u -
\hat{\boldsymbol{\Sigma}}_{uo} \hat{\boldsymbol{\Sigma}}^{-1}_o
\mathbf{X}_o\)</span>, the usual form for the mean squared prediction
error of <span class="math inline">\(\hat{\mathbf{w}}_u\)</span>. Next
note that (viewing <span class="math inline">\(\mathbf{w}_u\)</span> as
a constant) <span class="math inline">\(\text{E}_{\mathbf{w}_o}[\text{Cov}(\Lambda
\hat{\mathbf{w}}_o - \mathbf{w}_u | \mathbf{w}_o)]\)</span> can be
viewed as the observed Fisher Information inverse, <span class="math inline">\(- \mathbf{G}^{-1}\)</span>. Evaluating <span class="math inline">\(\mathbf{G}\)</span> at <span class="math inline">\(\mathbf{a}\)</span> eventually yields the adjusted
covariance matrix of <span class="math inline">\(\Lambda
\hat{\mathbf{w}}_o - \mathbf{w}_u\)</span>
(<code>var_correct = TRUE</code>) given by <span class="math display">\[\begin{equation}\label{eq:glm_blup_cov}
  \text{Cov}(\Lambda \hat{\mathbf{w}}_o - \mathbf{w}_u) =
\dot{\boldsymbol{\Sigma}}_u = \Lambda (-\mathbf{G}_\mathbf{a})^{-1}
\Lambda^\top + \hat{\boldsymbol{\Sigma}}_u -
\hat{\boldsymbol{\Sigma}}_{uo} \hat{\boldsymbol{\Sigma}}^{-1}_o
\hat{\boldsymbol{\Sigma}}^\top_{uo} + \mathbf{Q}(\mathbf{X}_o^\top
\hat{\boldsymbol{\Sigma}}_o^{-1} \mathbf{X}_o)^{-1}\mathbf{Q}^\top .
\end{equation}\]</span></p>
<p>The unadjusted covariance matrix of <span class="math inline">\(\Lambda \hat{\mathbf{w}}_o - \mathbf{w}_u\)</span>
(<code>var_correct = FALSE</code>) can also be returned and is given by
<span class="math display">\[\begin{equation}\label{eq:glm_blup_cov_unadj}
  \text{Cov}(\Lambda \hat{\mathbf{w}}_o - \mathbf{w}_u) =
\dot{\boldsymbol{\Sigma}}_u = \hat{\boldsymbol{\Sigma}}_u -
\hat{\boldsymbol{\Sigma}}_{uo} \hat{\boldsymbol{\Sigma}}^{-1}_o
\hat{\boldsymbol{\Sigma}}^\top_{uo} + \mathbf{Q}(\mathbf{X}_o^\top
\hat{\boldsymbol{\Sigma}}_o^{-1} \mathbf{X}_o)^{-1}\mathbf{Q}^\top .
\end{equation}\]</span></p>
<p>When <code>se.fit = TRUE</code>, standard errors are returned on the
link scale by taking the square root of the diagonal of the relevant
<span class="math inline">\(\dot{\boldsymbol{\Sigma}}_u\)</span>.</p>
</div>
<div class="section level4">
<h4 id="sec:predict-predict-glm">
<code>interval = "prediction"</code><a class="anchor" aria-label="anchor" href="#sec:predict-predict-glm"></a>
</h4>
<p>Predictions of <span class="math inline">\(\mathbf{w}_u\)</span> are
returned on the link scale (<code>type = "link"</code>) by evaluating
<span class="math inline">\(\mathbf{\dot{w}}_u\)</span>. The (100 <span class="math inline">\(\times\)</span> <code>level</code>)% prediction
interval for <span class="math inline">\((w_u)_i\)</span> is <span class="math inline">\((\dot{w}_u)_i \pm z^*
\sqrt{(\dot{\boldsymbol{\Sigma}}_u)_{i, i}}\)</span>, where <span class="math inline">\(\sqrt{(\dot{\boldsymbol{\Sigma}}_u)_{i,
i}}\)</span> is the standard error of <span class="math inline">\((\dot{w}_u)_i\)</span> obtained from
<code>se.fit = TRUE</code>, <span class="math inline">\(\Phi(z^*) = 1 -
\alpha / 2\)</span>, <span class="math inline">\(\Phi(\cdot)\)</span> is
the standard normal (Gaussian) cumulative distribution function, <span class="math inline">\(\alpha = 1 -\)</span> <code>level</code>, and
<code>level</code> is an argument to <code><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict()</a></code>. The default
for <code>level</code> is 0.95, which corresponds to a <span class="math inline">\(z^*\)</span> of approximately 1.96. These
predictions and corresponding prediction intervals are returned on the
response scale (<code>type = "response"</code>) by applying <span class="math inline">\(g^{-1}(\cdot)\)</span> (inverse link) to <span class="math inline">\((\dot{w}_u)_i\)</span> (the prediction), <span class="math inline">\((\dot{w}_u)_i - z^*
\sqrt{(\dot{\boldsymbol{\Sigma}}_u)_{i, i}}\)</span> (the prediction
interval lower bound), and <span class="math inline">\((\dot{w}_u)_i +
z^* \sqrt{(\dot{\boldsymbol{\Sigma}}_u)_{i, i}}\)</span> (the prediction
interval upper bound). Note that the prediction intervals are symmetric
on the link scale but are not generally symmetric on the response scale.
One could obtain symmetric prediction intervals on the response scale
using the delta method <span class="citation">(Ver Hoef
2012)</span>.</p>
</div>
<div class="section level4">
<h4 id="sec:predict-conf-glm">
<code>interval = "confidence"</code><a class="anchor" aria-label="anchor" href="#sec:predict-conf-glm"></a>
</h4>
<p>Estimates for <span class="math inline">\((\mathbf{X}_u)_i
\boldsymbol{\beta}\)</span> (the fixed effects portion of the model) are
returned on the link scale (<code>type = "link"</code>) by evaluating
<span class="math inline">\((\mathbf{X}_u)_i
\hat{\boldsymbol{\beta}}\)</span> (i.e., fitted values corresponding to
<span class="math inline">\((\mathbf{X}_u)_i)\)</span>. The (100 <span class="math inline">\(\times\)</span> <code>level</code>)% confidence
interval for <span class="math inline">\((\mathbf{X}_u)_i
\boldsymbol{\beta}\)</span> is <span class="math inline">\((\mathbf{X}_u)_i \hat{\boldsymbol{\beta}} \pm z^*
\sqrt{(\mathbf{X}_u)_i [\mathbf{B} (-\mathbf{G}_a)^{-1} \mathbf{B}^\top
+ (\mathbf{X}^\top \hat{\boldsymbol{\Sigma}}^{-1}_o
\mathbf{X})^{-1}](\mathbf{X}_u)_i^\top}\)</span>, where <span class="math inline">\((\mathbf{X}_u)_i\)</span> is the <span class="math inline">\(i\)</span>th row of <span class="math inline">\(\mathbf{X}_u\)</span>, <span class="math inline">\(\mathbf{B} (-\mathbf{G}_a)^{-1} \mathbf{B}^\top +
(\mathbf{X}^\top \hat{\boldsymbol{\Sigma}}^{-1}
\mathbf{X})^{-1}\)</span> is the standard error of <span class="math inline">\((\mathbf{X}_u)_i \hat{\boldsymbol{\beta}}\)</span>
obtained from <code>se.fit = TRUE</code>, <span class="math inline">\(\Phi(z^*) = 1 - \alpha / 2\)</span>, <span class="math inline">\(\Phi(\cdot)\)</span> is the standard normal
(Gaussian) cumulative distribution function, <span class="math inline">\(\alpha = 1 -\)</span> <code>level</code>, and
<code>level</code> is an argument to <code><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict()</a></code>. The default
for <code>level</code> is 0.95, which corresponds to a <span class="math inline">\(z^*\)</span> of approximately 1.96. These
estimates and corresponding confidence intervals are returned on the
response scale (<code>type = "response"</code>) by applying <span class="math inline">\(g^{-1}(\cdot)\)</span> (inverse link) to <span class="math inline">\((\mathbf{X}_u)_i \hat{\boldsymbol{\beta}}\)</span>
(the estimate), <span class="math inline">\((\mathbf{X}_u)_i
\hat{\boldsymbol{\beta}} - z^* \sqrt{(\mathbf{X}_u)_i [\mathbf{B}
(-\mathbf{G}_a)^{-1} \mathbf{B}^\top + (\mathbf{X}^\top
\hat{\boldsymbol{\Sigma}}^{-1}_o
\mathbf{X})^{-1}](\mathbf{X}_u)_i^\top}\)</span> (the confidence
interval lower bound), and <span class="math inline">\((\mathbf{X}_u)_i
\hat{\boldsymbol{\beta}} + z^* \sqrt{(\mathbf{X}_u)_i [\mathbf{B}
(-\mathbf{G}_a)^{-1} \mathbf{B}^\top + (\mathbf{X}^\top
\hat{\boldsymbol{\Sigma}}^{-1}_o
\mathbf{X})^{-1}](\mathbf{X}_u)_i^\top}\)</span> (the confidence
interval upper bound). Note that the confidence intervals are symmetric
on the link scale but are generally not symmetric on the response scale.
One could obtain symmetric confidence intervals on the response scale
using the delta method <span class="citation">(Ver Hoef
2012)</span>.</p>
</div>
<div class="section level4">
<h4 id="sec:predict-spautor-glm">
<code>spgautor()</code> extra steps<a class="anchor" aria-label="anchor" href="#sec:predict-spautor-glm"></a>
</h4>
<p>The extra step required to obtain <span class="math inline">\(\hat{\boldsymbol{\Sigma}}^{-1}_o\)</span>, <span class="math inline">\(\hat{\boldsymbol{\Sigma}}_u\)</span>, and <span class="math inline">\(\hat{\boldsymbol{\Sigma}}_{uo}\)</span> is the
same for spatial generalized autoregressive models as it is for spatial
autoregressive models.</p>
</div>
<div class="section level4">
<h4 id="sec:bigdata-predict-glm">Big Data<a class="anchor" aria-label="anchor" href="#sec:bigdata-predict-glm"></a>
</h4>
<p><code><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict()</a></code> for big data spatial generalized linear models
is defined similarly as for big data spatial linear models, except that
<span class="math inline">\(\mathbf{w}\)</span> are subset or predicted
(instead of <span class="math inline">\(\mathbf{y}\)</span>) to find
<span class="math inline">\(\check{\mathbf{w}}_o\)</span> (instead of
<span class="math inline">\(\check{\mathbf{y}}_o\)</span>). It standard
errors are required, <span class="math inline">\(\check{\mathbf{G}}_o\)</span> is also found.</p>
</div>
</div>
<div class="section level3">
<h3 id="sec:pr2-glm">
<code>pseudoR2()</code><a class="anchor" aria-label="anchor" href="#sec:pr2-glm"></a>
</h3>
<p><code><a href="../reference/pseudoR2.html">pseudoR2()</a></code> for spatial generalized linear models is
defined the same as for spatial linear models.</p>
</div>
<div class="section level3">
<h3 id="sec:residuals-glm">
<code>residuals()</code><a class="anchor" aria-label="anchor" href="#sec:residuals-glm"></a>
</h3>
<p>The residuals are obtained by applying standard generalized linear
model definitions after conditioning on <span class="math inline">\(\mathbf{w}\)</span>.</p>
<p>When <code>type = "response"</code>, response residuals are returned:
<span class="math display">\[\begin{equation*}
\mathbf{e}_{r} = \mathbf{y} - g^{-1}(\mathbf{w}).
\end{equation*}\]</span></p>
<p>When <code>type = "pearson"</code>, Pearson residuals are returned:
<span class="math display">\[\begin{equation*}
\mathbf{e}_{p} = \mathbf{V}^{-1/2}\mathbf{e}_{r},
\end{equation*}\]</span> where <span class="math inline">\(\mathbf{V}\)</span> is a diagonal matrix with
<span class="math inline">\(i\)</span>th diagonal element equal to <span class="math inline">\(\text{Var}(w_i)\)</span>.</p>
<p>When <code>type = "deviance"</code>, deviance residuals are returned:
<span class="math display">\[\begin{equation*}
\mathbf{e}_{d} = \text{sign}(\mathbf{e}_r) \odot
\sqrt{\text{deviance}_i},
\end{equation*}\]</span> where <span class="math inline">\(\text{deviance}_i\)</span> is the deviance of
<span class="math inline">\(y_i\)</span> (conditional on <span class="math inline">\(w_i\)</span>) and <span class="math inline">\(\odot\)</span> denotes the Hadmard (element-wise)
product.</p>
<p>When <code>type = "standardized"</code>, standardized residuals are
returned: <span class="math display">\[\begin{equation*}
\mathbf{e}_{s} = \mathbf{e}_{d} \odot \frac{1}{\sqrt{1 -
diag(\mathbf{H}_c)}},
\end{equation*}\]</span> where <span class="math inline">\(diag(\mathbf{H}_c)\)</span> is the diagonal of the
leverage (hat) matrix conditional on <span class="math inline">\(\mathbf{w}\)</span> and given by <span class="math inline">\(\mathbf{H}_c \equiv \mathbf{X}_v
(\mathbf{X}_v^\top\mathbf{X}_v)^{-1} \mathbf{X}_v^\top\)</span>, where
<span class="math inline">\(\mathbf{X}_v =
\mathbf{V}^{1/2}\mathbf{X}\)</span>, and <span class="math inline">\(\odot\)</span> denotes the Hadmard (element-wise)
product.</p>
</div>
<div class="section level3">
<h3 id="sec:spgmod">
<code>spgautor()</code> and <code>spglm()</code><a class="anchor" aria-label="anchor" href="#sec:spgmod"></a>
</h3>
<p>Many of the details regarding <code><a href="../reference/spglm.html">spglm()</a></code> and
<code><a href="../reference/spgautor.html">spgautor()</a></code> for spatial generalized linear models are the
same as <code><a href="../reference/splm.html">splm()</a></code> and <code><a href="../reference/spautor.html">spautor()</a></code> for spatial
linear models, though occasional differences are noted in the following
subsection headers.</p>
<div class="section level4">
<h4 id="sec:spgautor-fn">
<code>spgautor()</code> Spatial Covariance Functions<a class="anchor" aria-label="anchor" href="#sec:spgautor-fn"></a>
</h4>
<p>Covariance functions for <code><a href="../reference/spgautor.html">spgautor()</a></code> are defined the same
as covariance functions for <code><a href="../reference/spautor.html">spautor()</a></code>.</p>
</div>
<div class="section level4">
<h4 id="sec:spglm-glm">
<code>spglm()</code> Spatial Covariance Functions<a class="anchor" aria-label="anchor" href="#sec:spglm-glm"></a>
</h4>
<p>Covariance functions for <code><a href="../reference/spglm.html">spglm()</a></code> are defined the same as
covariance functions for <code><a href="../reference/splm.html">splm()</a></code> except that for
<code>"none"</code>, the <code>ie</code> parameter is also set to zero
(analogous to <code><a href="https://rdrr.io/r/stats/glm.html" class="external-link">glm()</a></code> models).</p>
</div>
<div class="section level4">
<h4 id="sec:estimation-glm">Model-fitting<a class="anchor" aria-label="anchor" href="#sec:estimation-glm"></a>
</h4>
<p>Recall that the likelihood of interest is <span class="math display">\[\begin{equation*}
  \int_\mathbf{w} \exp(\ell_\mathbf{w}) d\mathbf{w} \approx [\mathbf{y}
| g^{-1}(\mathbf{a}), \varphi] [\mathbf{a} | \mathbf{X},
\boldsymbol{\theta}] (2 \pi)^{n/2} |-\mathbf{G}_a|^{-1/2}|,
\end{equation*}\]</span> and that <span class="math inline">\(\mathbf{a}\)</span> is the value at which the
gradient, <span class="math inline">\(\mathbf{g}\)</span>, equals zero.
Given <span class="math inline">\(\mathbf{a}\)</span>, minus twice a
profiled (by <span class="math inline">\(\boldsymbol{\beta}\)</span>)
marginal Laplace log-likelihood is given by <span class="math display">\[\begin{equation*}
  -2\ell_p(\varphi, \boldsymbol{\theta}) = -2\ln([\mathbf{y} |
g^{-1}(\mathbf{a}), \varphi]) -2\ln([\mathbf{a} | \mathbf{X},
\boldsymbol{\theta}]) -n \ln(2 \pi) + \ln(|- \mathbf{G}_{\mathbf{a}}|).
\end{equation*}\]</span> Note that <span class="math inline">\(\ln[\mathbf{a} | \mathbf{X},
\boldsymbol{\theta}]\)</span> are the ML or REML log-likelihood
equations the spatial linear model, respectively, where now <span class="math inline">\(\tilde{\boldsymbol{\beta}} = (\mathbf{X}^\top
\mathbf{\Sigma}^{-1} \mathbf{X})^{-1} \mathbf{X}^\top
\mathbf{\Sigma}^{-1} \mathbf{a}\)</span>.</p>
<p>Assuming <span class="math inline">\(\mathbf{y}\)</span> and <span class="math inline">\(g^{-1}(\mathbf{w})\)</span> are conditionally
independent and <span class="math inline">\(\varphi\)</span> and <span class="math inline">\(\boldsymbol{\theta}\)</span> are known, it can be
shown that maximizing <span class="math inline">\(\ell_{\mathbf{w}}\)</span> (with respect to <span class="math inline">\(\mathbf{w}\)</span>) amounts to maximizing (up to
a constant) <span class="math display">\[\begin{equation*}
  \sum_{i = 1}^N \ln[y_i | w_i, \varphi] - \frac{1}{2}(\mathbf{w} -
\mathbf{X} \hat{\boldsymbol{\beta}})^\top
\boldsymbol{\Sigma}^{-1}(\mathbf{w} -
\mathbf{X}\hat{\boldsymbol{\beta}}).
\end{equation*}\]</span> Thus the gradient <span class="math inline">\(\ell_{\mathbf{w}}\)</span> can be shown to equal
<span class="math display">\[\begin{equation*}
\mathbf{g} = \mathbf{d} - \boldsymbol{\Sigma}^{-1}\mathbf{w} +
\boldsymbol{\Sigma}^{-1}\mathbf{X} \hat{\boldsymbol{\beta}} = \mathbf{d}
- \mathbf{P}\mathbf{w},
\end{equation*}\]</span> where <span class="math inline">\(d_i =
\partial \ln[y_i | g^{-1}(w_i), \varphi] / \partial w_i\)</span> and
<span class="math inline">\(\mathbf{P} = \boldsymbol{\Sigma}^{-1} -
\boldsymbol{\Sigma}^{-1} \mathbf{X} (\mathbf{X}^\top
\boldsymbol{\Sigma}^{-1} \mathbf{X})^{-1} \mathbf{X}^\top
\boldsymbol{\Sigma}^{-1}\)</span>. And the Hessian of <span class="math inline">\(\ell_{\mathbf{w}}\)</span> is <span class="math display">\[\begin{equation*}
  \mathbf{G} = \mathbf{D} - \mathbf{P},
\end{equation*}\]</span> where <span class="math inline">\(D_{i, i} =
\partial^2 \ln[y_i | g^{-1}(w_i), \varphi] / \partial w_i^2\)</span>.
Note that <span class="math inline">\(D_{i, j} = 0\)</span> for <span class="math inline">\(i \neq j\)</span> because of conditional
independence.</p>
<p>Next the Newton-Rhapson algorithm can be used to maximize <span class="math inline">\(\ell_{\mathbf{w}}\)</span>, where an update is
given by <span class="math display">\[\begin{equation*}
\mathbf{w}^{k + 1} = \mathbf{w}^k - \alpha \mathbf{G}^{-1}\mathbf{g}.
\end{equation*}\]</span> where <span class="math inline">\(0 &lt; \alpha
\leq 1\)</span>. Typically, <span class="math inline">\(\alpha =
1\)</span>, but it can be lowered if the <span class="math inline">\(\mathbf{g}\)</span> is unstable. Generally, the
Newton-Rhapson converges rapidly. The value of <span class="math inline">\(\mathbf{w}\)</span> at convergence is defined as
<span class="math inline">\(\mathbf{a}\)</span>.</p>
<p>It follows that finding <span class="math inline">\(\varphi\)</span>
and <span class="math inline">\(\boldsymbol{\theta}\)</span> for unknown
<span class="math inline">\(\mathbf{a}\)</span> requires a
doubly-iterative algorithm. First, a value of <span class="math inline">\(\mathbf{a}\)</span> is proposed (e.g., <span class="math inline">\(\mathbf{a} = \mathbf{0}\)</span>). Then given
<span class="math inline">\(\mathbf{a}_0\)</span>, the Laplace
log-likelihood is maximized, yielding <span class="math inline">\(\hat{\varphi}_0\)</span> and <span class="math inline">\(\hat{\boldsymbol{\theta}}_0\)</span>. Then given
<span class="math inline">\(\hat{\varphi}_0\)</span> and <span class="math inline">\(\hat{\boldsymbol{\theta}}_0\)</span>, <span class="math inline">\(\ell_{\mathbf{w}}\)</span> is maximized, yielding
<span class="math inline">\(\mathbf{a}_1\)</span>. Then given <span class="math inline">\(\mathbf{a}_1\)</span>, the Laplace log-likelihood
is maximized, yielding <span class="math inline">\(\hat{\varphi}_1\)</span> and <span class="math inline">\(\hat{\boldsymbol{\theta}}_1\)</span>. Then given
<span class="math inline">\(\hat{\varphi}_1\)</span> and <span class="math inline">\(\hat{\boldsymbol{\theta}}_1\)</span>, <span class="math inline">\(\ell_{\mathbf{w}}\)</span> is maximized, yielding
<span class="math inline">\(\mathbf{a}_2\)</span>. This process
continues until convergence, yielding optimized values for <span class="math inline">\(\varphi\)</span> and <span class="math inline">\(\boldsymbol{\theta}\)</span> and, using these
optimized values, a value of <span class="math inline">\(\mathbf{a}\)</span>.</p>
<p>Note that the Laplace approximation incorporates a likelihood, and as
a result, the only estimation methods available via the
<code>estmethod</code> argument are <code>"reml"</code> (the default)
and <code>"ml"</code>. The doubly-iterative algorithm used to fit
spatial generalized linear models is far more computationally expensive
than fitting spatial linear models.</p>
</div>
<div class="section level4">
<h4 id="sec:optim-glm">Optimization<a class="anchor" aria-label="anchor" href="#sec:optim-glm"></a>
</h4>
<p>Optimization for <code><a href="../reference/spglm.html">spglm()</a></code> and <code><a href="../reference/spgautor.html">spgautor()</a></code>
works as it does for <code><a href="../reference/splm.html">splm()</a></code> and <code><a href="../reference/spautor.html">spautor()</a></code>,
with one additional step. The convergence criteria for <span class="math inline">\(\mathbf{w}\)</span> (within each covariance
parameter iteration) is achieved when the largest absolute value of
<span class="math inline">\(\mathbf{w}_k - \mathbf{w}_{k - 1}\)</span>
is less than <span class="math inline">\(1/10^4\)</span> or <span class="math inline">\(k &gt; 50\)</span> (<span class="math inline">\(k\)</span> indexes the Newton-Rhapson
iterations).</p>
<div class="section level5">
<h5 id="sec:grid-glm">Grid Search<a class="anchor" aria-label="anchor" href="#sec:grid-glm"></a>
</h5>
<p>The grid search for <code><a href="../reference/spglm.html">spglm()</a></code> and <code><a href="../reference/spgautor.html">spgautor()</a></code>
works as it does for <code><a href="../reference/splm.html">splm()</a></code> and <code><a href="../reference/spautor.html">spautor()</a></code>
except that for <code><a href="../reference/spglm.html">spglm()</a></code> and <code><a href="../reference/spgautor.html">spgautor()</a></code>, the
grid search initial values are on the link scale and the grid search
sample variance is calculated by regressing <span class="math inline">\(\ln(\mathbf{y} + 1)\)</span> on <span class="math inline">\(\mathbf{X}\)</span> instead of <span class="math inline">\(\mathbf{y}\)</span> on <span class="math inline">\(\mathbf{X}\)</span>. For negative binomial, beta,
gamma, and inverse Gaussian families, the initial value of the
dispersion parameter is set to one.</p>
</div>
</div>
<div class="section level4">
<h4 id="sec:testing-glm">Hypothesis Testing<a class="anchor" aria-label="anchor" href="#sec:testing-glm"></a>
</h4>
<p>Hypothesis testing for spatial generalized linear models is defined
the same as for spatial linear models. That is, the hypothesis tests are
asymptotic z-tests based on the normal (Gaussian) distribution (Wald
tests). The null hypothesis for the test associated with each <span class="math inline">\(\hat{\beta}_i\)</span> is that <span class="math inline">\(\beta_i = 0\)</span>. The spatial generalized
linear models, <span class="math inline">\(\text{Cov}(\hat{\boldsymbol{\beta}})\)</span>
requires an adjustment to account for the fact that the <span class="math inline">\(\mathbf{w}\)</span> are not actually observed.
First let <span class="math inline">\(\mathbf{B} = (\mathbf{X}^\top
\boldsymbol{\Sigma}^{-1} \mathbf{X})^{-1} \mathbf{X}^\top
\boldsymbol{\Sigma}^{-1}\)</span> and note that <span class="math inline">\(\hat{\boldsymbol{\beta}} =
\mathbf{B}\mathbf{w}\)</span>. Using the law of conditional variance and
conditioning on <span class="math inline">\(\mathbf{w}\)</span> as if we
had observed them, it follows that <span class="math display">\[\begin{equation*}
  \text{Cov}(\mathbf{B} \hat{\mathbf{w}}) =
\text{E}_\mathbf{w}[\text{Cov}(\mathbf{B}\hat{\mathbf{w}} | \mathbf{w})]
+ \text{Cov}_\mathbf{w}[\text{E}(\mathbf{B}\hat{\mathbf{w}} |
\mathbf{w})]
\end{equation*}\]</span> We assume <span class="math inline">\(\hat{\mathbf{w}}\)</span> is unbiased for <span class="math inline">\(\mathbf{w}\)</span> (i.e., <span class="math inline">\(\text{E}(\hat{\mathbf{w}} | \mathbf{w}) =
\mathbf{w}\)</span>). Then <span class="math inline">\(\text{Cov}(\text{E}(\mathbf{B} \hat{\mathbf{w}} |
\mathbf{w})) = \text{Cov}(\mathbf{B}\mathbf{w}) = \mathbf{B}
\boldsymbol{\Sigma} \mathbf{B}^\top\)</span>, which reduces to <span class="math inline">\((\mathbf{X}^\top \boldsymbol{\Sigma}^{-1}
\mathbf{X})^{-1}\)</span>, the usual form for <span class="math inline">\(\text{Cov}(\hat{\boldsymbol{\beta}})\)</span>.
Next note that <span class="math inline">\(\text{Cov}(\hat{\mathbf{w}} |
\mathbf{w})\)</span> can be viewed as the inverse of the observed Fisher
Information, which is <span class="math inline">\(-\mathbf{G}^{-1}\)</span>, which depends on <span class="math inline">\(\mathbf{w}\)</span> through the diagonal elements
in <span class="math inline">\(\mathbf{D}\)</span>. Evaluating <span class="math inline">\(-\mathbf{G}\)</span> at <span class="math inline">\(\mathbf{a}\)</span> yields the adjusted covariance
matrix of <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>
given by <span class="math display">\[\begin{equation*}
  \text{Cov}(\hat{\boldsymbol{\beta}}) = \mathbf{B} (-\mathbf{G}_a)^{-1}
\mathbf{B}^\top + (\mathbf{X}^\top \boldsymbol{\Sigma}^{-1}
\mathbf{X})^{-1} .
\end{equation*}\]</span></p>
</div>
<div class="section level4">
<h4 id="sec:random-glm">Random Effects (<code>spglm()</code> only)<a class="anchor" aria-label="anchor" href="#sec:random-glm"></a>
</h4>
<p>Random effects for spatial generalized linear models are defined the
same as for spatial linear models. Note that random effects for spatial
generalized linear models are on the link scale.</p>
</div>
<div class="section level4">
<h4 id="sec:anisotropy-glm">Anisotropy (<code>spglm()</code> only)<a class="anchor" aria-label="anchor" href="#sec:anisotropy-glm"></a>
</h4>
<p>Anisotropy for spatial generalized linear models are defined the same
as for spatial linear models. Note that anisotropy parameters for
spatial generalized linear models are on the link scale.</p>
</div>
<div class="section level4">
<h4 id="sec:partition-glm">Partition Factors<a class="anchor" aria-label="anchor" href="#sec:partition-glm"></a>
</h4>
<p>Partition factors for spatial generalized linear models are defined
the same as for spatial linear models.</p>
</div>
<div class="section level4">
<h4 id="sec:bigdata-glm">Big Data (<code>spglm()</code> only)<a class="anchor" aria-label="anchor" href="#sec:bigdata-glm"></a>
</h4>
<p>Big data model-fitting for spatial generalized linear models is in
many ways the same as for spatial linear models. The <code>local</code>
argument behaves the same for spatial generalized linear models as it
does for spatial linear models. This is because fundamentally, the
“local” spatial indexing (SPIN) approach to representing <span class="math inline">\(\boldsymbol{\Sigma}\)</span> blockwise is still
applied and serves as the basis for massive computational gains when
fitting spatial generalized linear models <span class="citation">(Ver
Hoef et al. 2023)</span>.</p>
<p>The additional step that is required to fit big data spatial
generalized linear models involves efficiently manipulating the Hessian,
<span class="math inline">\(\mathbf{G}\)</span>, to obtain its inverse
and log determinant. Before providing further details, we review the
Sherman-Morrison-Woodbury (SMW) formula <span class="citation">(Sherman
1949; Sherman and Morrison 1950; Woodbury 1950)</span>. The SMW formula
states that for an <span class="math inline">\(n \times n\)</span>
matrix <span class="math inline">\(\mathbf{A}\)</span>, an <span class="math inline">\(n \times k\)</span> matrix <span class="math inline">\(\mathbf{U}\)</span>, a <span class="math inline">\(k \times k\)</span> matrix <span class="math inline">\(\mathbf{C}\)</span>, and a <span class="math inline">\(k \times n\)</span> matrix <span class="math inline">\(\mathbf{V}\)</span>, <span class="math display">\[\begin{equation*}
  (\mathbf{A} + \mathbf{U} \mathbf{C} \mathbf{V})^{-1} = \mathbf{A}^{-1}
- \mathbf{A}^{-1} \mathbf{U} (\mathbf{C}^{-1} +
\mathbf{V}\mathbf{A}^{-1}\mathbf{U})^{-1} \mathbf{V} \mathbf{A}^{-1}
\end{equation*}\]</span> and <span class="math display">\[\begin{equation*}
  |\mathbf{A} + \mathbf{U} \mathbf{C} \mathbf{V}| =
|\mathbf{A}||\mathbf{C}||\mathbf{C}^{-1} +
\mathbf{V}\mathbf{A}^{-1}\mathbf{U}|.
\end{equation*}\]</span> The determinant result above implies <span class="math display">\[\begin{equation*}
  \ln|\mathbf{A} + \mathbf{U} \mathbf{C} \mathbf{V}| = \ln|\mathbf{A}| +
\ln|\mathbf{C}| + \ln|\mathbf{C}^{-1} +
\mathbf{V}\mathbf{A}^{-1}\mathbf{U}|.
\end{equation*}\]</span> The SMW formula is important because if the
inverse and log determinant of <span class="math inline">\(\mathbf{A}\)</span> is efficient to compute and
<span class="math inline">\(k &lt;&lt; n\)</span>, then the inverse and
log determinant of the desired sum can also be efficient to compute.
This is because except for <span class="math inline">\(\mathbf{A}^{-1}\)</span> and <span class="math inline">\(\mathbf{|A|}\)</span>, the SMW formula only
requires finding <span class="math inline">\(k \times k\)</span>
inverses and log determinants.</p>
<p>Recall that the Hessian can be written as <span class="math display">\[\begin{equation*}
  \mathbf{G} = \mathbf{D} - \boldsymbol{\Sigma}^{-1} +
\boldsymbol{\Sigma}^{-1} \mathbf{X} (\mathbf{X}^\top
\boldsymbol{\Sigma}^{-1} \mathbf{X})^{-1} \mathbf{X}^\top
\boldsymbol{\Sigma}^{-1},
\end{equation*}\]</span> Because <span class="math inline">\(\boldsymbol{\Sigma}\)</span> can be represented
blockwise, <span class="math inline">\(\boldsymbol{\Sigma}^{-1}\)</span>
can be represented blockwise and thus the inverse and log determinant
can be efficiently computed. Because <span class="math inline">\(\mathbf{D}\)</span> is diagonal, <span class="math inline">\(\mathbf{D} - \boldsymbol{\Sigma}^{-1}\)</span> can
be represented blockwise and thus the inverse and log determinant can be
efficiently computed. Then the SMW formula can be used, taking <span class="math inline">\(\mathbf{A} = \mathbf{D} -
\boldsymbol{\Sigma}^{-1}\)</span>, <span class="math inline">\(\mathbf{U} = \boldsymbol{\Sigma}^{-1}
\mathbf{X}\)</span>, <span class="math inline">\(\mathbf{C} =
(\mathbf{X}^\top \boldsymbol{\Sigma}^{-1} \mathbf{X})^{-1}\)</span>, and
<span class="math inline">\(\mathbf{V} = \mathbf{X}^\top
\boldsymbol{\Sigma}^{-1}\)</span>.</p>
<p>As previously mentioned, fitting big data spatial generalized linear
models requires a doubly-iterative algorithm. This makes it far more
computationally expensive than fitting big data spatial linear
models.</p>
</div>
</div>
<div class="section level3">
<h3 id="sec:sim-fns">Simulation Functions (<code>sprpois()</code>,
<code>sprnbinom()</code>, <code>sprbinom()</code>,
<code>sprbeta()</code>, <code>sprgamma()</code>,
<code>sprinvgauss()</code>)<a class="anchor" aria-label="anchor" href="#sec:sim-fns"></a>
</h3>
<p>Poisson, negative binomial, binomial, beta, gamma, and inverse
Gaussian random variables can be simulated using <code><a href="../reference/sprpois.html">sprpois()</a></code>,
<code><a href="../reference/sprnbinom.html">sprnbinom()</a></code>, <code><a href="../reference/sprbinom.html">sprbinom()</a></code>,
<code><a href="../reference/sprbeta.html">sprbeta()</a></code>, <code><a href="../reference/sprgamma.html">sprgamma()</a></code>, and
<code><a href="../reference/sprinvgauss.html">sprinvgauss()</a></code>, respectively. All of these functions work
similarly. First, relevant arguments are passed to
<code><a href="../reference/sprnorm.html">sprnorm()</a></code> to simulate <span class="math inline">\(\mathbf{w}\)</span> on the link scale. Then using
<span class="math inline">\(\mathbf{w}\)</span> and the dispersion
parameter (when required), relevant generalized linear model random
variables are simulated independently for each <span class="math inline">\(w_i\)</span>. Note that the dispersion parameter
is not required for <code><a href="../reference/sprpois.html">sprpois()</a></code> and
<code><a href="../reference/sprbinom.html">sprbinom()</a></code>.</p>
</div>
<div class="section level3">
<h3 id="sec:vcov-glm">
<code>vcov()</code><a class="anchor" aria-label="anchor" href="#sec:vcov-glm"></a>
</h3>
<p>The corrected variance-covariance matrix of the fixed effects
(<code>var_correct = TRUE</code>) is given by <span class="math inline">\(\mathbf{B} (-\mathbf{G}_a)^{-1} \mathbf{B}^\top +
(\mathbf{X}^\top \boldsymbol{\Sigma}^{-1} \mathbf{X})^{-1}\)</span>. The
uncorrected variance-covariance matrix
(<code>var_correct = FALSE</code>) is given by <span class="math inline">\(\text{Cov}(\hat{\boldsymbol{\beta}}) =
(\mathbf{X}^\top \boldsymbol{\Sigma}^{-1} \mathbf{X})^{-1}\)</span>.</p>
</div>
<div class="section level3">
<h3 id="sec:distributions-glm">Distribution Parameterizations<a class="anchor" aria-label="anchor" href="#sec:distributions-glm"></a>
</h3>
<p>Below we provide definitions associated with each of the six
generalized linear models families available in <code>spmodel</code>.
Note that the poisson, binomial, gamma, and inverse Gaussian
distributions are members of the exponential family, and their deviance
is typically expressed as twice the difference in log-likelihoods
between the saturated and fitted model times the dispersion parameter
(consistent with <code><a href="https://rdrr.io/r/stats/glm.html" class="external-link">glm()</a></code>). The negative binomial and beta
distributions are members of the exponential family when the dispersion
parameter (<span class="math inline">\(\varphi\)</span>) is known, and
their deviance is typically expressed as just twice the difference in
log-likelihoods between the saturated and fitted model (consistent with
<code>glm.nb()</code> from <code>MASS</code> <span class="citation">(Venables and Ripley 2002)</span> for negative binomial
regression and <code>betareg()</code> from <code>betareg</code> <span class="citation">(Cribari-Neto and Zeileis 2010)</span> for beta
regression).</p>
<div class="section level4">
<h4 id="sec:poisson">Poisson Distribution<a class="anchor" aria-label="anchor" href="#sec:poisson"></a>
</h4>
<p>The Poisson distribution is defined as <span class="math display">\[\begin{equation*}
  f(y | \mu) = \frac{\mu^y \exp(-\mu)}{y!},
\end{equation*}\]</span> where <span class="math inline">\(y\)</span> is
a non-negative integer, <span class="math inline">\(\mu &gt; 0\)</span>,
<span class="math inline">\(\text{E}(y) = \mu\)</span>, and <span class="math inline">\(\text{Var}(y) = \mu\)</span>.</p>
<p>The log-likelihood (of a single observation) is defined as <span class="math display">\[\begin{equation*}
  \ln f(y | \mu) = y \ln(\mu) - \mu - \ln(y!).
\end{equation*}\]</span></p>
<p>Using the inverse log link and writing in terms of <span class="math inline">\(\mu = \exp(w)\)</span>, the log-likelihood can be
written as <span class="math display">\[\begin{equation*}
  \ln f(y | \mu) = yw - \exp(w) - \ln(y!).
\end{equation*}\]</span></p>
<p>The derivative of the Poisson distribution with respect to <span class="math inline">\(w\)</span> is <span class="math display">\[\begin{equation*}
  \frac{d}{d w} \ln f(y | \mu) = y - \exp(w).
\end{equation*}\]</span></p>
<p>The second derivative of the Poisson distribution with respect to
<span class="math inline">\(w\)</span> is <span class="math display">\[\begin{equation*}
  \frac{d^2}{d w^2} \ln f(y | \mu) = - \exp(w).
\end{equation*}\]</span></p>
<p>Twice the log-likelihood of the saturated model is <span class="math display">\[\begin{equation*}
  2 \ln f(\mathbf{y} | \boldsymbol{\mu}_s) = \sum_i y_i \ln(y_i) - y_i -
\ln(y_i!).
\end{equation*}\]</span></p>
<p>Twice the log-likelihood of the fitted (observed) model is <span class="math display">\[\begin{equation*}
  2 \ln f(\mathbf{y} | \hat{\boldsymbol{\mu}}) = \sum_i y_i
\ln(\hat{\mu}_i) - \hat{\mu}_i - \ln(y_i!).
\end{equation*}\]</span></p>
<p>Thus the deviance is <span class="math display">\[\begin{equation*}
  2 \sum_i y_i log(y_i / \hat{\mu}_i) - (y_i - \hat{\mu}_i).
\end{equation*}\]</span></p>
</div>
<div class="section level4">
<h4 id="sec:nbinomial">Negative Binomial Distribution<a class="anchor" aria-label="anchor" href="#sec:nbinomial"></a>
</h4>
<p>The negative binomial distribution is defined as <span class="math display">\[\begin{equation*}
  f(y | \mu, \varphi) = \frac{\Gamma(y + \varphi)}{\Gamma(\varphi) y!}
\left( \frac{\mu}{\mu + \varphi} \right)^y \left( \frac{\varphi}{\mu +
\varphi} \right)^\varphi,
\end{equation*}\]</span> where <span class="math inline">\(y\)</span> is
a non-negative integer, <span class="math inline">\(\mu &gt; 0\)</span>,
<span class="math inline">\(\varphi &gt; 0\)</span>, <span class="math inline">\(\text{E}(y) = \mu\)</span>, and <span class="math inline">\(\text{Var}(y) = \mu +
\frac{\mu^2}{\varphi}\)</span>.</p>
<p>The log-likelihood (of a single observation) is defined as <span class="math display">\[\begin{equation*}
  \ln f(y | \mu, \varphi) = \ln(\Gamma(y + \varphi)) -
\ln(\Gamma(\varphi)) - \ln(y!) + y \ln(\mu) - y \ln(\mu + \varphi) +
\varphi \ln(\varphi) - \varphi \ln(\mu + \varphi).
\end{equation*}\]</span></p>
<p>Using the inverse log link and writing in terms of <span class="math inline">\(\mu = \exp(w)\)</span>, the log-likelihood can be
written as <span class="math display">\[\begin{equation*}
  \ln f(y | \mu) = \ln(\Gamma(y + \varphi)) - \ln(\Gamma(\varphi)) -
\ln(y!) + y [w - \ln(\exp(w) + \varphi)] + \varphi [\ln(\varphi) -
\ln(\exp(w) + \varphi)].
\end{equation*}\]</span></p>
<p>The derivative of the negative binomial distribution with respect to
<span class="math inline">\(w\)</span> is <span class="math display">\[\begin{equation*}
\begin{split}
  \frac{d}{d w} \ln f(y | \mu) &amp; = y\left(1 - \frac{\exp(w)}{\exp(w)
+ \varphi}\right) + \varphi \left(- \frac{\exp(w)}{\exp(w) + \varphi}
\right) \\
  &amp; = \frac{y\varphi}{\exp(w) + \varphi} - \frac{\varphi
\exp(w)}{\exp(w) + \varphi} \\
  &amp; =  \frac{\varphi (y - \exp(w))}{\exp(w) + \varphi}.
\end{split}
\end{equation*}\]</span></p>
<p>The second derivative of the negative binomial distribution with
respect to <span class="math inline">\(w\)</span> is <span class="math display">\[\begin{equation*}
  \frac{d^2}{d w^2} \ln f(y | \mu) = - \frac{\varphi \exp(w) (y +
\varphi)}{(\varphi + \exp(w))^2}
\end{equation*}\]</span></p>
<p>Twice the log-likelihood of the saturated model is <span class="math display">\[\begin{equation*}
  2 \ln f(\mathbf{y} | \boldsymbol{\mu}_s) = \sum_i \ln(\Gamma(y_i +
\varphi)) - \ln(\Gamma(\varphi)) - \ln(y_i!) + y_i \ln(y_i) - y_i
\ln(y_i + \varphi) + \varphi \ln(\varphi) - \varphi \ln(y_i + \varphi).
\end{equation*}\]</span></p>
<p>Twice the log-likelihood of the fitted (observed) model is <span class="math display">\[\begin{equation*}
  2 \ln f(\mathbf{y} | \hat{\boldsymbol{\mu}}) = \sum_i \ln(\Gamma(y_i +
\varphi)) - \ln(\Gamma(\varphi)) - \ln(y_i!) + y_i \ln(\hat{\mu}_i) -
y_i \ln(\hat{\mu}_i + \varphi) + \varphi \ln(\varphi) - \varphi
\ln(\hat{\mu}_i + \varphi).
\end{equation*}\]</span></p>
<p>Thus the deviance is <span class="math display">\[\begin{equation*}
  2 \sum_i y_i [\ln(y_i) - \ln(y_i + \varphi) - \ln(\hat{\mu}_i) +
\ln(\hat{\mu}_i + \varphi)] + \varphi [ - \ln(y_i + \varphi) +
\ln(\hat{\mu}_i + \varphi)].
\end{equation*}\]</span></p>
</div>
<div class="section level4">
<h4 id="sec:binomial">Binomial Distribution<a class="anchor" aria-label="anchor" href="#sec:binomial"></a>
</h4>
<p>The binomial distribution is defined as <span class="math display">\[\begin{equation*}
  f(y | \mu, m) = \binom{m}{y} \mu^y (1 - \mu)^{m - y},
\end{equation*}\]</span> where <span class="math inline">\(m\)</span> is
the (known) number of Bernoulli trials, <span class="math inline">\(y\)</span> is a non-negative integer, <span class="math inline">\(0 \le \mu \le 1\)</span>, <span class="math inline">\(\text{E}(y) = m\mu\)</span>, and <span class="math inline">\(\text{Var}(y) = m\mu (1 - \mu)\)</span>.</p>
<p>The log-likelihood (of a single observation) is defined as <span class="math display">\[\begin{equation*}
  \ln f(y | \mu) = \ln \left[ \binom{m}{y} \right] + y \ln(\mu) + (m -
y) \ln(1 - \mu).
\end{equation*}\]</span></p>
<p>Using the inverse logit link and writing in terms of <span class="math inline">\(\mu = \exp(w) / (1 + \exp(w))\)</span>, the
log-likelihood can be written as <span class="math display">\[\begin{equation*}
  \ln f(y | \mu) = \ln \left[ \binom{m}{y} \right] + y \ln(\exp(w) / (1
+ \exp(w))) + (m - y) \ln(1 - \exp(w) / (1 + \exp(w))).
\end{equation*}\]</span></p>
<p>The derivative of the binomial distribution with respect to <span class="math inline">\(w\)</span> is <span class="math display">\[\begin{equation*}
  \frac{d}{d w} \ln f(y | \mu) = y - \frac{m \exp(w)}{1 + \exp(w)}
\end{equation*}\]</span></p>
<p>The second derivative of the binomial distribution with respect to
<span class="math inline">\(w\)</span> is <span class="math display">\[\begin{equation*}
  \frac{d^2}{d w^2} \ln f(y | \mu) = - \frac{m \exp(w)}{(1 +
\exp(w))^2}.
\end{equation*}\]</span></p>
<p>Twice the log-likelihood of the saturated model is <span class="math display">\[\begin{equation*}
  2 \ln f(\mathbf{y} | \boldsymbol{\mu}_s) = \sum_i \ln \left[
\binom{m_i}{y_i} \right] + y_i \ln(y_i) + (m_i - y_i) \ln(m_i - y_i).
\end{equation*}\]</span></p>
<p>Twice the log-likelihood of the fitted (observed) model is <span class="math display">\[\begin{equation*}
  2 \ln f(\mathbf{y} | \hat{\boldsymbol{\mu}}) = \sum_i \ln \left[
\binom{m_i}{y_i} \right] + y_i \ln(m_i\hat{\mu}_i) + (m_i - y_i) \ln(m_i
- m_i \hat{\mu}_i).
\end{equation*}\]</span></p>
<p>Thus the deviance is <span class="math display">\[\begin{equation*}
  2 \sum_i y_i[ \ln(y_i) - \ln(m_i \hat{\mu}_i)] + (m_i - y_i)[ \ln(m_i
- y_i) - \ln(m_i - m_i \hat{\mu}_i) ]
\end{equation*}\]</span></p>
</div>
<div class="section level4">
<h4 id="sec:beta">Beta Distribution<a class="anchor" aria-label="anchor" href="#sec:beta"></a>
</h4>
<p>The beta distribution is defined as <span class="math display">\[\begin{equation*}
  f(y | \mu, \varphi) = \frac{\Gamma(\varphi)}{\Gamma(\mu \varphi)
\Gamma((1 - \mu)\varphi)} y^{\mu \varphi - 1} (1 - y)^{(1 - \mu)\varphi
- 1},
\end{equation*}\]</span> where <span class="math inline">\(0 &lt; y &lt;
1\)</span>, <span class="math inline">\(0 &lt; \mu &lt; 1\)</span>,
<span class="math inline">\(\text{E}(y) = \mu\)</span>, and <span class="math inline">\(\text{Var}(y) = \mu (1 - \mu) / (1 +
\varphi)\)</span>.</p>
<p>The log-likelihood (of a single observation) is defined as <span class="math display">\[\begin{equation*}
  \ln f(y | \mu) = \ln(\Gamma(\varphi)) - \ln(\Gamma(\mu \varphi)) -
\ln(\Gamma((1 - \mu)\varphi)) +  (\mu \varphi - 1) \ln(y) + ((1 -
\mu)\varphi - 1) \ln(1 - y).
\end{equation*}\]</span></p>
<p>Using the inverse logit link and writing in terms of <span class="math inline">\(\mu = \exp(w)\)</span>, the log-likelihood can be
written as <span class="math display">\[\begin{equation*}
\begin{split}
  \ln f(y | \mu, \varphi) &amp; = \ln(\Gamma(\varphi)) -
\ln(\Gamma(\frac{\exp(w)}{1 + \exp(w)} \varphi)) - \ln(\Gamma((1 -
\frac{\exp(w)}{1 + \exp(w)})\varphi)) \\
  &amp; +  (\frac{\exp(w)}{1 + \exp(w)} \varphi - 1) \ln(y) + ((1 -
\frac{\exp(w)}{1 + \exp(w)})\varphi - 1) \ln(1 - y).
\end{split}
\end{equation*}\]</span></p>
<p>It can be shown that the derivative of beta distribution with respect
to <span class="math inline">\(w\)</span> is <span class="math display">\[\begin{equation*}
  \frac{d}{d w} \ln f(y | \mu) = - \frac{\varphi \exp(w) k_0(w | y,
\varphi)}{(1 + \exp(w))^2},
\end{equation*}\]</span> where <span class="math inline">\(k_0(w | y,
\varphi) = \psi^{(0)}(\frac{\varphi \exp(w)}{1 + \exp(w)}) -
\psi^{(0)}(\frac{\varphi}{1 + \exp(w)}) + \ln( \frac{1}{y} - 1)\)</span>
and <span class="math inline">\(\psi^{(0)}\)</span> is the <span class="math inline">\(0th\)</span> derivative of the digamma
function.</p>
<p>It can be shown that the second derivative of the beta distribution
with respect to <span class="math inline">\(w\)</span> is <span class="math display">\[\begin{equation*}
  \frac{d^2}{d w^2} \ln f(y | \mu) = - \frac{\varphi \exp(2w) k_1(w | y,
\varphi)}{(1 + \exp(w))^4},
\end{equation*}\]</span> where <span class="math inline">\(k_1(w | y,
\varphi) = \varphi [\psi^{(1)}(\frac{\varphi \exp(w)}{1 + \exp(w)}) +
\psi^{(1)}( \frac{\varphi}{1 + \exp(w)})] - 2\sinh(w)[k_0(w | y,
\varphi) + 2\tanh^{-1}(1 - 2y)]\)</span> and <span class="math inline">\(\psi^{(n)}\)</span> is the <span class="math inline">\(nth\)</span> derivative of the digamma
function.</p>
<p>Twice the log-likelihood of the saturated model is <span class="math display">\[\begin{equation*}
  2 \ln f(\mathbf{y} | \boldsymbol{\mu}_s) = \sum_i \ln(\Gamma(\varphi))
- \ln(\Gamma(y_i \varphi)) - \ln(\Gamma((1 - y_i)\varphi)) +  (y_i
\varphi - 1) \ln(y_i) + ((1 - y_i)\varphi - 1) \ln(1 - y_i).
\end{equation*}\]</span></p>
<p>Twice the log-likelihood of the fitted (observed) model is <span class="math display">\[\begin{equation*}
  2 \ln f(\mathbf{y} | \hat{\boldsymbol{\mu}}) = \ln(\Gamma(\varphi)) -
\ln(\Gamma(\hat{\mu}_i \varphi)) - \ln(\Gamma((1 - \hat{\mu}_i)\varphi))
+  (\hat{\mu}_i \varphi - 1) \ln(y_i) + ((1 - \hat{\mu}_i)\varphi - 1)
\ln(1 - y_i).
\end{equation*}\]</span></p>
<p>Thus the deviance is <span class="math display">\[\begin{equation*}
  2 \sum_i - \ln(\Gamma(y_i \varphi)) - \ln(\Gamma((1 - y_i) \varphi ))
+ \ln(\Gamma(\hat{\mu}_i \varphi)) + \ln(\Gamma((1 - \hat{\mu}_i)
\varphi )) + (y_i - \hat{\mu}_i) \varphi \ln(y_i) + (\hat{\mu}_i - y_i)
\varphi \ln(1 - y_i)
\end{equation*}\]</span></p>
<p>Sometimes the deviance contribution from the <span class="math inline">\(i\)</span>th observation can be computationally
unstable and yield a negative value <span class="citation">(Espinheira,
Ferrari, and Cribari-Neto 2008)</span>. This can happen, for example,
when <span class="math inline">\(y_i\)</span> is close to zero or one.
When this happens, the deviance contribution is truncated to zero to
reflect the fact that the theoretical deviance contribution must be
non-negative.</p>
</div>
<div class="section level4">
<h4 id="sec:gamma">Gamma Distribution<a class="anchor" aria-label="anchor" href="#sec:gamma"></a>
</h4>
<p>The gamma distribution is defined as <span class="math display">\[\begin{equation*}
  f(y | \mu, \varphi) = \frac{1}{\Gamma(\varphi)} \left(
\frac{\varphi}{\mu} \right)^\varphi y^{\varphi - 1} \exp(\frac{-y
\varphi}{\mu}),
\end{equation*}\]</span> where <span class="math inline">\(y &gt;
0\)</span>, <span class="math inline">\(\mu &gt; 0\)</span>, <span class="math inline">\(\text{E}(y) = \mu\)</span>, and <span class="math inline">\(\text{Var}(y) = \mu^2/\varphi\)</span>.</p>
<p>The log-likelihood (of a single observation) is defined as <span class="math display">\[\begin{equation*}
  \ln f(y | \mu) = - \ln(\Gamma(\varphi)) + \varphi [\ln(\varphi) -
\ln(\mu)] + (\varphi - 1) \ln(y) - \frac{y \varphi}{\mu}.
\end{equation*}\]</span></p>
<p>Using the inverse log link and writing in terms of <span class="math inline">\(\mu = \exp(w)\)</span>, the log-likelihood can be
written as <span class="math display">\[\begin{equation*}
  \ln f(y | \mu) = - \ln(\Gamma(\varphi)) \varphi [\ln(\varphi) - w] +
(\varphi - 1) \ln(y) - \frac{y \varphi}{\exp(w)}.
\end{equation*}\]</span></p>
<p>The derivative of the gamma distribution with respect to <span class="math inline">\(w\)</span> is <span class="math display">\[\begin{equation*}
  \frac{d}{d w} \ln f(y | \mu) = -\varphi + \frac{\varphi y}{\exp(w)}.
\end{equation*}\]</span></p>
<p>The second derivative of the gamma distribution with respect to <span class="math inline">\(w\)</span> is <span class="math display">\[\begin{equation*}
  \frac{d^2}{d w^2} \ln f(y | \mu) = - \frac{\varphi y}{\exp(w)}.
\end{equation*}\]</span></p>
<p>Twice the log-likelihood of the saturated model is <span class="math display">\[\begin{equation*}
  2 \ln f(\mathbf{y} | \boldsymbol{\mu}_s) = - \ln(\Gamma(\varphi)) +
\varphi [\ln(\varphi) - \ln(y_i)] + (\varphi - 1) \ln(y_i) - \frac{y_i
\varphi}{y_i}.
\end{equation*}\]</span></p>
<p>Twice the log-likelihood of the fitted (observed) model is <span class="math display">\[\begin{equation*}
  2 \ln f(\mathbf{y} | \hat{\boldsymbol{\mu}}) = \sum_i y_i -
\ln(\Gamma(\varphi)) + \varphi [\ln(\varphi) - \ln(\hat{\mu}_i)] +
(\varphi - 1) \ln(y_i) - \frac{y_i \varphi}{\hat{\mu}_i}.
\end{equation*}\]</span></p>
<p>Thus the deviance is <span class="math display">\[\begin{equation*}
  2 \sum_i - \ln(\frac{y}{\hat{\mu}_i}) + \frac{y -
\hat{\mu}_i}{\hat{\mu}_i}
\end{equation*}\]</span> after scaling by <span class="math inline">\(\varphi\)</span>.</p>
</div>
<div class="section level4">
<h4 id="sec:invgauss">Inverse Gaussian Distribution<a class="anchor" aria-label="anchor" href="#sec:invgauss"></a>
</h4>
<p>The inverse Gaussian distribution is defined as <span class="math display">\[\begin{equation*}
  f(y | \mu, \varphi) = \sqrt{\frac{\varphi \mu}{2 \pi y^3}} \exp \left(
- \frac{\varphi (y - \mu^2)}{2 \mu y} \right),
\end{equation*}\]</span> where <span class="math inline">\(y &gt;
0\)</span>, <span class="math inline">\(\mu &gt; 0\)</span>, <span class="math inline">\(\text{E}(y) = \mu\)</span>, and <span class="math inline">\(\text{Var}(y) = \mu^2/\varphi\)</span>.</p>
<p>The log-likelihood (of a single observation) is defined as <span class="math display">\[\begin{equation*}
  \ln f(y | \mu) = \frac{1}{2}[\ln(\varphi / 2 \pi y^3)  + \ln(\mu)] -
\varphi \frac{(y - \mu)^2}{2 \mu y}.
\end{equation*}\]</span></p>
<p>Using the inverse log link and writing in terms of <span class="math inline">\(\mu = \exp(w)\)</span>, the log-likelihood can be
written as <span class="math display">\[\begin{equation*}
  \ln f(y | \mu) = \frac{1}{2}[\ln(\varphi / 2 \pi y^3)  + w] - \varphi
\frac{(y - \exp(w))^2}{2 \exp(w) y}.
\end{equation*}\]</span></p>
<p>The derivative of the gamma distribution with respect to <span class="math inline">\(w\)</span> is <span class="math display">\[\begin{equation*}
  \frac{d}{d w} \ln f(y | \mu) = \varphi \left( \frac{y}{2 \exp(w)} -
\frac{\exp(w)}{2y} \right) + \frac{1}{2}.
\end{equation*}\]</span></p>
<p>The second derivative of the gamma distribution with respect to <span class="math inline">\(w\)</span> is <span class="math display">\[\begin{equation*}
  \frac{d^2}{d w^2} \ln f(y | \mu) = - \frac{\varphi(\exp(2w) +
y^2)}{2y\exp(w)}.
\end{equation*}\]</span></p>
<p>Note that this is not a typical parameterization of the inverse
Gaussian distribution. The typical parameterization of the inverse
Gaussian distribution is <span class="math display">\[\begin{equation*}
f(y | \mu, \lambda) = \sqrt{\frac{\lambda}{2 \pi y^3}} \exp \left( -
\frac{\lambda(y - \mu)^2}{2u^2y} \right),
\end{equation*}\]</span> where <span class="math inline">\(y &gt;
0\)</span>, <span class="math inline">\(\mu &gt; 0\)</span>, <span class="math inline">\(\text{E}(y) = \mu\)</span>, and <span class="math inline">\(\text{Var}(y) = \mu^3/\lambda\)</span>, and <span class="math inline">\(\lambda = \mu \varphi\)</span>.</p>
<p>Twice the log-likelihood of the saturated model is <span class="math display">\[\begin{equation*}
  2 \ln f(\mathbf{y} | \boldsymbol{\mu}_s) = \frac{1}{2} \left(
\frac{\lambda}{2 \pi y_i^3} \right)
\end{equation*}\]</span></p>
<p>Twice the log-likelihood of the fitted (observed) model is <span class="math display">\[\begin{equation*}
  2 \ln f(\mathbf{y} | \hat{\boldsymbol{\mu}}) = \frac{1}{2} \left(
\frac{\lambda}{2 \pi y_i^3} \right) - \frac{\lambda (y_i -
\hat{\mu}_i)^2}{2 \hat{\mu}_i^2 y_i}.
\end{equation*}\]</span></p>
<p>Thus the deviance is <span class="math display">\[\begin{equation*}
\sum_i (y_i - \hat{\mu}_i)^2 / (\hat{\mu}_i^2 y_i),
\end{equation*}\]</span> after scaling by <span class="math inline">\(\lambda\)</span>.</p>
</div>
<div class="section level4">
<h4 id="sec:di_and_Di">Table of Inverse Link Functions, <span class="math inline">\(d_i\)</span>, and <span class="math inline">\(D_{i, i}\)</span><a class="anchor" aria-label="anchor" href="#sec:di_and_Di"></a>
</h4>
<p>The following table contains a table of inverse link functions, <span class="math inline">\(d_i\)</span>, and <span class="math inline">\(D_{i, i}\)</span> for each spatial generalized
linear model family. See more details for each family in the previous
subsections.</p>
<table class="table">
<caption>A table of inverse link functions and relevant quantities for
each spatial generalized linear model family.</caption>
<colgroup>
<col width="23%">
<col width="53%">
<col width="8%">
<col width="13%">
</colgroup>
<thead><tr class="header">
<th>Family</th>
<th><span class="math inline">\(u = g^{-1}(w)\)</span></th>
<th><span class="math inline">\(d_i\)</span></th>
<th><span class="math inline">\(D_{i,i}\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Poisson</td>
<td><span class="math inline">\(\mu = \exp(w)\)</span></td>
<td><span class="math inline">\(y_i - \exp(w_i)\)</span></td>
<td><span class="math inline">\(- \exp(w_i)\)</span></td>
</tr>
<tr class="even">
<td>Negative Binomial</td>
<td><span class="math inline">\(\mu = \exp(w)\)</span></td>
<td><span class="math inline">\(\frac{\varphi (y_i -
\exp(w_i))}{\exp(w_i) + \varphi}\)</span></td>
<td><span class="math inline">\(- \frac{\varphi \exp(w_i) (y_i +
\varphi)}{(\varphi + \exp(w_i))^2}\)</span></td>
</tr>
<tr class="odd">
<td>Binomial</td>
<td><span class="math inline">\(\mu = \frac{\exp(w)}{1 +
\exp(w)}\)</span></td>
<td><span class="math inline">\(y_i - \frac{m_i \exp(w_i)}{1 +
\exp(w_i)}\)</span></td>
<td><span class="math inline">\(- \frac{m_i \exp(w_i)}{(1 +
\exp(w_i))^2}\)</span></td>
</tr>
<tr class="even">
<td>Beta</td>
<td><span class="math inline">\(\mu = \frac{\exp(w)}{1 +
\exp(w)}\)</span></td>
<td><span class="math inline">\(- \frac{\varphi \exp(w_i) k_0(w_i | y_i,
\varphi)}{(1 + \exp(w_i))^2}\)</span></td>
<td><span class="math inline">\(- \frac{\varphi \exp(2w_i) k_1(w_i |
y_i, \varphi)}{(1 + \exp(w_i))^4}\)</span></td>
</tr>
<tr class="odd">
<td>Gamma</td>
<td><span class="math inline">\(\mu = \exp(w)\)</span></td>
<td><span class="math inline">\(-\varphi + \frac{\varphi
y_i}{\exp(w_i)}\)</span></td>
<td><span class="math inline">\(- \frac{\varphi
y_i}{\exp(w_i)}\)</span></td>
</tr>
<tr class="even">
<td>Inverse Gaussian</td>
<td><span class="math inline">\(\mu = \exp(w)\)</span></td>
<td><span class="math inline">\(\varphi \left( \frac{y_i}{2 \exp(w_i)} -
\frac{\exp(w_i)}{2y_i} \right) + \frac{1}{2}\)</span></td>
<td><span class="math inline">\(- \frac{\varphi(\exp(2w_i) +
y_i^2)}{2y_i\exp(w_i)}\)</span></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="section level2">
<h2 id="sec:esv">The Empirical Semivariogram (<code>esv()</code>)<a class="anchor" aria-label="anchor" href="#sec:esv"></a>
</h2>
<p>The empirical semivariogram is a moment-based estimate of the
theoretical semivariogram. The empirical semivariogram quantifies half
of the average squared difference in the response among observations in
several distance classes. More formally, the empirical semivariogram is
defined as <span class="math display">\[\begin{equation}\label{eq:esv}
  \hat{\gamma}(h) = \frac{1}{2|N(h)|} \sum_{N(h)} (y_i - y_j)^2,
\end{equation}\]</span> where <span class="math inline">\(N(h)\)</span>
is the set of observations in <span class="math inline">\(\mathbf{y}\)</span> that are <span class="math inline">\(h\)</span> distance apart (distance classes) and
<span class="math inline">\(|N(h)|\)</span> is the cardinality of <span class="math inline">\(N(h)\)</span> <span class="citation">(Cressie
1993)</span>. Often the set <span class="math inline">\(N(h)\)</span>
contains observations that are <span class="math inline">\(h \pm
c\)</span> apart, where <span class="math inline">\(c\)</span> is some
constant. This approach is known as “binning” the empirical
semivariogram. The default in <code>spmodel</code> is to construct the
semivariogram using 15 equally spaced bins where <span class="math inline">\(h\)</span> is contained in <span class="math inline">\((0, h_{max}]\)</span>, and <span class="math inline">\(h_{max}\)</span> is known as a “distance cutoff”.
Distance cutoffs are commonly used when constructing <span class="math inline">\(\hat{\gamma}(h)\)</span> because there tend to be
few pairs with large distances. The default in <code>spmodel</code> is
to use a cutoff of half the maximum distance (hypotenuse) of the
domain’s bounding box.</p>
<p>The main purpose of the empirical semivariogram is its use in
semivariogram weighted least squares estimation for spatial linear
models, though it can also be used as a visual diagnostic to assess the
fit of a spatial covariance function.</p>
</div>
<div class="section level2">
<h2 id="sec:iprod">A Note on Covariance Square Roots and Inverse Products<a class="anchor" aria-label="anchor" href="#sec:iprod"></a>
</h2>
<p>Often <span class="math inline">\(\boldsymbol{\Sigma}^{-1}\)</span>
is not strictly needed for estimation, prediction, or other purposes,
but at least the product between <span class="math inline">\(\boldsymbol{\Sigma}^{-1}\)</span> and some other
matrix is needed. Consider the example of the covariance matrix of <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> and observe
<span class="math inline">\(\mathbf{X}^\top \boldsymbol{\Sigma}^{-1}
\mathbf{X}\)</span> is needed. The most direct way to find this product
is certainly to obtain <span class="math inline">\(\boldsymbol{\Sigma}^{-1}\)</span> and then
multiply by <span class="math inline">\(\mathbf{X}^\top\)</span> on the
left and <span class="math inline">\(\mathbf{X}\)</span> on the right.
This is both computationally expensive and cannot be used to compute
products that involve <span class="math inline">\(\boldsymbol{\Sigma}^{-1/2}\)</span>, which are
often useful. It is helpful to define <span class="math inline">\(\boldsymbol{\Sigma} = \mathbf{S}
\mathbf{S}^\top\)</span> for some matrix <span class="math inline">\(\mathbf{S}\)</span> and rewrite <span class="math inline">\(\mathbf{X}^\top \boldsymbol{\Sigma}^{-1}
\mathbf{X}\)</span> as <span class="math inline">\(\mathbf{X}^\top
(\mathbf{S}^\top)^{-1} \mathbf{S}^{-1} \mathbf{X} = (\mathbf{S}^{-1}
\mathbf{X})^\top \mathbf{S}^{-1} \mathbf{X}\)</span>. Then one computes
the inverse products by finding <span class="math inline">\(\mathbf{S}\)</span>.</p>
<p>One way to find <span class="math inline">\(\mathbf{S}\)</span> is to
use an eigendecomposition. The eigendecomposition of <span class="math inline">\(\boldsymbol{\Sigma}\)</span> (which is real and
symmetric) is given by <span class="math display">\[\begin{equation*}
\boldsymbol{\Sigma} = \mathbf{U} \mathbf{D} \mathbf{U}^\top,
\end{equation*}\]</span> where <span class="math inline">\(\mathbf{U}\)</span> is an orthogonal matrix of
eigenvectors of <span class="math inline">\(\boldsymbol{\Sigma}\)</span>
and <span class="math inline">\(\mathbf{D}\)</span> is a diagonal matrix
with eigenvalues of <span class="math inline">\(\boldsymbol{\Sigma}\)</span> on the diagonal. Then
taking <span class="math inline">\(\mathbf{S} =
\mathbf{U}\mathbf{D}^{1/2}\)</span> implies <span class="math inline">\(\mathbf{S}^{-1} = \mathbf{D}^{-1/2}
\mathbf{U}^\top\)</span>, which follows because <span class="math inline">\(\mathbf{U}\)</span> is orthonormal (<span class="math inline">\(\mathbf{U}^{-1} = \mathbf{U}^\top\)</span>) and is
straightforward to calculate as <span class="math inline">\(\mathbf{D}^{1/2}\)</span> is diagonal. Also notice
that <span class="math inline">\(\boldsymbol{\Sigma}^{1/2} = \mathbf{U}
\mathbf{D}^{1/2} \mathbf{U}^\top\)</span>, where <span class="math inline">\(\mathbf{D}^{1/2}\)</span> is a diagonal matrix
with square roots of eigenvalues of <span class="math inline">\(\boldsymbol{\Sigma}\)</span> on the diagonal. This
result follows because <span class="math display">\[\begin{equation*}
\boldsymbol{\Sigma}^{1/2}\boldsymbol{\Sigma}^{1/2} = \mathbf{U}
\mathbf{D}^{1/2} \mathbf{U}^\top \mathbf{U} \mathbf{D}^{1/2}
\mathbf{U}^\top = \mathbf{U} \mathbf{D}^{1/2} (\mathbf{U}^\top
\mathbf{U}) \mathbf{D}^{1/2} \mathbf{U}^\top = \mathbf{U} \mathbf{D}
\mathbf{U}^\top = \boldsymbol{\Sigma}.
\end{equation*}\]</span> So not only does the eigendecomposition
approach give us the inverse products, it also gives us <span class="math inline">\(\boldsymbol{\Sigma}^{1/2}\)</span> and <span class="math inline">\(\boldsymbol{\Sigma}^{-1/2}\)</span>. While
straightforward, this approach is less efficient than the Cholesky
decomposition <span class="citation">(Golub and Van Loan 2013)</span>,
which we discuss next.</p>
<p>The Cholesky decomposition decomposes <span class="math inline">\(\boldsymbol{\Sigma}\)</span> into the product
between <span class="math inline">\(\mathbf{C}\)</span> and <span class="math inline">\(\mathbf{C}^\top\)</span> (<span class="math inline">\(\boldsymbol{\Sigma} =
\mathbf{C}\mathbf{C}^\top\)</span>), where <span class="math inline">\(\mathbf{C}\)</span> is a lower triangular matrix.
Note that <span class="math inline">\(\mathbf{C}\)</span> is generally
not equal to <span class="math inline">\(\boldsymbol{\Sigma}^{1/2}\)</span>. Taking <span class="math inline">\(\mathbf{S}\)</span> to be <span class="math inline">\(\mathbf{C}\)</span>, we see that finding the
inverse products requires solving <span class="math inline">\(\mathbf{C}^{-1}\mathbf{X}\)</span>. Observe that
<span class="math inline">\(\mathbf{C}^{-1}\mathbf{X} =
\mathbf{A}\)</span> for some matrix <span class="math inline">\(\mathbf{A}\)</span>. This implies <span class="math inline">\(\mathbf{X} = \mathbf{C}\mathbf{A}\)</span>, which
for <span class="math inline">\(\mathbf{A}\)</span> can be efficiently
solved using forward substitution because <span class="math inline">\(\mathbf{C}\)</span> is lower triangular.</p>
<p>The products in this document that involve <span class="math inline">\(\boldsymbol{\Sigma}^{1/2}\)</span> and <span class="math inline">\(\boldsymbol{\Sigma}^{-1}\)</span> are generally
implemented in <code>spmodel</code> using <span class="math inline">\(\mathbf{C}\)</span> and <span class="math inline">\(\mathbf{C}^{-1}\)</span>. The products in this
document that involve <span class="math inline">\(\boldsymbol{\Sigma}^{-1/2}\)</span> still rely on
an eigendecomposition (because recall that generally, <span class="math inline">\(\mathbf{C}^{-1} \mathbf{A} \neq
\boldsymbol{\Sigma}^{-1/2} \mathbf{A}\)</span>). An example is computing
the Pearson residuals.</p>
</div>
<div class="section level2">
<h2 id="sec:computational">A Note on Computational Stability<a class="anchor" aria-label="anchor" href="#sec:computational"></a>
</h2>
<p>Spatial covariance matrices that have approximately no independent
error variance (<span class="math inline">\(\sigma^2_{ie}\)</span>) can
have unstable inverses. When this occurs, a small value can be added to
the diagonal of the covariance matrix (via updating <span class="math inline">\(\sigma^2_{ie}\)</span>) to impose some
computational stability. In <code>spmodel</code>, if <span class="math inline">\(\sigma^2_{ie}\)</span> is approximately zero, a
small amount is added to the diagonal of the covariance matrix.
Specifically, for spatial linear models, <span class="math inline">\(\sigma^2_{ie, up} = \text{max}(\sigma^2_{ie},
\sigma^2_{de}/10^4)\)</span>, where <span class="math inline">\(\sigma^2_{ie, up}\)</span> denotes an “updated”
version of <span class="math inline">\(\sigma^2_{ie}\)</span>. For
spatial generalized linear models, <span class="math inline">\(\sigma^2_{ie, up} = \text{max}(\sigma^2_{ie},
\sigma^2_{de}/10^4, d)\)</span>, where d = <span class="math inline">\(1/10^4\)</span>. Previously (<code>spmodel</code>
v<span class="math inline">\(\le 0.8.0\)</span>), <span class="math inline">\(d = \text{min}(1/10^4, s^2/10^4)\)</span>, where
<span class="math inline">\(s^2\)</span> is the sample variance of a
linear regression of <span class="math inline">\(\ln(\mathbf{y} +
1)\)</span> on <span class="math inline">\(\mathbf{X}\)</span>. This
value of <span class="math inline">\(\sigma^2_{ie, up}\)</span> is also
added to the diagonal of <span class="math inline">\(\mathbf{X}^\top
\boldsymbol{\Sigma}^{-1} \mathbf{X} + \mathbf{X}^\top
\boldsymbol{\Sigma}^{-1} (\mathbf{D} - \boldsymbol{\Sigma}^{-1})^{-1}
\boldsymbol{\Sigma}^{-1} \mathbf{X}\)</span>, used via the
Sherman-Morrison-Woodbury formula required to efficiently find the log
determinant and inverse of the Hessian, <span class="math inline">\(\mathbf{G}\)</span>, when using spatial indexing
for big data. For more on stability of spatial covariance matrices, see
<span class="citation">Diamond and Armstrong (1984)</span>, <span class="citation">Posa (1989)</span>, <span class="citation">O’Dowd
(1991)</span>, <span class="citation">Ababou, Bagtzoglou, and Wood
(1994)</span>, <span class="citation">Booker et al. (1999)</span>, <span class="citation">Martin and Simpson (2005)</span>, <span class="citation">Bivand, Pebesma, and Gomez-Rubio (2013)</span>, and
<span class="citation">Ver Hoef (2018)</span>, among others.</p>
</div>
<div class="section level2">
<h2 class="unnumbered" id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-ababou1994condition" class="csl-entry">
Ababou, Rachid, Amvrossios C Bagtzoglou, and Eric F Wood. 1994.
<span>“On the Condition Number of Covariance Matrices in Kriging,
Estimation, and Simulation of Random Fields.”</span> <em>Mathematical
Geology</em> 26: 99–133.
</div>
<div id="ref-anselin2010geoda" class="csl-entry">
Anselin, Luc, Ibnu Syabri, and Youngihn Kho. 2010. <span>“GeoDa: An
Introduction to Spatial Data Analysis.”</span> In <em>Handbook of
Applied Spatial Analysis</em>, 73–89. Springer.
</div>
<div id="ref-bivand2013sp" class="csl-entry">
Bivand, Roger S., Edzer Pebesma, and Virgilio Gomez-Rubio. 2013.
<em>Applied Spatial Data Analysis with <span>R</span>, Second
Edition</em>. Springer, NY. <a href="https://asdar-book.org/" class="external-link">https://asdar-book.org/</a>.
</div>
<div id="ref-booker1999rigorous" class="csl-entry">
Booker, Andrew J, John E Dennis, Paul D Frank, David B Serafini,
Virginia Torczon, and Michael W Trosset. 1999. <span>“A Rigorous
Framework for Optimization of Expensive Functions by Surrogates.”</span>
<em>Structural Optimization</em> 17: 1–13.
</div>
<div id="ref-breiman2001random" class="csl-entry">
Breiman, Leo. 2001. <span>“Random Forests.”</span> <em>Machine
Learning</em> 45 (1): 5–32.
</div>
<div id="ref-brent1971algorithm" class="csl-entry">
Brent, Richard P. 1971. <span>“An Algorithm with Guaranteed Convergence
for Finding a Zero of a Function.”</span> <em>The Computer Journal</em>
14 (4): 422–25.
</div>
<div id="ref-cook1979influential" class="csl-entry">
Cook, R Dennis. 1979. <span>“Influential Observations in Linear
Regression.”</span> <em>Journal of the American Statistical
Association</em> 74 (365): 169–74.
</div>
<div id="ref-cook1982residuals" class="csl-entry">
Cook, R Dennis, and Sanford Weisberg. 1982. <em>Residuals and Influence
in Regression</em>. New York: Chapman; Hall.
</div>
<div id="ref-cressie1985fitting" class="csl-entry">
Cressie, Noel. 1985. <span>“Fitting Variogram Models by Weighted Least
Squares.”</span> <em>Journal of the International Association for
Mathematical Geology</em> 17 (5): 563–86.
</div>
<div id="ref-cressie1993statistics" class="csl-entry">
———. 1993. <em>Statistics for Spatial Data</em>. John Wiley &amp; Sons.
</div>
<div id="ref-cribari2010beta" class="csl-entry">
Cribari-Neto, Francisco, and Achim Zeileis. 2010. <span>“Beta Regression
in <span>R</span>.”</span> <em>Journal of Statistical Software</em> 34
(2): 1–24. <a href="https://doi.org/10.18637/jss.v034.i02" class="external-link">https://doi.org/10.18637/jss.v034.i02</a>.
</div>
<div id="ref-curriero1999composite" class="csl-entry">
Curriero, Frank C, and Subhash Lele. 1999. <span>“A Composite Likelihood
Approach to Semivariogram Estimation.”</span> <em>Journal of
Agricultural, Biological, and Environmental Statistics</em>, 9–28.
</div>
<div id="ref-diamond1984robustness" class="csl-entry">
Diamond, Phil, and Margaret Armstrong. 1984. <span>“Robustness of
Variograms and Conditioning of Kriging Matrices.”</span> <em>Journal of
the International Association for Mathematical Geology</em> 16: 809–22.
</div>
<div id="ref-espinheira2008beta" class="csl-entry">
Espinheira, Patrícia L, Silvia LP Ferrari, and Francisco Cribari-Neto.
2008. <span>“On Beta Regression Residuals.”</span> <em>Journal of
Applied Statistics</em> 35 (4): 407–19.
</div>
<div id="ref-fawcett2006introduction" class="csl-entry">
Fawcett, Tom. 2006. <span>“An Introduction to ROC Analysis.”</span>
<em>Pattern Recognition Letters</em> 27 (8): 861–74.
</div>
<div id="ref-fox2020comparing" class="csl-entry">
Fox, Eric W, Jay M Ver Hoef, and Anthony R Olsen. 2020. <span>“Comparing
Spatial Regression to Random Forests for Large Environmental Data
Sets.”</span> <em>PloS One</em> 15 (3): e0229509.
</div>
<div id="ref-goldman2000statistical" class="csl-entry">
Goldman, Nick, and Simon Whelan. 2000. <span>“Statistical Tests of
Gamma-Distributed Rate Heterogeneity in Models of Sequence Evolution in
Phylogenetics.”</span> <em>Molecular Biology and Evolution</em> 17 (6):
975–78.
</div>
<div id="ref-golub2013matrix" class="csl-entry">
Golub, Gene H, and Charles F Van Loan. 2013. <em>Matrix
Computations</em>. JHU press.
</div>
<div id="ref-hanley1982meaning" class="csl-entry">
Hanley, James A, and Barbara J McNeil. 1982. <span>“The Meaning and Use
of the Area Under a Receiver Operating Characteristic (ROC)
Curve.”</span> <em>Radiology</em> 143 (1): 29–36.
</div>
<div id="ref-harville1977maximum" class="csl-entry">
Harville, David A. 1977. <span>“Maximum Likelihood Approaches to
Variance Component Estimation and to Related Problems.”</span>
<em>Journal of the American Statistical Association</em> 72 (358):
320–38.
</div>
<div id="ref-harville1992mean" class="csl-entry">
Harville, David A, and Daniel R Jeske. 1992. <span>“Mean Squared Error
of Estimation or Prediction Under a General Linear Model.”</span>
<em>Journal of the American Statistical Association</em> 87 (419):
724–31.
</div>
<div id="ref-henderson1975best" class="csl-entry">
Henderson, Charles R. 1975. <span>“Best Linear Unbiased Estimation and
Prediction Under a Selection Model.”</span> <em>Biometrics</em>, 423–47.
</div>
<div id="ref-hoeting2006model" class="csl-entry">
Hoeting, Jennifer A, Richard A Davis, Andrew A Merton, and Sandra E
Thompson. 2006. <span>“Model Selection for Geostatistical
Models.”</span> <em>Ecological Applications</em> 16 (1): 87–98.
</div>
<div id="ref-hrong1996approximate" class="csl-entry">
Hrong-Tai Fai, Alex, and Paul L Cornelius. 1996. <span>“Approximate
f-Tests of Multiple Degree of Freedom Hypotheses in Generalized Least
Squares Analyses of Unbalanced Split-Plot Experiments.”</span>
<em>Journal of Statistical Computation and Simulation</em> 54 (4):
363–78.
</div>
<div id="ref-james2013introduction" class="csl-entry">
James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.
2013. <em>An Introduction to Statistical Learning</em>. Vol. 112.
Springer.
</div>
<div id="ref-kackar1984approximations" class="csl-entry">
Kackar, Raghu N, and David A Harville. 1984. <span>“Approximations for
Standard Errors of Estimators of Fixed and Random Effects in Mixed
Linear Models.”</span> <em>Journal of the American Statistical
Association</em> 79 (388): 853–62.
</div>
<div id="ref-kenward1997small" class="csl-entry">
Kenward, Michael G, and James H Roger. 1997. <span>“Small Sample
Inference for Fixed Effects from Restricted Maximum Likelihood.”</span>
<em>Biometrics</em>, 983–97.
</div>
<div id="ref-kenward2009improved" class="csl-entry">
———. 2009. <span>“An Improved Approximation to the Precision of Fixed
Effects from Restricted Maximum Likelihood.”</span> <em>Computational
Statistics &amp; Data Analysis</em> 53 (7): 2583–95.
</div>
<div id="ref-littell2006sas" class="csl-entry">
Littell, Ramon C, George A Milliken, Walter W Stroup, Russell D
Wolfinger, and Schabenberber Oliver. 2006. <em>SAS for Mixed
Models</em>. SAS publishing.
</div>
<div id="ref-macqueen1967classification" class="csl-entry">
MacQueen, J. 1967. <span>“Classification and Analysis of Multivariate
Observations.”</span> In <em>5th Berkeley Symp. Math. Statist.
Probability</em>, 281–97. University of California Los Angeles LA USA.
</div>
<div id="ref-martin2005use" class="csl-entry">
Martin, Jay D, and Timothy W Simpson. 2005. <span>“Use of Kriging Models
to Approximate Deterministic Computer Models.”</span> <em>AIAA
Journal</em> 43 (4): 853–63.
</div>
<div id="ref-meinshausen2006quantile" class="csl-entry">
Meinshausen, Nicolai, and Greg Ridgeway. 2006. <span>“Quantile
Regression Forests.”</span> <em>Journal of Machine Learning
Research</em> 7 (6).
</div>
<div id="ref-montgomery2021introduction" class="csl-entry">
Montgomery, Douglas C, Elizabeth A Peck, and G Geoffrey Vining. 2021.
<em>Introduction to Linear Regression Analysis</em>. John Wiley &amp;
Sons.
</div>
<div id="ref-muschelli2020roc" class="csl-entry">
Muschelli III, John. 2020. <span>“ROC and AUC with a Binary Predictor: A
Potentially Misleading Metric.”</span> <em>Journal of
Classification</em> 37 (3): 696–708.
</div>
<div id="ref-myers2012generalized" class="csl-entry">
Myers, Raymond H, Douglas C Montgomery, G Geoffrey Vining, and Timothy J
Robinson. 2012. <em>Generalized Linear Models: With Applications in
Engineering and the Sciences</em>. John Wiley &amp; Sons.
</div>
<div id="ref-nelder1965simplex" class="csl-entry">
Nelder, John A, and Roger Mead. 1965. <span>“A Simplex Method for
Function Minimization.”</span> <em>The Computer Journal</em> 7 (4):
308–13.
</div>
<div id="ref-odowd1991conditioning" class="csl-entry">
O’Dowd, RJ. 1991. <span>“Conditioning of Coefficient Matrices of
Ordinary Kriging.”</span> <em>Mathematical Geology</em> 23: 721–39.
</div>
<div id="ref-patterson1971recovery" class="csl-entry">
Patterson, Desmond, and Robin Thompson. 1971. <span>“Recovery of
Inter-Block Information When Block Sizes Are Unequal.”</span>
<em>Biometrika</em> 58 (3): 545–54.
</div>
<div id="ref-pinheiro2006mixed" class="csl-entry">
Pinheiro, José, and Douglas Bates. 2006. <em>Mixed-Effects Models in s
and s-PLUS</em>. Springer science &amp; business media.
</div>
<div id="ref-posa1989conditioning" class="csl-entry">
Posa, D. 1989. <span>“Conditioning of the Stationary Kriging Matrices
for Some Well-Known Covariance Models.”</span> <em>Mathematical
Geology</em> 21: 755–65.
</div>
<div id="ref-prasad1990estimation" class="csl-entry">
Prasad, NG Narasimha, and Jon NK Rao. 1990. <span>“The Estimation of the
Mean Squared Error of Small-Area Estimators.”</span> <em>Journal of the
American Statistical Association</em> 85 (409): 163–71.
</div>
<div id="ref-robin2011proc" class="csl-entry">
Robin, Xavier, Natacha Turck, Alexandre Hainard, Natalia Tiberti,
Frédérique Lisacek, Jean-Charles Sanchez, and Markus Müller. 2011.
<span>“pROC: An Open-Source Package for r and s+ to Analyze and Compare
ROC Curves.”</span> <em>BMC Bioinformatics</em> 12: 77.
</div>
<div id="ref-satterthwaite1946approximate" class="csl-entry">
Satterthwaite, Franklin E. 1946. <span>“An Approximate Distribution of
Estimates of Variance Components.”</span> <em>Biometrics Bulletin</em> 2
(6): 110–14.
</div>
<div id="ref-schluchter1990small" class="csl-entry">
Schluchter, Mark D, and Janet T Elashoff. 1990. <span>“Small-Sample
Adjustments to Tests with Unbalanced Repeated Measures Assuming Several
Covariance Structures.”</span> <em>Journal of Statistical Computation
and Simulation</em> 37 (1-2): 69–87.
</div>
<div id="ref-schwarz1978estimating" class="csl-entry">
Schwarz, Gideon. 1978. <span>“Estimating the Dimension of a
Model.”</span> <em>The Annals of Statistics</em>, 461–64.
</div>
<div id="ref-searle2009variance" class="csl-entry">
Searle, Shayle R, George Casella, and Charles E McCulloch. 2009.
<em>Variance Components</em>. John Wiley &amp; Sons.
</div>
<div id="ref-self1987asymptotic" class="csl-entry">
Self, Steven G, and Kung-Yee Liang. 1987. <span>“Asymptotic Properties
of Maximum Likelihood Estimators and Likelihood Ratio Tests Under
Nonstandard Conditions.”</span> <em>Journal of the American Statistical
Association</em> 82 (398): 605–10.
</div>
<div id="ref-sherman1949adjustment" class="csl-entry">
Sherman, Jack. 1949. <span>“Adjustment of an Inverse Matrix
Corresponding to Changes in the Elements of a Given Column or a Given
Row of the Original Matrix.”</span> <em>Annals of Mathematical
Statistics</em> 20 (4): 621.
</div>
<div id="ref-sherman1950adjustment" class="csl-entry">
Sherman, Jack, and Winifred J Morrison. 1950. <span>“Adjustment of an
Inverse Matrix Corresponding to a Change in One Element of a Given
Matrix.”</span> <em>The Annals of Mathematical Statistics</em> 21 (1):
124–27.
</div>
<div id="ref-stram1994variance" class="csl-entry">
Stram, Daniel O, and Jae Won Lee. 1994. <span>“Variance Components
Testing in the Longitudinal Mixed Effects Model.”</span>
<em>Biometrics</em>, 1171–77.
</div>
<div id="ref-tobler1970computer" class="csl-entry">
Tobler, Waldo R. 1970. <span>“A Computer Movie Simulating Urban Growth
in the Detroit Region.”</span> <em>Economic Geography</em> 46 (sup1):
234–40.
</div>
<div id="ref-venables2002mass" class="csl-entry">
Venables, W. N., and B. D. Ripley. 2002. <em>Modern Applied Statistics
with s</em>. Fourth. New York: Springer. <a href="http://www.stats.ox.ac.uk/pub/MASS4/" class="external-link">http://www.stats.ox.ac.uk/pub/MASS4/</a>.
</div>
<div id="ref-ver2012invented" class="csl-entry">
Ver Hoef, Jay M. 2012. <span>“Who Invented the Delta Method?”</span>
<em>The American Statistician</em> 66 (2): 124–27.
</div>
<div id="ref-ver2018kriging" class="csl-entry">
———. 2018. <span>“Kriging Models for Linear Networks and Non-Euclidean
Distances: Cautions and Solutions.”</span> <em>Methods in Ecology and
Evolution</em> 9 (6): 1600–1613.
</div>
<div id="ref-ver2024marginal" class="csl-entry">
Ver Hoef, Jay M, Eryn Blagg, Michael Dumelle, Philip M Dixon, Dale L
Zimmerman, and Paul B Conn. 2024. <span>“Marginal Inference for
Hierarchical Generalized Linear Mixed Models with Patterned Covariance
Matrices Using the Laplace Approximation.”</span>
<em>Environmetrics</em>, e2872.
</div>
<div id="ref-ver2023indexing" class="csl-entry">
Ver Hoef, Jay M, Michael Dumelle, Matt Higham, Erin E Peterson, and
Daniel J Isaak. 2023. <span>“Indexing and Partitioning the Spatial
Linear Model for Large Data Sets.”</span> <em>arXiv Preprint
arXiv:2305.07811</em>.
</div>
<div id="ref-ver2018spatial" class="csl-entry">
Ver Hoef, Jay M, Erin E Peterson, Mevin B Hooten, Ephraim M Hanks, and
Marie-Josèe Fortin. 2018. <span>“Spatial Autoregressive Models for
Statistical Inference from Ecological Data.”</span> <em>Ecological
Monographs</em> 88 (1): 36–59.
</div>
<div id="ref-wolf1978helmert" class="csl-entry">
Wolf, Helmut. 1978. <span>“The Helmert Block Method-Its Origin and
Development.”</span> In <em>Proceedings of the Second International
Symposium on Problems Related to the Redefinition of North American
Geodetic Networks,(NOAA, Arlington-Va, 1978)</em>, 319–26.
</div>
<div id="ref-wolfinger1994computing" class="csl-entry">
Wolfinger, Russ, Randy Tobias, and John Sall. 1994. <span>“Computing
Gaussian Likelihoods and Their Derivatives for General Linear Mixed
Models.”</span> <em>SIAM Journal on Scientific Computing</em> 15 (6):
1294–1310.
</div>
<div id="ref-woodbury1950inverting" class="csl-entry">
Woodbury, Max A. 1950. <em>Inverting Modified Matrices</em>. Department
of Statistics, Princeton University.
</div>
<div id="ref-wright2015ranger" class="csl-entry">
Wright, Marvin N, and Andreas Ziegler. 2015. <span>“Ranger: A Fast
Implementation of Random Forests for High Dimensional Data in c++ and
r.”</span> <em>arXiv Preprint arXiv:1508.04409</em>.
</div>
<div id="ref-zimmerman2024spatial" class="csl-entry">
Zimmerman, Dale L, and Jay M Ver Hoef. 2024. <em>Spatial Linear Models
for Environmental Data</em>. CRC Press.
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p></p>
<p>Developed by Michael Dumelle, Matt Higham, Jay M. Ver Hoef.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.9.</p>
</div>

      </footer>
</div>

  


  

  </body>
</html>
