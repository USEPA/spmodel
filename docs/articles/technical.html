<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Technical Details • spmodel</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.4.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.4.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Technical Details">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">spmodel</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.7.0</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/guide.html">A Detailed Guide to spmodel</a></li>
    <li><a class="dropdown-item" href="../articles/introduction.html">An Introduction to spmodel</a></li>
    <li><a class="dropdown-item" href="../articles/SPGLMs.html">Spatial Generalized Linear Models in spmodel</a></li>
    <li><a class="dropdown-item" href="../articles/technical.html">Technical Details</a></li>
  </ul>
</li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/USEPA/spmodel/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>Technical Details</h1>
                        <h4 data-toc-skip class="author">Michael
Dumelle, Matt Higham, and Jay M. Ver Hoef</h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/USEPA/spmodel/blob/HEAD/vignettes/articles/technical.Rmd" class="external-link"><code>vignettes/articles/technical.Rmd</code></a></small>
      <div class="d-none name"><code>technical.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="sec:introduction">Introduction<a class="anchor" aria-label="anchor" href="#sec:introduction"></a>
</h2>
<p>This vignette covers technical details regarding the functions in
that perform computations in <code>spmodel</code>. We first provide a
notation guide and then describe relevant details for each function.</p>
<p>If you use <code>spmodel</code> in a formal publication or report,
please cite it. Citing <code>spmodel</code> lets us devote more
resources to it in the future. To view the <code>spmodel</code>
citation, run</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/citation.html" class="external-link">citation</a></span><span class="op">(</span>package <span class="op">=</span> <span class="st">"spmodel"</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">#&gt; To cite spmodel in publications use:</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;   Dumelle M, Higham M, Ver Hoef JM (2023). spmodel: Spatial statistical</span></span>
<span><span class="co">#&gt;   modeling and prediction in R. PLOS ONE 18(3): e0282524.</span></span>
<span><span class="co">#&gt;   https://doi.org/10.1371/journal.pone.0282524</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; A BibTeX entry for LaTeX users is</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;   @Article{,</span></span>
<span><span class="co">#&gt;     title = {{spmodel}: Spatial statistical modeling and prediction in {R}},</span></span>
<span><span class="co">#&gt;     author = {Michael Dumelle and Matt Higham and Jay M. {Ver Hoef}},</span></span>
<span><span class="co">#&gt;     journal = {PLOS ONE},</span></span>
<span><span class="co">#&gt;     year = {2023},</span></span>
<span><span class="co">#&gt;     volume = {18},</span></span>
<span><span class="co">#&gt;     number = {3},</span></span>
<span><span class="co">#&gt;     pages = {1--32},</span></span>
<span><span class="co">#&gt;     doi = {10.1371/journal.pone.0282524},</span></span>
<span><span class="co">#&gt;     url = {https://doi.org/10.1371/journal.pone.0282524},</span></span>
<span><span class="co">#&gt;   }</span></span></code></pre>
</div>
<div class="section level2">
<h2 id="sec:notation">Notation Guide<a class="anchor" aria-label="anchor" href="#sec:notation"></a>
</h2>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mi>n</mi></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mtext mathvariant="normal">Sample size</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mi>𝐲</mi></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mtext mathvariant="normal">Response vector</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mi>𝛃</mi></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mtext mathvariant="normal">Fixed effect parameter vector</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mi>𝐗</mi></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mtext mathvariant="normal">Design matrix of known explanatory variables (covariates)</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mi>p</mi></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mrow><mtext mathvariant="normal">The number of linearly independent columns in </mtext><mspace width="0.333em"></mspace></mrow><mi>𝐗</mi></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mi>𝛍</mi></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mtext mathvariant="normal">Mean vector</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mi>𝐰</mi></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mtext mathvariant="normal">Latent generalized linear model mean on the link scale</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mi>φ</mi></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mtext mathvariant="normal">Dispersion parameter</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mi>𝐙</mi></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mtext mathvariant="normal">Design matrix of known random effect variables</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mi>𝛉</mi></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mtext mathvariant="normal">Covariance parameter vector</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mi>𝚺</mi></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mrow><mtext mathvariant="normal">Covariance matrix evaluated at </mtext><mspace width="0.333em"></mspace></mrow><mi>𝛉</mi></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mrow><mtext mathvariant="normal">The inverse of </mtext><mspace width="0.333em"></mspace></mrow><mi>𝚺</mi></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><msup><mi>𝚺</mi><mrow><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mrow><mtext mathvariant="normal">The square root of </mtext><mspace width="0.333em"></mspace></mrow><mi>𝚺</mi></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mrow><mtext mathvariant="normal">The inverse of </mtext><mspace width="0.333em"></mspace></mrow><msup><mi>𝚺</mi><mrow><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mi>𝚯</mi></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mtext mathvariant="normal">General parameter vector</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mo>ℓ</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝚯</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mrow><mtext mathvariant="normal">Log-likelihood evaluated at </mtext><mspace width="0.333em"></mspace></mrow><mi>𝚯</mi></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mi>𝛕</mi></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mtext mathvariant="normal">Spatial (dependent) random error</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mi>𝛜</mi></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mtext mathvariant="normal">Independent (non-spatial) random error</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><msup><mi>𝐀</mi><mo>*</mo></msup></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><mi>𝐀</mi><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> for a general matrix </mtext><mspace width="0.333em"></mspace></mrow><mi>𝐀</mi><mrow><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> (this is known as whitening </mtext><mspace width="0.333em"></mspace></mrow><mi>𝐀</mi><mtext mathvariant="normal">)</mtext></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{equation*}
  \begin{split}
   n &amp; = \text{Sample size} \\
   \mathbf{y} &amp; = \text{Response vector} \\
   \boldsymbol{\beta} &amp; = \text{Fixed effect parameter vector} \\
   \mathbf{X} &amp; = \text{Design matrix of known explanatory variables (covariates)} \\
   p &amp; = \text{The number of linearly independent columns in } \mathbf{X} \\
   \boldsymbol{\mu} &amp; = \text{Mean vector} \\
   \mathbf{w} &amp; = \text{Latent generalized linear model mean on the link scale} \\
   \varphi &amp; = \text{Dispersion parameter} \\
   \mathbf{Z} &amp; = \text{Design matrix of known random effect variables} \\
   \boldsymbol{\theta} &amp; = \text{Covariance parameter vector} \\   
   \boldsymbol{\Sigma} &amp; = \text{Covariance matrix evaluated at } \boldsymbol{\theta} \\
   \boldsymbol{\Sigma}^{-1} &amp; = \text{The inverse of } \boldsymbol{\Sigma} \\
   \boldsymbol{\Sigma}^{1/2} &amp; = \text{The square root of } \boldsymbol{\Sigma} \\
   \boldsymbol{\Sigma}^{-1/2} &amp; = \text{The inverse of } \boldsymbol{\Sigma}^{1/2} \\
   \boldsymbol{\Theta} &amp; = \text{General parameter vector} \\  
   \ell(\boldsymbol{\Theta}) &amp; = \text{Log-likelihood evaluated at } \boldsymbol{\Theta} \\
   \boldsymbol{\tau} &amp; = \text{Spatial (dependent) random error} \\
   \boldsymbol{\epsilon} &amp; = \text{Independent (non-spatial) random error} \\
   \mathbf{A}^* &amp; = \boldsymbol{\Sigma}^{-1/2}\mathbf{A} \text{ for a general matrix } \mathbf{A} \text{ (this is known as whitening $\mathbf{A}$)} 
  \end{split}
\end{equation*}</annotation></semantics></math><p>A hat indicates the parameters are estimated (i.e.,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat{\boldsymbol{\beta}}</annotation></semantics></math>)
or evaluated at a relevant estimated parameter vector (e.g.,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat{\boldsymbol{\Sigma}}</annotation></semantics></math>
is evaluated at
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>𝛉</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat{\boldsymbol{\theta}}</annotation></semantics></math>).
When
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>ℓ</mo><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>𝚯</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\ell(\boldsymbol{\hat{\Theta}})</annotation></semantics></math>
is written, it means the log-likelihood evaluated at its maximum,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>𝚯</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\boldsymbol{\hat{\Theta}}</annotation></semantics></math>.
When the covariance matrix of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐀</mi><annotation encoding="application/x-tex">\mathbf{A}</annotation></semantics></math>
is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝚺</mi><annotation encoding="application/x-tex">\boldsymbol{\Sigma}</annotation></semantics></math>,
we say
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>𝐀</mi><mo>*</mo></msup><annotation encoding="application/x-tex">\mathbf{A}^*</annotation></semantics></math>
“whitens”
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐀</mi><annotation encoding="application/x-tex">\mathbf{A}</annotation></semantics></math>
because
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐀</mi><mo>*</mo></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><mi>𝐀</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐀</mi><mo stretchy="true" form="postfix">)</mo></mrow><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><mo>=</mo><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><mi>𝚺</mi><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><msup><mi>𝚺</mi><mrow><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝚺</mi><mrow><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>𝐈</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
\text{Cov}(\mathbf{A}^*) = \text{Cov}(\boldsymbol{\Sigma}^{-1/2}\mathbf{A}) = \boldsymbol{\Sigma}^{-1/2}\text{Cov}(\mathbf{A})\boldsymbol{\Sigma}^{-1/2} = \boldsymbol{\Sigma}^{-1/2}\boldsymbol{\Sigma} \boldsymbol{\Sigma}^{-1/2} = (\boldsymbol{\Sigma}^{-1/2}\boldsymbol{\Sigma}^{1/2})(\boldsymbol{\Sigma}^{1/2}\boldsymbol{\Sigma}^{-1/2}) = \mathbf{I}.
\end{equation*}</annotation></semantics></math> Later we discuss how to
obtain
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>𝚺</mi><mrow><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><annotation encoding="application/x-tex">\boldsymbol{\Sigma}^{1/2}</annotation></semantics></math>.</p>
<p>Additional notation is used in the <code><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict()</a></code> section:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>𝐲</mi><mi>o</mi></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mtext mathvariant="normal">Observed response vector</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>𝐲</mi><mi>u</mi></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mtext mathvariant="normal">Unobserved response vector</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>𝐗</mi><mi>o</mi></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mtext mathvariant="normal">Design matrix of known explanatory variables at observed response variable locations</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>𝐗</mi><mi>u</mi></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mtext mathvariant="normal">Design matrix of known explanatory variables at unobserved response variable locations</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>𝚺</mi><mi>o</mi></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mrow><mrow><mtext mathvariant="normal">Covariance matrix of </mtext><mspace width="0.333em"></mspace></mrow><msub><mi>𝐲</mi><mi>o</mi></msub><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> evaluated at </mtext><mspace width="0.333em"></mspace></mrow></mrow><mi>𝛉</mi></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>𝚺</mi><mi>u</mi></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mrow><mrow><mtext mathvariant="normal">Covariance matrix of </mtext><mspace width="0.333em"></mspace></mrow><msub><mi>𝐲</mi><mi>u</mi></msub><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> evaluated at </mtext><mspace width="0.333em"></mspace></mrow></mrow><mi>𝛉</mi></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>𝚺</mi><mrow><mi>u</mi><mi>o</mi></mrow></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mrow><mrow><mtext mathvariant="normal">A matrix of covariances between </mtext><mspace width="0.333em"></mspace></mrow><msub><mi>𝐲</mi><mi>u</mi></msub><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> and </mtext><mspace width="0.333em"></mspace></mrow><msub><mi>𝐲</mi><mi>o</mi></msub><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> evaluated at </mtext><mspace width="0.333em"></mspace></mrow></mrow><mi>𝛉</mi></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>𝐰</mi><mi>o</mi></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mrow><mrow><mtext mathvariant="normal">Latent </mtext><mspace width="0.333em"></mspace></mrow><mi>𝐰</mi><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> for each observation in </mtext><mspace width="0.333em"></mspace></mrow><msub><mi>𝐲</mi><mi>o</mi></msub></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>𝐰</mi><mi>u</mi></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mrow><mrow><mtext mathvariant="normal">Latent </mtext><mspace width="0.333em"></mspace></mrow><mi>𝐰</mi><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> for each observation in </mtext><mspace width="0.333em"></mspace></mrow><msub><mi>𝐲</mi><mi>o</mi></msub></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>𝐆</mi><mi>o</mi></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mrow><mrow><mtext mathvariant="normal">Hessian for </mtext><mspace width="0.333em"></mspace></mrow><msub><mi>𝐰</mi><mi>o</mi></msub></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{equation*}
  \begin{split}
   \mathbf{y}_o &amp; = \text{Observed response vector} \\
   \mathbf{y}_u &amp; = \text{Unobserved response vector} \\
   \mathbf{X}_o &amp; = \text{Design matrix of known explanatory variables at observed response variable locations} \\
   \mathbf{X}_u &amp; = \text{Design matrix of known explanatory variables at unobserved response variable locations} \\
   \boldsymbol{\Sigma}_o &amp; = \text{Covariance matrix of $\mathbf{y}_o$ evaluated at } \boldsymbol{\theta} \\
   \boldsymbol{\Sigma}_u &amp; = \text{Covariance matrix of $\mathbf{y}_u$ evaluated at } \boldsymbol{\theta} \\
   \boldsymbol{\Sigma}_{uo} &amp; = \text{A matrix of covariances between $\mathbf{y}_u$ and $\mathbf{y}_o$ evaluated at } \boldsymbol{\theta} \\
   \mathbf{w}_o &amp; = \text{Latent $\mathbf{w}$ for each observation in $\mathbf{y}_o$} \\
   \mathbf{w}_u &amp; = \text{Latent $\mathbf{w}$ for each observation in $\mathbf{y}_o$} \\
   \mathbf{G}_o &amp; = \text{Hessian for $\mathbf{w}_o$} \\
  \end{split}
\end{equation*}</annotation></semantics></math></p>
</div>
<div class="section level2">
<h2 id="sec:splms">Spatial Linear Models<a class="anchor" aria-label="anchor" href="#sec:splms"></a>
</h2>
<p>Statistical linear models are often parameterized as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐲</mi><mo>=</mo><mi>𝐗</mi><mi>𝛃</mi><mo>+</mo><mi>𝛜</mi><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation}\label{eq:lm}
 \mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon},
\end{equation}</annotation></semantics></math> where for a sample size
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐲</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math>
is an
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n \times 1</annotation></semantics></math>
column vector of response variables,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐗</mi><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics></math>
is an
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">n \times p</annotation></semantics></math>
design (model) matrix of explanatory variables,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛃</mi><annotation encoding="application/x-tex">\boldsymbol{\beta}</annotation></semantics></math>
is a
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">p \times 1</annotation></semantics></math>
column vector of fixed effects controlling the impact of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐗</mi><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics></math>
on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐲</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛜</mi><annotation encoding="application/x-tex">\boldsymbol{\epsilon}</annotation></semantics></math>
is an
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n \times 1</annotation></semantics></math>
column vector of random errors. We typically assume that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝛜</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>𝟎</mn></mrow><annotation encoding="application/x-tex">\text{E}(\boldsymbol{\epsilon}) = \mathbf{0}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝛜</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msubsup><mi>σ</mi><mi>ϵ</mi><mn>2</mn></msubsup><mi>𝐈</mi></mrow><annotation encoding="application/x-tex">\text{Cov}(\boldsymbol{\epsilon}) = \sigma^2_\epsilon \mathbf{I}</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mo>⋅</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\text{E}(\cdot)</annotation></semantics></math>
denotes expectation,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mo>⋅</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\text{Cov}(\cdot)</annotation></semantics></math>
denotes covariance,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>σ</mi><mi>ϵ</mi><mn>2</mn></msubsup><annotation encoding="application/x-tex">\sigma^2_\epsilon</annotation></semantics></math>
denotes a variance parameter, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐈</mi><annotation encoding="application/x-tex">\mathbf{I}</annotation></semantics></math>
denotes the identity matrix.</p>
<p>The model
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐲</mi><mo>=</mo><mi>𝐗</mi><mi>𝛃</mi><mo>+</mo><mi>𝛜</mi></mrow><annotation encoding="application/x-tex">\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}</annotation></semantics></math>
assumes the elements of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐲</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math>
are uncorrelated. Typically for spatial data, elements of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐲</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math>
are correlated, as observations close together in space tend to be more
similar than observations far apart <span class="citation">(Tobler
1970)</span>. Failing to properly accommodate the spatial dependence in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐲</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math>
can cause researchers to draw incorrect conclusions about their data. To
accommodate spatial dependence in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐲</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math>,
an
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n \times 1</annotation></semantics></math>
spatial random effect,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛕</mi><annotation encoding="application/x-tex">\boldsymbol{\tau}</annotation></semantics></math>,
is added to the linear model, yielding the model
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐲</mi><mo>=</mo><mi>𝐗</mi><mi>𝛃</mi><mo>+</mo><mi>𝛕</mi><mo>+</mo><mi>𝛜</mi><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation}\label{eq:splm}
 \mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\tau} + \boldsymbol{\epsilon},
\end{equation}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛕</mi><annotation encoding="application/x-tex">\boldsymbol{\tau}</annotation></semantics></math>
is independent of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛜</mi><annotation encoding="application/x-tex">\boldsymbol{\epsilon}</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝛕</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>𝟎</mn></mrow><annotation encoding="application/x-tex">\text{E}(\boldsymbol{\tau}) = \mathbf{0}</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝛕</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msubsup><mi>σ</mi><mi>τ</mi><mn>2</mn></msubsup><mi>𝐑</mi></mrow><annotation encoding="application/x-tex">\text{Cov}(\boldsymbol{\tau}) = \sigma^2_\tau \mathbf{R}</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐑</mi><annotation encoding="application/x-tex">\mathbf{R}</annotation></semantics></math>
is a matrix that determines the spatial dependence structure in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐲</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math>
and depends on a range parameter,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ϕ</mi><annotation encoding="application/x-tex">\phi</annotation></semantics></math>.
We discuss
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐑</mi><annotation encoding="application/x-tex">\mathbf{R}</annotation></semantics></math>
in more detail shortly. The parameter
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>σ</mi><mi>τ</mi><mn>2</mn></msubsup><annotation encoding="application/x-tex">\sigma^2_\tau</annotation></semantics></math>
is called the spatially dependent random error variance or partial sill.
The parameter
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>σ</mi><mi>ϵ</mi><mn>2</mn></msubsup><annotation encoding="application/x-tex">\sigma^2_\epsilon</annotation></semantics></math>
is called the spatially independent random error variance or nugget.
These two variance parameters are henceforth more intuitively written as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>σ</mi><mrow><mi>d</mi><mi>e</mi></mrow><mn>2</mn></msubsup><annotation encoding="application/x-tex">\sigma^2_{de}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>σ</mi><mrow><mi>i</mi><mi>e</mi></mrow><mn>2</mn></msubsup><annotation encoding="application/x-tex">\sigma^2_{ie}</annotation></semantics></math>,
respectively. The covariance of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐲</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math>
is denoted
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝚺</mi><annotation encoding="application/x-tex">\boldsymbol{\Sigma}</annotation></semantics></math>
and given by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>σ</mi><mrow><mi>d</mi><mi>e</mi></mrow><mn>2</mn></msubsup><mi>𝐑</mi><mo>+</mo><msubsup><mi>σ</mi><mrow><mi>i</mi><mi>e</mi></mrow><mn>2</mn></msubsup><mi>𝐈</mi></mrow><annotation encoding="application/x-tex">\sigma^2_{de} \mathbf{R} + \sigma^2_{ie} \mathbf{I}</annotation></semantics></math>.
The parameters that compose this covariance are contained in the vector
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛉</mi><annotation encoding="application/x-tex">\boldsymbol{\theta}</annotation></semantics></math>,
which is called the covariance parameter vector.</p>
<p>The model
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐲</mi><mo>=</mo><mi>𝐗</mi><mi>𝛃</mi><mo>+</mo><mi>𝛕</mi><mo>+</mo><mi>𝛜</mi></mrow><annotation encoding="application/x-tex">\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\tau} + \boldsymbol{\epsilon}</annotation></semantics></math>
is called the spatial linear model. The spatial linear model applies to
both point-referenced and areal (i.e., lattice) data. Spatial data are
point-referenced when the elements in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐲</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math>
are observed at point-locations indexed by x-coordinates and
y-coordinates on a spatially continuous surface with an infinite number
of locations. The <code><a href="../reference/splm.html">splm()</a></code> function is used to fit spatial
linear models for point-referenced data (these are sometimes called
geostatistical models). One spatial covariance function available in
<code><a href="../reference/splm.html">splm()</a></code> is the exponential spatial covariance function,
which has an
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐑</mi><annotation encoding="application/x-tex">\mathbf{R}</annotation></semantics></math>
matrix given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐑</mi><mo>=</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mo>−</mo><mi>𝐌</mi><mi>/</mi><mi>ϕ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \mathbf{R} = \exp(-\mathbf{M} / \phi),
\end{equation*}</annotation></semantics></math><br>
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐌</mi><annotation encoding="application/x-tex">\mathbf{M}</annotation></semantics></math>
is a matrix of Euclidean distances among observations. Recall that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ϕ</mi><annotation encoding="application/x-tex">\phi</annotation></semantics></math>
is the range parameter, controlling the behavior of of the covariance
function as a function of distance. Parameterizations for
<code><a href="../reference/splm.html">splm()</a></code> spatial covariance types and their
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐑</mi><annotation encoding="application/x-tex">\mathbf{R}</annotation></semantics></math>
matrices can be seen by running <code><a href="../reference/splm.html">help("splm", "spmodel")</a></code> or
<code><a href="https://usepa.github.io/spmodel/articles/technical.html" class="external-link">vignette("technical", "spmodel")</a></code>. Some of these spatial
covariance types (e.g., Matérn) depend on an extra parameter beyond
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>σ</mi><mrow><mi>d</mi><mi>e</mi></mrow><mn>2</mn></msubsup><annotation encoding="application/x-tex">\sigma^2_{de}</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>σ</mi><mrow><mi>i</mi><mi>e</mi></mrow><mn>2</mn></msubsup><annotation encoding="application/x-tex">\sigma^2_{ie}</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ϕ</mi><annotation encoding="application/x-tex">\phi</annotation></semantics></math>.</p>
<p>Spatial data are areal when the elements in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐲</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math>
are observed as part of a finite network of polygons whose connections
are indexed by a neighborhood structure. For example, the polygons may
represent counties in a state that are neighbors if they share at least
one boundary. Areal data are often equivalently called lattice data
<span class="citation">(Cressie 1993)</span>. The <code><a href="../reference/spautor.html">spautor()</a></code>
function is used to fit spatial linear models for areal data (these are
sometimes called spatial autoregressive models). One spatial
autoregressive covariance function available in <code><a href="../reference/spautor.html">spautor()</a></code>
is the simultaneous autoregressive spatial covariance function, which
has an
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐑</mi><annotation encoding="application/x-tex">\mathbf{R}</annotation></semantics></math>
matrix given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐑</mi><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">[</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐈</mi><mo>−</mo><mi>ϕ</mi><mi>𝐖</mi><mo stretchy="true" form="postfix">)</mo></mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐈</mi><mo>−</mo><mi>ϕ</mi><mi>𝐖</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>⊤</mi></msup><mo stretchy="true" form="postfix">]</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \mathbf{R} = [(\mathbf{I} - \phi \mathbf{W})(\mathbf{I} - \phi \mathbf{W})^\top]^{-1},
\end{equation*}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐖</mi><annotation encoding="application/x-tex">\mathbf{W}</annotation></semantics></math>
is a weight matrix describing the neighborhood structure in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐲</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math>.
Parameterizations for <code><a href="../reference/spautor.html">spautor()</a></code> spatial covariance types
and their
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐑</mi><annotation encoding="application/x-tex">\mathbf{R}</annotation></semantics></math>
matrices can be seen by running <code><a href="../reference/spautor.html">help("spautor", "spmodel")</a></code>
or <code><a href="https://usepa.github.io/spmodel/articles/technical.html" class="external-link">vignette("technical", "spmodel")</a></code>.</p>
<p>One way to define
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐖</mi><annotation encoding="application/x-tex">\mathbf{W}</annotation></semantics></math>
is through queen contiguity <span class="citation">(Anselin, Syabri, and
Kho 2010)</span>. Two observations are queen contiguous if they share a
boundary. The
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mi>j</mi></mrow><annotation encoding="application/x-tex">ij</annotation></semantics></math>th
element of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐖</mi><annotation encoding="application/x-tex">\mathbf{W}</annotation></semantics></math>
is then one if observation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>
and observation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>
are queen contiguous and zero otherwise. Observations are not considered
neighbors with themselves, so each diagonal element of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐖</mi><annotation encoding="application/x-tex">\mathbf{W}</annotation></semantics></math>
is zero.</p>
<p>Sometimes each element in the weight matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐖</mi><annotation encoding="application/x-tex">\mathbf{W}</annotation></semantics></math>
is divided by its respective row sum. This is called
row-standardization. Row-standardizing
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐖</mi><annotation encoding="application/x-tex">\mathbf{W}</annotation></semantics></math>
has several benefits, which are discussed in detail by <span class="citation">Ver Hoef et al. (2018)</span>.</p>
<div class="section level3">
<h3 id="sec:aic-lm">
<code>AIC()</code> and <code>AICc()</code><a class="anchor" aria-label="anchor" href="#sec:aic-lm"></a>
</h3>
<p>The <code><a href="https://rdrr.io/r/stats/AIC.html" class="external-link">AIC()</a></code> and <code><a href="../reference/AIC.spmodel.html">AICc()</a></code> functions in
<code>spmodel</code> are defined for restricted maximum likelihood and
maximum likelihood estimation, which maximize a likelihood. The AIC and
AICc as defined by <span class="citation">Hoeting et al. (2006)</span>
are given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">AIC</mtext></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mo>−</mo><mn>2</mn><mo>ℓ</mo><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>𝚯</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mn>2</mn><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mo stretchy="true" form="prefix">|</mo><mover><mi>𝚯</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">|</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">AICc</mtext></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mo>−</mo><mn>2</mn><mo>ℓ</mo><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>𝚯</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mn>2</mn><mi>n</mi><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mo stretchy="true" form="prefix">|</mo><mover><mi>𝚯</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">|</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mi>/</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo>−</mo><mrow><mo stretchy="true" form="prefix">|</mo><mover><mi>𝚯</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">|</mo></mrow><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{equation*}\label{eq:sp_aic}
  \begin{split}
    \text{AIC} &amp; = -2\ell(\hat{\boldsymbol{\Theta}}) + 2(|\hat{\boldsymbol{\Theta}}|) \\
    \text{AICc} &amp; = -2\ell(\hat{\boldsymbol{\Theta}}) + 2n(|\hat{\boldsymbol{\Theta}}|) / (n - |\hat{\boldsymbol{\Theta}}| - 1),
  \end{split}
\end{equation*}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">|</mo><mover><mi>𝚯</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">|</mo></mrow><annotation encoding="application/x-tex">|\hat{\boldsymbol{\Theta}}|</annotation></semantics></math>
is the cardinality of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>𝚯</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat{\boldsymbol{\Theta}}</annotation></semantics></math>.
For restricted maximum likelihood,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>𝚯</mi><mo accent="true">̂</mo></mover><mo>≡</mo><mo stretchy="false" form="prefix">{</mo><mover><mi>𝛉</mi><mo accent="true">̂</mo></mover><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">\hat{\boldsymbol{\Theta}} \equiv \{\hat{\boldsymbol{\theta}}\}</annotation></semantics></math>.
For maximum likelihood,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>𝚯</mi><mo accent="true">̂</mo></mover><mo>≡</mo><mo stretchy="false" form="prefix">{</mo><mover><mi>𝛉</mi><mo accent="true">̂</mo></mover><mo>,</mo><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">\hat{\boldsymbol{\Theta}} \equiv \{\hat{\boldsymbol{\theta}}, \hat{\boldsymbol{\beta}}\}</annotation></semantics></math>
The discrepancy arises because restricted maximum likelihood integrates
the fixed effects out of the likelihood, and so the likelihood does not
depend on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛃</mi><annotation encoding="application/x-tex">\boldsymbol{\beta}</annotation></semantics></math>.</p>
<p>AIC comparisons between a model fit using restricted maximum
likelihood and a model fit using maximum likelihood are meaningless, as
the models are fit with different likelihoods. AIC comparisons between
models fit using restricted maximum likelihood are only valid when the
models have the same fixed effect structure. In contrast, AIC
comparisons between models fit using maximum likelihood are valid when
the models have different fixed effect structures.</p>
</div>
<div class="section level3">
<h3 id="sec:anova-lm">
<code>anova()</code><a class="anchor" aria-label="anchor" href="#sec:anova-lm"></a>
</h3>
<p>Test statistics from are formed using the general linear hypothesis
test. Let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐋</mi><annotation encoding="application/x-tex">\mathbf{L}</annotation></semantics></math>
be an
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mo>×</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">l \times p</annotation></semantics></math>
contrast matrix and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>l</mi><mn>0</mn></msub><annotation encoding="application/x-tex">l_0</annotation></semantics></math>
be an
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">l \times 1</annotation></semantics></math>
vector. The null hypothesis is that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐋</mi><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo>=</mo><msub><mi>l</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">\mathbf{L} \boldsymbol{\hat{\beta}} = l_0</annotation></semantics></math>
and the alternative hypothesis is that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐋</mi><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo>≠</mo><msub><mi>l</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">\mathbf{L} \boldsymbol{\hat{\beta}} \neq l_0</annotation></semantics></math>.
Usually,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>l</mi><mn>0</mn></msub><annotation encoding="application/x-tex">l_0</annotation></semantics></math>
is the zero vector (in <code>spmodel</code>, this is assumed). The test
statistic is denoted
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mi>h</mi><mi>i</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">Chi2</annotation></semantics></math>
and is given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mi>h</mi><mi>i</mi><mn>2</mn><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐋</mi><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo>−</mo><msub><mi>l</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>⊤</mi></msup><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐋</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐗</mi><mi>⊤</mi></msup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mi>𝐗</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>𝐋</mi><mi>⊤</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐋</mi><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo>−</mo><msub><mi>l</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\begin{equation*}\label{eq:glht}
  Chi2 = [(\mathbf{L} \boldsymbol{\hat{\beta}} - l_0)^\top(\mathbf{L} (\mathbf{X}^\top \mathbf{\hat{\Sigma}} \mathbf{X})^{-1} \mathbf{L}^\top)^{-1}(\mathbf{L} \boldsymbol{\hat{\beta}} - l_0)]
\end{equation*}</annotation></semantics></math> By default,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐋</mi><annotation encoding="application/x-tex">\mathbf{L}</annotation></semantics></math>
is chosen such that each variable in the data used to fit the model is
tested marginally (i.e., controlling for the other variables) against
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>l</mi><mn>0</mn></msub><mo>=</mo><mn>𝟎</mn></mrow><annotation encoding="application/x-tex">l_0 = \mathbf{0}</annotation></semantics></math>.
If this default is not desired, the and arguments can be used to pass
user-defined
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐋</mi><annotation encoding="application/x-tex">\mathbf{L}</annotation></semantics></math>
matrices to . They must be constructed in such a way that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>l</mi><mn>0</mn></msub><mo>=</mo><mn>𝟎</mn></mrow><annotation encoding="application/x-tex">l_0 = \mathbf{0}</annotation></semantics></math>.</p>
<p>It is notoriously difficult to determine appropriate p-values for
linear mixed models based on the general linear hypothesis test. lme4,
for example, does not report p-values by default. A few reasons why
obtaining p-values is so challenging:</p>
<ul>
<li>The first (and often most important) challenge is that when
estimating
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛉</mi><annotation encoding="application/x-tex">\boldsymbol{\theta}</annotation></semantics></math>
using a finite sample, it is usually not clear what the null
distribution of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mi>h</mi><mi>i</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">Chi2</annotation></semantics></math>
is. In certain cases such as ordinary least squares regression or some
experimental designs (e.g., blocked design, split plot design, etc.),
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mi>h</mi><mi>i</mi><mn>2</mn><mi>/</mi><mi>r</mi><mi>a</mi><mi>n</mi><mi>k</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐋</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">Chi2 / rank(\mathbf{L})</annotation></semantics></math>
is F-distributed with known numerator and denominator degrees of
freedom. But outside of these well-studied cases, no general results
exist.</li>
<li>The second challenge is that the standard error of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mi>h</mi><mi>i</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">Chi2</annotation></semantics></math>
does not account for the uncertainty in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>𝛉</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\boldsymbol{\hat{\theta}}</annotation></semantics></math>.
For some approaches to addressing this problem, see <span class="citation">Kackar and Harville (1984)</span>, <span class="citation">Prasad and Rao (1990)</span>, <span class="citation">Harville and Jeske (1992)</span>, and <span class="citation">Kenward and Roger (1997)</span>.</li>
<li>The third challenge is in determining denominator degrees of
freedom. Again, in some cases, these are known – but this is not true in
general. For some approaches to addressing this problem, see <span class="citation">Satterthwaite (1946)</span>, <span class="citation">Schluchter and Elashoff (1990)</span>, <span class="citation">Hrong-Tai Fai and Cornelius (1996)</span>, <span class="citation">Kenward and Roger (1997)</span>, <span class="citation">Littell et al. (2006)</span>, <span class="citation">Pinheiro and Bates (2006)</span>, and <span class="citation">Kenward and Roger (2009)</span>.</li>
</ul>
<p>For these reasons, <code>spmodel</code> uses an asymptotic (i.e.,
large sample) Chi-squared test when calculating p-values using
<code><a href="https://rdrr.io/r/stats/anova.html" class="external-link">anova()</a></code>. This approach addresses the three points above by
assuming that with a large enough sample size:</p>
<ul>
<li>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mi>h</mi><mi>i</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">Chi2</annotation></semantics></math>
is asymptotically Chi-squared (under certain conditions) with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mi>a</mi><mi>n</mi><mi>k</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐋</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">rank(\mathbf{L})</annotation></semantics></math>
degrees of freedom when the null hypothesis is true.</li>
<li>The uncertainty from estimating
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>𝛉</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\boldsymbol{\hat{\theta}}</annotation></semantics></math>
is small enough to be safely ignored.</li>
</ul>
<p>Because the approximation is asymptotic, degree of freedom
adjustments can be ignored (it is also worth noting that an F
distribution with infinite denominator degrees of freedom is a
Chi-squared distribution scaled by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mi>a</mi><mi>n</mi><mi>k</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐋</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">rank(\mathbf{L})</annotation></semantics></math>.
This asymptotic approximation implies these p-values are likely
unreliable with small samples.</p>
<p>Note that when comparing full and reduced models, the general linear
hypothesis test is analogous to an extra sum of (whitened) squares
approach <span class="citation">(Myers et al. 2012)</span>.</p>
<p>A second approach to determining p-values is a likelihood ratio test.
Let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>ℓ</mo><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>𝚯</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\ell(\boldsymbol{\hat{\Theta}})</annotation></semantics></math>
be the log-likelihood for some full model and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>ℓ</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>𝚯</mi><mo accent="true">̂</mo></mover><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\ell(\boldsymbol{\hat{\Theta}}_0)</annotation></semantics></math>
be the log-likelihood for some reduced model. For the likelihood ratio
test to be valid, the reduced model must be nested in the full model,
which means that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>ℓ</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>𝚯</mi><mo accent="true">̂</mo></mover><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\ell(\boldsymbol{\hat{\Theta}}_0)</annotation></semantics></math>
is obtained by fixing some parameters in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝚯</mi><annotation encoding="application/x-tex">\boldsymbol{\Theta}</annotation></semantics></math>.
When the likelihood ratio test is valid,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>X</mi><mn>2</mn></msup><mo>=</mo><mn>2</mn><mo>ℓ</mo><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>𝚯</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mn>2</mn><mo>ℓ</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>𝚯</mi><mo accent="true">̂</mo></mover><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X^2 = 2\ell(\boldsymbol{\hat{\Theta}}) - 2\ell(\boldsymbol{\hat{\Theta}}_0)</annotation></semantics></math>
is asymptotically Chi-squared with degrees of freedom equal to the
difference in estimated parameters between the full and reduced
models.</p>
<p>For restricted maximum likelihood estimation, likelihood ratio tests
can only be used to compare nested models with the same explanatory
variables. To use likelihood ratio tests for comparing different
explanatory variable structures, parameters must be estimated using
maximum likelihood estimation. When using likelihood ratio tests to
assess the importance of parameters on the boundary of a parameter space
(e.g., a variance parameter being zero), p-values tend to be too large
<span class="citation">(Self and Liang 1987; Stram and Lee 1994; Goldman
and Whelan 2000; Pinheiro and Bates 2006)</span>.</p>
</div>
<div class="section level3">
<h3 id="sec:bic-lm">
<code>BIC()</code><a class="anchor" aria-label="anchor" href="#sec:bic-lm"></a>
</h3>
<p>The <code><a href="https://rdrr.io/r/stats/AIC.html" class="external-link">BIC()</a></code> function in <code>spmodel</code> is defined
for restricted maximum likelihood and maximum likelihood estimation,
which maximize a likelihood. The BIC as defined by <span class="citation">Schwarz (1978)</span> is given by <span class="math display">$$\begin{equation*}\label{eq:sp_aic}
    \text{BIC} &amp; = -2\ell(\hat{\boldsymbol{\Theta}}) +
\ln(n)(|\hat{\boldsymbol{\Theta}}|)  \\
\end{equation*}$$</span> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
is the sample size and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">|</mo><mover><mi>𝚯</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">|</mo></mrow><annotation encoding="application/x-tex">|\hat{\boldsymbol{\Theta}}|</annotation></semantics></math>
is the cardinality of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>𝚯</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat{\boldsymbol{\Theta}}</annotation></semantics></math>.
For restricted maximum likelihood,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>𝚯</mi><mo accent="true">̂</mo></mover><mo>≡</mo><mo stretchy="false" form="prefix">{</mo><mover><mi>𝛉</mi><mo accent="true">̂</mo></mover><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">\hat{\boldsymbol{\Theta}} \equiv \{\hat{\boldsymbol{\theta}}\}</annotation></semantics></math>.
For maximum likelihood,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>𝚯</mi><mo accent="true">̂</mo></mover><mo>≡</mo><mo stretchy="false" form="prefix">{</mo><mover><mi>𝛉</mi><mo accent="true">̂</mo></mover><mo>,</mo><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">\hat{\boldsymbol{\Theta}} \equiv \{\hat{\boldsymbol{\theta}}, \hat{\boldsymbol{\beta}}\}</annotation></semantics></math>
The discrepancy arises because restricted maximum likelihood integrates
the fixed effects out of the likelihood, and so the likelihood does not
depend on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛃</mi><annotation encoding="application/x-tex">\boldsymbol{\beta}</annotation></semantics></math>.</p>
<p>BIC comparisons between a model fit using restricted maximum
likelihood and a model fit using maximum likelihood are meaningless, as
the models are fit with different likelihoods. BIC comparisons between
models fit using restricted maximum likelihood are only valid when the
models have the same fixed effect structure. In contrast, BIC
comparisons between models fit using maximum likelihood are valid when
the models have different fixed effect structures. While BIC was derived
by <span class="citation">Schwarz (1978)</span> for independent data,
<span class="citation">Zimmerman and Ver Hoef (2024)</span> show it can
be useful for spatially-dependent data as well.</p>
</div>
<div class="section level3">
<h3 id="sec:coef-lm">
<code>coef()</code><a class="anchor" aria-label="anchor" href="#sec:coef-lm"></a>
</h3>
<p><code><a href="https://rdrr.io/r/stats/coef.html" class="external-link">coef()</a></code> returns relevant coefficients based on the
<code>type</code> argument. When <code>type = "fixed"</code> (the
default), <code><a href="https://rdrr.io/r/stats/coef.html" class="external-link">coef()</a></code> returns
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐗</mi><mi>⊤</mi></msup><msup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐗</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>𝐗</mi><mi>⊤</mi></msup><msup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐲</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
\hat{\boldsymbol{\beta}} = (\mathbf{X}^\top \hat{\boldsymbol{\Sigma}}^{-1} \mathbf{X})^{-1}\mathbf{X}^\top \hat{\boldsymbol{\Sigma}}^{-1} \mathbf{y} .
\end{equation*}</annotation></semantics></math> If the estimation method
is restricted maximum likelihood or maximum likelihood,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat{\boldsymbol{\beta}}</annotation></semantics></math>
is known as the restricted maximum likelihood or maximum likelihood
estimator of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛃</mi><annotation encoding="application/x-tex">\boldsymbol{\beta}</annotation></semantics></math>.
If the estimation method is semivariogram weighted least squares or
semivariogram composite likelihood,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat{\boldsymbol{\beta}}</annotation></semantics></math>
is known as the empirical generalized least squares estimator of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛃</mi><annotation encoding="application/x-tex">\boldsymbol{\beta}</annotation></semantics></math>.
When <code>type = "spcov"</code>, the estimated spatial covariance
parameters are returned (available for all estimation methods). When
<code>type = "randcov"</code>, the estimated random effect variance
parameters are returned (available for restricted maximum likelihood and
maximum likelihood estimation).</p>
</div>
<div class="section level3">
<h3 id="sec:confint-lm">
<code>confint()</code><a class="anchor" aria-label="anchor" href="#sec:confint-lm"></a>
</h3>
<p><code><a href="https://rdrr.io/r/stats/confint.html" class="external-link">confint()</a></code> returns confidence intervals for estimated
parameters. Currently, <code><a href="https://rdrr.io/r/stats/confint.html" class="external-link">confint()</a></code> only returns confidence
intervals for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛃</mi><annotation encoding="application/x-tex">\boldsymbol{\beta}</annotation></semantics></math>.
The
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>α</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(1 - \alpha)</annotation></semantics></math>%
confidence interval for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>β</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\beta_i</annotation></semantics></math>
is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover><mi>β</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><mo>±</mo><msup><mi>z</mi><mo>*</mo></msup><msqrt><msubsup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐗</mi><mi>⊤</mi></msup><msup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐗</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>i</mi><mo>,</mo><mi>i</mi></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup></msqrt><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
\hat{\beta}_i \pm z^* \sqrt{(\mathbf{X}^\top \hat{\boldsymbol{\Sigma}}^{-1} \mathbf{X})^{-1}_{i, i}},
\end{equation*}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐗</mi><mi>⊤</mi></msup><msup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐗</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>i</mi><mo>,</mo><mi>i</mi></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><annotation encoding="application/x-tex">(\mathbf{X}^\top \hat{\boldsymbol{\Sigma}}^{-1} \mathbf{X})^{-1}_{i, i}</annotation></semantics></math>
is the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>th
diagonal element in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐗</mi><mi>⊤</mi></msup><msup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐗</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">(\mathbf{X}^\top \hat{\boldsymbol{\Sigma}}^{-1} \mathbf{X})^{-1}</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Φ</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>z</mi><mo>*</mo></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>−</mo><mi>α</mi><mi>/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">\Phi(z^*) = 1 - \alpha / 2</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Φ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo>⋅</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Phi(\cdot)</annotation></semantics></math>
is the standard normal (Gaussian) cumulative distribution function, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>1</mn><mo>−</mo></mrow><annotation encoding="application/x-tex">\alpha = 1 -</annotation></semantics></math><code>level</code>, where <code>level</code> is an argument to
<code><a href="https://rdrr.io/r/stats/confint.html" class="external-link">confint()</a></code>. The default for <code>level</code> is 0.95,
which corresponds to a
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>z</mi><mo>*</mo></msup><annotation encoding="application/x-tex">z^*</annotation></semantics></math>
of approximately 1.96.</p>
</div>
<div class="section level3">
<h3 id="sec:cooks-lm">
<code>cooks.distance()</code><a class="anchor" aria-label="anchor" href="#sec:cooks-lm"></a>
</h3>
<p>Cook’s distance measures the influence of an observation <span class="citation">(Cook 1979; Cook and Weisberg 1982)</span>. An
influential observation has a large impact on the model fit. The vector
of Cook’s distances for the spatial linear model is given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><msubsup><mi>𝐞</mi><mi>p</mi><mn>2</mn></msubsup><mi>p</mi></mfrac><mo>⊙</mo><mi>d</mi><mi>i</mi><mi>a</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐇</mi><mi>s</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>⊙</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>−</mo><mi>d</mi><mi>i</mi><mi>a</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐇</mi><mi>s</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation} \label{eq:cooksd}
\frac{\mathbf{e}_p^2}{p} \odot diag(\mathbf{H}_s) \odot \frac{1}{1 - diag(\mathbf{H}_s)},
\end{equation}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐞</mi><mi>p</mi></msub><annotation encoding="application/x-tex">\mathbf{e}_p</annotation></semantics></math>
are the Pearson residuals and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>i</mi><mi>a</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐇</mi><mi>s</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">diag(\mathbf{H}_s)</annotation></semantics></math>
is the diagonal of the spatial hat matrix,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐇</mi><mi>s</mi></msub><mo>≡</mo><msup><mi>𝐗</mi><mo>*</mo></msup><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐗</mi><mrow><mo>*</mo><mi>⊤</mi></mrow></msup><msup><mi>𝐗</mi><mo>*</mo></msup><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>𝐗</mi><mrow><mo>*</mo><mi>⊤</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{H}_s \equiv \mathbf{X}^* (\mathbf{X}^{* \top} \mathbf{X}^*)^{-1} \mathbf{X}^{* \top}</annotation></semantics></math><span class="citation">(Montgomery, Peck, and Vining 2021)</span>, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mo>⊙</mo><annotation encoding="application/x-tex">\odot</annotation></semantics></math>
denotes the Hadmard (element-wise) product. The larger the Cook’s
distance, the larger the influence.</p>
<p>To better understand the previous form, recall that the the
non-spatial linear model
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐲</mi><mo>=</mo><mi>𝐗</mi><mi>𝛃</mi><mo>+</mo><mi>𝛜</mi></mrow><annotation encoding="application/x-tex">\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}</annotation></semantics></math>
assumes elements of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛜</mi><annotation encoding="application/x-tex">\boldsymbol{\epsilon}</annotation></semantics></math>
are independent and identically distributed (iid) with constant
variance. In this context the vector of non-spatial Cook’s distances is
given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><msubsup><mi>𝐞</mi><mi>p</mi><mn>2</mn></msubsup><mi>p</mi></mfrac><mo>⊙</mo><mi>d</mi><mi>i</mi><mi>a</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐇</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>⊙</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>−</mo><mi>d</mi><mi>i</mi><mi>a</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐇</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
\frac{\mathbf{e}_p^2}{p} \odot diag(\mathbf{H}) \odot \frac{1}{1 - diag(\mathbf{H})},
\end{equation*}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>i</mi><mi>a</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐇</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">diag(\mathbf{H})</annotation></semantics></math>
is the diagonal of the non-spatial hat matrix,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐇</mi><mo>≡</mo><mi>𝐗</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐗</mi><mi>⊤</mi></msup><mi>𝐗</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>𝐗</mi><mi>⊤</mi></msup></mrow><annotation encoding="application/x-tex">\mathbf{H} \equiv \mathbf{X} (\mathbf{X}^{\top} \mathbf{X})^{-1} \mathbf{X}^{\top}</annotation></semantics></math>.
When the elements of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛜</mi><annotation encoding="application/x-tex">\boldsymbol{\epsilon}</annotation></semantics></math>
are not iid or do not have constant variance or both, the spatial Cook’s
distance cannot be calculated using
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐇</mi><annotation encoding="application/x-tex">\mathbf{H}</annotation></semantics></math>.
First the linear model must be whitened according to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>𝐲</mi><mo>*</mo></msup><mo>=</mo><msup><mi>𝐗</mi><mo>*</mo></msup><mi>𝛃</mi><mo>+</mo><msup><mi>𝛜</mi><mo>*</mo></msup></mrow><annotation encoding="application/x-tex">\mathbf{y}^* = \mathbf{X}^* \boldsymbol{\beta} + \boldsymbol{\epsilon}^*</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>𝛜</mi><mo>*</mo></msup><annotation encoding="application/x-tex">\boldsymbol{\epsilon}^*</annotation></semantics></math>
is the whitened version of the sum of all random errors in the model.
Then the spatial Cook’s distance follows using
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>𝐗</mi><mo>*</mo></msup><annotation encoding="application/x-tex">\mathbf{X}^*</annotation></semantics></math>,
the whitened version of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐗</mi><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics></math>.</p>
</div>
<div class="section level3">
<h3 id="sec:deviance-lm">
<code>deviance()</code><a class="anchor" aria-label="anchor" href="#sec:deviance-lm"></a>
</h3>
<p>The deviance of a fitted model is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝒟</mi><mi>𝚯</mi></msub><mo>=</mo><mn>2</mn><mo>ℓ</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝚯</mi><mi>s</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mn>2</mn><mo>ℓ</mo><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>𝚯</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
\mathcal{D}_{\boldsymbol{\Theta}} = 2\ell(\boldsymbol{\Theta}_s) - 2\ell(\boldsymbol{\hat{\Theta}}),
\end{equation*}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>ℓ</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝚯</mi><mi>s</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\ell(\boldsymbol{\Theta}_s)</annotation></semantics></math>
is the log-likelihood of a “saturated” model that fits every observation
perfectly. For normal (Gaussian) random errors,
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝒟</mi><mi>𝚯</mi></msub><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐲</mi><mo>−</mo><mi>𝐗</mi><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mi>⊤</mi></msup><msup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐲</mi><mo>−</mo><mi>𝐗</mi><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\begin{equation*}
\mathcal{D}_{\boldsymbol{\Theta}} = (\mathbf{y} - \mathbf{X} \hat{\boldsymbol{\beta}})^\top \hat{\boldsymbol{\Sigma}}^{-1} (\mathbf{y} - \mathbf{X} \hat{\boldsymbol{\beta}})
\end{equation*}</annotation></semantics></math></p>
</div>
<div class="section level3">
<h3 id="sec:fitted-lm">
<code>fitted()</code><a class="anchor" aria-label="anchor" href="#sec:fitted-lm"></a>
</h3>
<p>Fitted values can be obtained for the response, spatial random
errors, and random effects. The fitted values for the response
(<code>type = "response"</code>), denoted
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>𝐲</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\mathbf{\hat{y}}</annotation></semantics></math>,
are given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>𝐲</mi><mo accent="true">̂</mo></mover><mo>=</mo><mi>𝐗</mi><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}\label{eq:fit_resp}
  \mathbf{\hat{y}} = \mathbf{X} \boldsymbol{\hat{\beta}} .
\end{equation*}</annotation></semantics></math> They are the estimated
mean response given the set of explanatory variables for each
observation.</p>
<p>Fitted values for spatial random errors (<code>type = "spcov"</code>)
and random effects (<code>type = "randcov"</code>) are linked to best
linear unbiased predictors from linear mixed model theory. Consider the
standard random effects parameterization
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐲</mi><mo>=</mo><mi>𝐗</mi><mi>𝛃</mi><mo>+</mo><mi>𝐙</mi><mi>𝐮</mi><mo>+</mo><mi>𝛜</mi><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \mathbf{Z} \mathbf{u} + \boldsymbol{\epsilon},
\end{equation*}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐙</mi><annotation encoding="application/x-tex">\mathbf{Z}</annotation></semantics></math>
denotes the random effects design matrix,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐮</mi><annotation encoding="application/x-tex">\mathbf{u}</annotation></semantics></math>
denotes the random effects, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛜</mi><annotation encoding="application/x-tex">\boldsymbol{\epsilon}</annotation></semantics></math>
denotes independent random error. <span class="citation">Henderson
(1975)</span> states that the best linear unbiased predictor (BLUP) of a
single random effect vector
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐮</mi><annotation encoding="application/x-tex">\mathbf{u}</annotation></semantics></math>,
denoted
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>𝐮</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\mathbf{\hat{u}}</annotation></semantics></math>,
is given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>𝐮</mi><mo accent="true">̂</mo></mover><mo>=</mo><msubsup><mi>σ</mi><mi>u</mi><mn>2</mn></msubsup><msup><mi>𝐙</mi><mi>⊤</mi></msup><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐲</mi><mo>−</mo><mi>𝐗</mi><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation}\label{eq:blup_mm}
  \mathbf{\hat{u}} = \sigma^2_u \mathbf{Z}^\top \mathbf{\Sigma}^{-1}(\mathbf{y} - \mathbf{X} \boldsymbol{\hat{\beta}}),
\end{equation}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>σ</mi><mi>u</mi><mn>2</mn></msubsup><annotation encoding="application/x-tex">\sigma^2_u</annotation></semantics></math>
is the variance of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐮</mi><annotation encoding="application/x-tex">\mathbf{u}</annotation></semantics></math>.</p>
<p><span class="citation">Searle, Casella, and McCulloch (2009)</span>
generalize this idea by showing that for a random vector
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛂</mi><annotation encoding="application/x-tex">\boldsymbol{\alpha}</annotation></semantics></math>
in a linear model, the best linear unbiased predictor (based on the
response,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐲</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math>)
of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛂</mi><annotation encoding="application/x-tex">\boldsymbol{\alpha}</annotation></semantics></math>,
denoted
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>𝛂</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\boldsymbol{\hat{\alpha}}</annotation></semantics></math>,
is given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>𝛂</mi><mo accent="true">̂</mo></mover><mo>=</mo><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝛂</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><msub><mi>𝚺</mi><mi>α</mi></msub><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐲</mi><mo>−</mo><mi>𝐗</mi><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation}\label{eq:blup_gen}
  \boldsymbol{\hat{\alpha}} = \text{E}(\boldsymbol{\alpha}) + \boldsymbol{\Sigma}_\alpha \boldsymbol{\Sigma}^{-1}(\mathbf{y} - \mathbf{X} \boldsymbol{\hat{\beta}}),
\end{equation}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝚺</mi><mi>α</mi></msub><mo>=</mo><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝛂</mi><mo>,</mo><mi>𝐲</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\boldsymbol{\Sigma}_\alpha = \text{Cov}(\boldsymbol{\alpha}, \mathbf{y})</annotation></semantics></math>.
Evaluating this equation at the plug-in (empirical) estimates of the
covariance parameters yields the empirical best linear unbiased
predictor (EBLUP) of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛂</mi><annotation encoding="application/x-tex">\boldsymbol{\alpha}</annotation></semantics></math>.</p>
<p>Recall that the spatial linear model with random effects is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐲</mi><mo>=</mo><mi>𝐗</mi><mi>𝛃</mi><mo>+</mo><mi>𝐙</mi><mi>𝐮</mi><mo>+</mo><mi>𝛕</mi><mo>+</mo><mi>𝛜</mi><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \mathbf{Z} \mathbf{u} + \boldsymbol{\tau} + \boldsymbol{\epsilon},
\end{equation*}</annotation></semantics></math> Building from previous
results, we can find BLUPs for each random term in the spatial linear
model
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐮</mi><annotation encoding="application/x-tex">\mathbf{u}</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛕</mi><annotation encoding="application/x-tex">\boldsymbol{\tau}</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛜</mi><annotation encoding="application/x-tex">\boldsymbol{\epsilon}</annotation></semantics></math>).
For example, the BLUP of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐮</mi><annotation encoding="application/x-tex">\mathbf{u}</annotation></semantics></math>
is found by noting that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐮</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>𝟎</mn></mrow><annotation encoding="application/x-tex">\text{E}(\mathbf{u}) = \mathbf{0}</annotation></semantics></math>
and
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝚺</mi><mi>u</mi></msub><mo>=</mo><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐮</mi><mo>,</mo><mi>𝐲</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐮</mi><mo>,</mo><mi>𝐗</mi><mi>𝛃</mi><mo>+</mo><mi>𝐙</mi><mi>𝐮</mi><mo>+</mo><mi>𝛕</mi><mo>+</mo><mi>𝛜</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐮</mi><mo>,</mo><mi>𝐙</mi><mi>𝐮</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐮</mi><mo>,</mo><mi>𝐮</mi><mo stretchy="true" form="postfix">)</mo></mrow><msup><mi>𝐙</mi><mi>⊤</mi></msup><mo>=</mo><msubsup><mi>σ</mi><mi>u</mi><mn>2</mn></msubsup><msup><mi>𝐙</mi><mi>⊤</mi></msup><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \mathbf{\Sigma}_u = \text{Cov}(\mathbf{u}, \mathbf{y}) = \text{Cov}(\mathbf{u}, \mathbf{X} \boldsymbol{\beta} + \mathbf{Z} \mathbf{u} + \boldsymbol{\tau} + \boldsymbol{\epsilon}) = \text{Cov}(\mathbf{u}, \mathbf{Z}\mathbf{u}) = \text{Cov}(\mathbf{u}, \mathbf{u})\mathbf{Z}^\top = \sigma^2_u \mathbf{Z}^\top,
\end{equation*}</annotation></semantics></math> where the result follows
because the random terms in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐲</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math>
are independent and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐮</mi><mo>,</mo><mi>𝐮</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msubsup><mi>σ</mi><mi>u</mi><mn>2</mn></msubsup><mi>𝐈</mi></mrow><annotation encoding="application/x-tex">\text{Cov}(\mathbf{u}, \mathbf{u}) = \sigma^2_u \mathbf{I}</annotation></semantics></math>.
Then it follows that
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>𝐮</mi><mo accent="true">̂</mo></mover><mo>=</mo><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐮</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><msub><mi>𝚺</mi><mi>u</mi></msub><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐲</mi><mo>−</mo><mi>𝐗</mi><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msubsup><mi>σ</mi><mi>u</mi><mn>2</mn></msubsup><msup><mi>𝐙</mi><mi>⊤</mi></msup><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐲</mi><mo>−</mo><mi>𝐗</mi><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \hat{\mathbf{u}} = \text{E}(\mathbf{u}) + \boldsymbol{\Sigma}_u \boldsymbol{\Sigma}^{-1}(\mathbf{y} - \mathbf{X} \boldsymbol{\hat{\beta}}) = \sigma^2_u \mathbf{Z}^\top \boldsymbol{\Sigma}^{-1}(\mathbf{y} - \mathbf{X} \boldsymbol{\hat{\beta}}),
\end{equation*}</annotation></semantics></math> which matches the
previous form of the BLUP. Similarly, the BLUP of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛕</mi><annotation encoding="application/x-tex">\boldsymbol{\tau}</annotation></semantics></math>
is found by noting that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝛕</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>𝟎</mn></mrow><annotation encoding="application/x-tex">\text{E}(\boldsymbol{\tau}) = \mathbf{0}</annotation></semantics></math>
and
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝚺</mi><mrow><mi>d</mi><mi>e</mi></mrow></msub><mo>=</mo><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝛕</mi><mo>,</mo><mi>𝐲</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝛕</mi><mo>,</mo><mi>𝐗</mi><mi>𝛃</mi><mo>+</mo><mi>𝐙</mi><mi>𝐮</mi><mo>+</mo><mi>𝛕</mi><mo>+</mo><mi>𝛜</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝛕</mi><mo>,</mo><mi>𝛕</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msubsup><mi>σ</mi><mrow><mi>d</mi><mi>e</mi></mrow><mn>2</mn></msubsup><mi>𝐑</mi><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \mathbf{\Sigma}_{de} = \text{Cov}(\boldsymbol{\tau}, \mathbf{y}) = \text{Cov}(\boldsymbol{\tau}, \mathbf{X} \boldsymbol{\beta} + \mathbf{Z} \mathbf{u} + \boldsymbol{\tau} + \boldsymbol{\epsilon}) = \text{Cov}(\boldsymbol{\tau}, \boldsymbol{\tau}) = \sigma^2_{de} \mathbf{R},
\end{equation*}</annotation></semantics></math> where the result follows
because the random terms in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐲</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math>
are independent and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝛕</mi><mo>,</mo><mi>𝛕</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msubsup><mi>σ</mi><mrow><mi>d</mi><mi>e</mi></mrow><mn>2</mn></msubsup><mi>𝐑</mi></mrow><annotation encoding="application/x-tex">\text{Cov}(\boldsymbol{\tau}, \boldsymbol{\tau}) = \sigma^2_{de} \mathbf{R}</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>σ</mi><mrow><mi>d</mi><mi>e</mi></mrow><mn>2</mn></msubsup><annotation encoding="application/x-tex">\sigma^2_{de}</annotation></semantics></math>
is the variance of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛕</mi><annotation encoding="application/x-tex">\boldsymbol{\tau}</annotation></semantics></math>.
Then it follows that
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>𝛕</mi><mo accent="true">̂</mo></mover><mo>=</mo><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝛕</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><msub><mi>𝚺</mi><mrow><mi>d</mi><mi>e</mi></mrow></msub><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐲</mi><mo>−</mo><mi>𝐗</mi><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msubsup><mi>σ</mi><mrow><mi>d</mi><mi>e</mi></mrow><mn>2</mn></msubsup><mi>𝐑</mi><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐲</mi><mo>−</mo><mi>𝐗</mi><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation}\label{eq:blup_sp}
  \hat{\boldsymbol{\tau}} = \text{E}(\boldsymbol{\tau}) + \boldsymbol{\Sigma}_{de} \boldsymbol{\Sigma}^{-1}(\mathbf{y} - \mathbf{X} \boldsymbol{\hat{\beta}}) = \sigma^2_{de} \mathbf{R} \boldsymbol{\Sigma}^{-1}(\mathbf{y} - \mathbf{X} \boldsymbol{\hat{\beta}}).
\end{equation}</annotation></semantics></math> Fitted values for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛜</mi><annotation encoding="application/x-tex">\boldsymbol{\epsilon}</annotation></semantics></math>
are obtained using similar arguments. Evaluating these equations at the
plug-in (empirical) estimates of the covariance parameters yields
EBLUPs.</p>
<p>When partition factors are used, the covariance matrix of all random
effects (spatial and non-spatial) can be viewed as the interaction
between the non-partitioned covariance matrix and the partition matrix,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐏</mi><annotation encoding="application/x-tex">\mathbf{P}</annotation></semantics></math>.
The
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mi>j</mi></mrow><annotation encoding="application/x-tex">ij</annotation></semantics></math>th
entry in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐏</mi><annotation encoding="application/x-tex">\mathbf{P}</annotation></semantics></math>
equals one if observation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>
and observation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>
share the same level of the partition factor and zero otherwise. For
spatial random effects, an adjustment is straightforward, as each column
in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝚺</mi><mrow><mi>d</mi><mi>e</mi></mrow></msub><annotation encoding="application/x-tex">\boldsymbol{\Sigma}_{de}</annotation></semantics></math>
corresponds to a distinct spatial random effect. Thus with partition
factors,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>𝚺</mi><mrow><mi>d</mi><mi>e</mi></mrow><mo>*</mo></msubsup><mo>=</mo><msub><mi>𝚺</mi><mrow><mi>d</mi><mi>e</mi></mrow></msub><mo>⊙</mo><mi>𝐏</mi><mo>=</mo><msubsup><mi>σ</mi><mrow><mi>d</mi><mi>e</mi></mrow><mn>2</mn></msubsup><mi>𝐑</mi><mo>⊙</mo><mi>𝐏</mi></mrow><annotation encoding="application/x-tex">\boldsymbol{\Sigma}_{de}^* = \boldsymbol{\Sigma}_{de} \odot \mathbf{P} = \sigma^2_{de} \mathbf{R} \odot \mathbf{P}</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mo>⊙</mo><annotation encoding="application/x-tex">\odot</annotation></semantics></math>
denotes the Hadmart (element-wise) product, is used instead of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝚺</mi><mrow><mi>d</mi><mi>e</mi></mrow></msub><annotation encoding="application/x-tex">\boldsymbol{\Sigma}_{de}</annotation></semantics></math>.
Note that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝚺</mi><mrow><mi>i</mi><mi>e</mi></mrow></msub><annotation encoding="application/x-tex">\boldsymbol{\Sigma}_{ie}</annotation></semantics></math>
is unchanged as it is proportional to the identity matrix. For
non-spatial random effects, however, the situation is more complicated.
Applying the BLUP formula directly yields BLUPs of random effects
corresponding to the interaction between random effect levels and
partition levels. Thus a logical approach is to average the non-zero
BLUPs for each random effect level across partition levels, yielding a
prediction for the random effect level. This does not imply, however,
that these estimates are BLUPs of the random effect.</p>
<p>For big data without partition factors, the local indexes act as
partition factors. That is, the BLUPs correspond to random effects
interacted with each local index. For big data with partition factors,
an adjusted partition factor is created as the interaction between each
local index and the partition factor. Then this adjusted partition
factor is applied to yield
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>𝛂</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat{\boldsymbol{\alpha}}</annotation></semantics></math>.</p>
</div>
<div class="section level3">
<h3 id="sec:hatvalues-lm">
<code>hatvalues()</code><a class="anchor" aria-label="anchor" href="#sec:hatvalues-lm"></a>
</h3>
<p>Hat values measure the leverage of an observation. An observation has
high leverage if its combination of explanatory variables is atypical
(far from the mean explanatory vector). The spatial leverage (hat)
matrix is given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐇</mi><mi>s</mi></msub><mo>=</mo><msup><mi>𝐗</mi><mo>*</mo></msup><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐗</mi><mrow><mo>*</mo><mi>⊤</mi></mrow></msup><msup><mi>𝐗</mi><mo>*</mo></msup><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>𝐗</mi><mrow><mo>*</mo><mi>⊤</mi></mrow></msup><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation}
\label{eq:leverage}
 \mathbf{H}_s = \mathbf{X}^* (\mathbf{X}^{* \top} \mathbf{X}^*)^{-1} \mathbf{X}^{* \top}.
\end{equation}</annotation></semantics></math> The diagonal of this
matrix yields the leverage (hat) values for each observation <span class="citation">(Montgomery, Peck, and Vining 2021)</span>. The larger
the hat value, the larger the leverage.</p>
<p>To better understand the previous form, recall that the the
non-spatial linear model
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐲</mi><mo>=</mo><mi>𝐗</mi><mi>𝛃</mi><mo>+</mo><mi>𝛜</mi></mrow><annotation encoding="application/x-tex">\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}</annotation></semantics></math>
assumes elements of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛜</mi><annotation encoding="application/x-tex">\boldsymbol{\epsilon}</annotation></semantics></math>
are independent and identically distributed (iid) with constant
variance. In this context, the leverage (hat) matrix is given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐇</mi><mo>≡</mo><mi>𝐗</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐗</mi><mi>⊤</mi></msup><mi>𝐗</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>𝐗</mi><mi>⊤</mi></msup><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
\mathbf{H} \equiv \mathbf{X} (\mathbf{X}^{\top} \mathbf{X})^{-1} \mathbf{X}^{\top},
\end{equation*}</annotation></semantics></math> When the elements of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛜</mi><annotation encoding="application/x-tex">\boldsymbol{\epsilon}</annotation></semantics></math>
are not iid or do not have constant variance or both, the spatial
leverage (hat) matrix is not
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐇</mi><annotation encoding="application/x-tex">\mathbf{H}</annotation></semantics></math>.
First the linear model must be whitened according to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>𝐲</mi><mo>*</mo></msup><mo>=</mo><msup><mi>𝐗</mi><mo>*</mo></msup><mi>𝛃</mi><mo>+</mo><msup><mi>𝛜</mi><mo>*</mo></msup></mrow><annotation encoding="application/x-tex">\mathbf{y}^* = \mathbf{X}^* \boldsymbol{\beta} + \boldsymbol{\epsilon}^*</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>𝛜</mi><mo>*</mo></msup><annotation encoding="application/x-tex">\boldsymbol{\epsilon}^*</annotation></semantics></math>
is the whitened version of the sum of all random errors in the model.
Then the spatial leverage (hat) matrix follows using
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>𝐗</mi><mo>*</mo></msup><annotation encoding="application/x-tex">\mathbf{X}^*</annotation></semantics></math>,
the whitened version of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐗</mi><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics></math>.</p>
</div>
<div class="section level3">
<h3 id="sec:loglik-lm">
<code>logLik()</code><a class="anchor" aria-label="anchor" href="#sec:loglik-lm"></a>
</h3>
<p>The log-likelihood is given by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>ℓ</mo><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>𝚯</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\ell(\boldsymbol{\hat{\Theta}})</annotation></semantics></math>.</p>
</div>
<div class="section level3">
<h3 id="sec:loocv-lm">
<code>loocv()</code><a class="anchor" aria-label="anchor" href="#sec:loocv-lm"></a>
</h3>
<p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>-fold
cross validation is a useful tool for evaluating model fits using
“hold-out” data. The data are split into
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>
sets. One-by-one, one of the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>
sets is held out, the model is fit to the remaining
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">k - 1</annotation></semantics></math>
sets, and predictions at each observation in the hold-out set are
compared to their true values. The closer the predictions are to the
true observations, the better the model fit. A special case where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">k = n</annotation></semantics></math>
is known as leave-one-out cross validation (loocv), as each observation
is left out one-by-one. Computationally efficient solutions exist for
leave-one-out cross validation in the non-spatial linear model (with
iid, constant variance errors). Outside of this case, however, fitting
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
separate models can be computationally infeasible. <code><a href="../reference/loocv.html">loocv()</a></code>
makes a compromise that balances an approximation to the true solution
with computational feasibility. First
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛉</mi><annotation encoding="application/x-tex">\boldsymbol{\theta}</annotation></semantics></math>
is estimated using all of the data. Then for each of the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
model fits, <code><a href="../reference/loocv.html">loocv()</a></code> does not re-estimate
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛉</mi><annotation encoding="application/x-tex">\boldsymbol{\theta}</annotation></semantics></math>
but does re-estimate
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛃</mi><annotation encoding="application/x-tex">\boldsymbol{\beta}</annotation></semantics></math>.
This approach relies on the assumption that the covariance parameter
estimates obtained using
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n - 1</annotation></semantics></math>
observations are approximately the same as the covariance parameter
estimates obtained using all
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
observations. For a large enough sample size, this is a reasonable
assumption.</p>
<p>First define
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝚺</mi><mrow><mo>−</mo><mi>i</mi><mo>,</mo><mo>−</mo><mi>i</mi></mrow></msub><annotation encoding="application/x-tex">\boldsymbol{\Sigma}_{-i, -i}</annotation></semantics></math>
as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝚺</mi><annotation encoding="application/x-tex">\boldsymbol{\Sigma}</annotation></semantics></math>
with the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>th
row and column deleted,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝚺</mi><mrow><mi>i</mi><mo>,</mo><mo>−</mo><mi>i</mi></mrow></msub><annotation encoding="application/x-tex">\boldsymbol{\Sigma}_{i, -i}</annotation></semantics></math>
as the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>th
row of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝚺</mi><annotation encoding="application/x-tex">\boldsymbol{\Sigma}</annotation></semantics></math>
with the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>th
column deleted,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝚺</mi><mrow><mi>i</mi><mo>,</mo><mi>i</mi></mrow></msub><annotation encoding="application/x-tex">\boldsymbol{\Sigma}_{i, i}</annotation></semantics></math>
as the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>th
diagonal element of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝚺</mi><annotation encoding="application/x-tex">\boldsymbol{\Sigma}</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐗</mi><mrow><mo>−</mo><mi>i</mi></mrow></msub><annotation encoding="application/x-tex">\mathbf{X}_{-i}</annotation></semantics></math>
as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐗</mi><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics></math>
with the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>th
row deleted,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐗</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\mathbf{X}_{i}</annotation></semantics></math>
as the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>th
row of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐗</mi><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>y</mi><mrow><mo>−</mo><mi>i</mi></mrow></msub><annotation encoding="application/x-tex">y_{-i}</annotation></semantics></math>
as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐲</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math>
with the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>th
element deleted, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐲</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\mathbf{y}_i</annotation></semantics></math>
as the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>th
element of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐲</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math>.
<span class="citation">Wolf (1978)</span> shows that given
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">\boldsymbol{\Sigma}^{-1}</annotation></semantics></math>,
a computationally efficient form for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>𝚺</mi><mrow><mo>−</mo><mi>i</mi></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><annotation encoding="application/x-tex">\boldsymbol{\Sigma}^{-1}_{-i}</annotation></semantics></math>
exists. First observe that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">\boldsymbol{\Sigma}^{-1}</annotation></semantics></math>
can be represented blockwise as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><msub><mover><mi>𝚺</mi><mo accent="true">̃</mo></mover><mrow><mo>−</mo><mi>i</mi><mo>,</mo><mo>−</mo><mi>i</mi></mrow></msub></mtd><mtd columnalign="center" style="text-align: center"><msubsup><mover><mi>𝚺</mi><mo accent="true">̃</mo></mover><mrow><mi>i</mi><mo>,</mo><mo>−</mo><mi>i</mi></mrow><mi>⊤</mi></msubsup></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msub><mover><mi>𝚺</mi><mo accent="true">̃</mo></mover><mrow><mi>i</mi><mo>,</mo><mo>−</mo><mi>i</mi></mrow></msub></mtd><mtd columnalign="center" style="text-align: center"><msub><mover><mi>𝚺</mi><mo accent="true">̃</mo></mover><mrow><mi>i</mi><mo>,</mo><mi>i</mi></mrow></msub></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
 \boldsymbol{\Sigma}^{-1} = 
 \begin{bmatrix}
  \tilde{\boldsymbol{\Sigma}}_{-i, -i} &amp; \tilde{\boldsymbol{\Sigma}}_{i,-i}^\top \\
  \tilde{\boldsymbol{\Sigma}}_{i,-i} &amp; \tilde{\boldsymbol{\Sigma}}_{i, i}
 \end{bmatrix},
\end{equation*}</annotation></semantics></math> where the dimensions of
each
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>𝚺</mi><mo accent="true">̃</mo></mover><annotation encoding="application/x-tex">\tilde{\boldsymbol{\Sigma}}</annotation></semantics></math>
match the respective dimensions of relevant blocks in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝚺</mi><annotation encoding="application/x-tex">\boldsymbol{\Sigma}</annotation></semantics></math>.
Then it follows that
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>𝚺</mi><mrow><mo>−</mo><mi>i</mi><mo>,</mo><mo>−</mo><mi>i</mi></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mo>=</mo><msub><mover><mi>𝚺</mi><mo accent="true">̃</mo></mover><mrow><mo>−</mo><mi>i</mi><mo>,</mo><mo>−</mo><mi>i</mi></mrow></msub><mo>−</mo><msubsup><mover><mi>𝚺</mi><mo accent="true">̃</mo></mover><mrow><mi>i</mi><mo>,</mo><mo>−</mo><mi>i</mi></mrow><mi>⊤</mi></msubsup><msubsup><mover><mi>𝚺</mi><mo accent="true">̃</mo></mover><mrow><mi>i</mi><mo>,</mo><mi>i</mi></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><msub><mover><mi>𝚺</mi><mo accent="true">̃</mo></mover><mrow><mi>i</mi><mo>,</mo><mo>−</mo><mi>i</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\begin{equation*}
 \boldsymbol{\Sigma}^{-1}_{-i, -i} = \tilde{\boldsymbol{\Sigma}}_{-i, -i} - \tilde{\boldsymbol{\Sigma}}_{i,-i}^\top \tilde{\boldsymbol{\Sigma}}_{i, i}^{-1}\tilde{\boldsymbol{\Sigma}}_{i,-i}
\end{equation*}</annotation></semantics></math> and
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝛃</mi><mrow><mo>−</mo><mi>i</mi></mrow></msub><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mi>𝐗</mi><mrow><mo>−</mo><mi>i</mi></mrow><mi>⊤</mi></msubsup><msubsup><mi>𝚺</mi><mrow><mo>−</mo><mi>i</mi><mo>,</mo><mo>−</mo><mi>i</mi></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><msub><mi>𝐗</mi><mrow><mo>−</mo><mi>i</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msubsup><mi>𝐗</mi><mrow><mo>−</mo><mi>i</mi></mrow><mi>⊤</mi></msubsup><msubsup><mi>𝚺</mi><mrow><mo>−</mo><mi>i</mi><mo>,</mo><mo>−</mo><mi>i</mi></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><msub><mi>𝐲</mi><mrow><mo>−</mo><mi>i</mi></mrow></msub><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \boldsymbol{\beta}_{-i} = (\mathbf{X}^\top_{-i} \boldsymbol{\Sigma}^{-1}_{-i, -i} \mathbf{X}_{-i})^{-1} \mathbf{X}^\top_{-i} \boldsymbol{\Sigma}^{-1}_{-i, -i} \mathbf{y}_{-i},
\end{equation*}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝛃</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\boldsymbol{\beta}_i</annotation></semantics></math>
is the estimate of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛃</mi><annotation encoding="application/x-tex">\boldsymbol{\beta}</annotation></semantics></math>
constructed without the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>th
observation.</p>
<p>The loocv prediction of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding="application/x-tex">y_i</annotation></semantics></math>
is then given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover><mi>y</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><mo>=</mo><msub><mi>𝐗</mi><mi>i</mi></msub><msub><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mrow><mo>−</mo><mi>i</mi></mrow></msub><mo>+</mo><msub><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mi>i</mi><mo>,</mo><mo>−</mo><mi>i</mi></mrow></msub><msub><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mo>−</mo><mi>i</mi><mo>,</mo><mo>−</mo><mi>i</mi></mrow></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐲</mi><mi>i</mi></msub><mo>−</mo><msub><mi>𝐗</mi><mrow><mo>−</mo><mi>i</mi></mrow></msub><msub><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mrow><mo>−</mo><mi>i</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \hat{y}_i = \mathbf{X}_i \hat{\boldsymbol{\beta}}_{-i} + \hat{\boldsymbol{\Sigma}}_{i, -i}\hat{\boldsymbol{\Sigma}}_{-i, -i}(\mathbf{y}_i - \mathbf{X}_{-i} \hat{\boldsymbol{\beta}}_{-i})
\end{equation*}</annotation></semantics></math> and the prediction
variance of the loocv prediction of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding="application/x-tex">y_i</annotation></semantics></math>
is given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mover><mi>σ</mi><mo accent="true">̇</mo></mover><mi>i</mi><mn>2</mn></msubsup><mo>=</mo><msub><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mi>i</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>−</mo><msub><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mi>i</mi><mo>,</mo><mo>−</mo><mi>i</mi></mrow></msub><msubsup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mo>−</mo><mi>i</mi><mo>,</mo><mo>−</mo><mi>i</mi></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><msubsup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mi>i</mi><mo>,</mo><mo>−</mo><mi>i</mi></mrow><mi>⊤</mi></msubsup><mo>+</mo><msub><mi>𝐐</mi><mi>i</mi></msub><msup><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mi>𝐗</mi><mrow><mo>−</mo><mi>i</mi></mrow><mi>⊤</mi></msubsup><msubsup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mo>−</mo><mi>i</mi><mo>,</mo><mo>−</mo><mi>i</mi></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><msub><mi>𝐗</mi><mrow><mo>−</mo><mi>i</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msubsup><mi>𝐐</mi><mi>i</mi><mi>⊤</mi></msubsup><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \dot{\sigma}^2_i = \hat{\boldsymbol{\Sigma}}_{i, i} - \hat{\boldsymbol{\Sigma}}_{i, - i} \hat{\boldsymbol{\Sigma}}^{-1}_{-i, -i} \hat{\boldsymbol{\Sigma}}_{i, - i}^\top + \mathbf{Q}_i(\mathbf{X}_{-i}^\top \hat{\boldsymbol{\Sigma}}_{-i, -i}^{-1} \mathbf{X}_{-i})^{-1}\mathbf{Q}_i^\top ,
\end{equation*}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐐</mi><mi>i</mi></msub><mo>=</mo><msub><mi>𝐗</mi><mi>i</mi></msub><mo>−</mo><msub><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mi>i</mi><mo>,</mo><mo>−</mo><mi>i</mi></mrow></msub><msubsup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mo>−</mo><mi>i</mi><mo>,</mo><mo>−</mo><mi>i</mi></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><msub><mi>𝐗</mi><mrow><mo>−</mo><mi>i</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\mathbf{Q}_i = \mathbf{X}_i - \hat{\boldsymbol{\Sigma}}_{i, -i} \hat{\boldsymbol{\Sigma}}^{-1}_{-i, -i} \mathbf{X}_{-i}</annotation></semantics></math>.
These formulas are analogous to the formulas used to obtain linear
unbiased predictions of unobserved data and prediction variances. Model
fits are evaluated using several statistics: bias,
mean-squared-prediction error (MSPE), root-mean-squared-prediction error
(RMSPE), and the squared correlation (cor2) between the observed data
and leave-one-out predictions (regarded as a prediction version of
r-squared appropriate for comparing across spatial and nonspatial
models).</p>
<p>Bias is formally defined as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi><mi>i</mi><mi>a</mi><mi>s</mi><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mover><mi>y</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
 bias = \frac{1}{n}\sum_{i = 1}^n(y_i - \hat{y}_i).
\end{equation*}</annotation></semantics></math></p>
<p>MSPE is formally defined as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mi>S</mi><mi>P</mi><mi>E</mi><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mover><mi>y</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
 MSPE = \frac{1}{n}\sum_{i = 1}^n(y_i - \hat{y}_i)^2.
\end{equation*}</annotation></semantics></math></p>
<p>RMSPE is formally defined as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mi>M</mi><mi>S</mi><mi>P</mi><mi>E</mi><mo>=</mo><msqrt><mrow><mfrac><mn>1</mn><mi>n</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mover><mi>y</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mrow></msqrt><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
 RMSPE = \sqrt{\frac{1}{n}\sum_{i = 1}^n(y_i - \hat{y}_i)^2}.
\end{equation*}</annotation></semantics></math></p>
<p>cor2 is formally defined as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi><mi>o</mi><mi>r</mi><mn>2</mn><mo>=</mo><mtext mathvariant="normal">Cor</mtext><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐲</mi><mo>,</mo><mover><mi>𝐲</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
 cor2 = \text{Cor}(\mathbf{y}, \hat{\mathbf{y}})^2,
\end{equation*}</annotation></semantics></math> where
Cor<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mo>⋅</mo><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(\cdot)</annotation></semantics></math>
is the correlation function. cor2 is only returned for spatial linear
models, as it is not applicable for spatial generalized linear models
(we are predicting a latent mean parameter, which is unknown and not on
the same scale as the original data).</p>
<p>Generally, bias should be near zero for well-fitting models. The
lower the MSPE and RMSPE, the better the model fit. The higher the cor2,
the better the model fit.</p>
<div class="section level4">
<h4 id="sec:bigdata-loocv-lm">Big Data<a class="anchor" aria-label="anchor" href="#sec:bigdata-loocv-lm"></a>
</h4>
<p>Options for big data leave-one-out cross validation rely on the
<code>local</code> argument, which is passed to <code><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict()</a></code>.
The <code>local</code> list for <code><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict()</a></code> is explained in
detail in the <code><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict()</a></code> section, but we provide a short
summary of how <code>local</code> interacts with <code><a href="../reference/loocv.html">loocv()</a></code>
here.</p>
<p>For <code><a href="../reference/splm.html">splm()</a></code> and <code><a href="../reference/spautor.html">spautor()</a></code> objects, the
<code>local</code> method can be <code>"all"</code>. When the
<code>local</code> method is<code>"all"</code>, all of the data are used
for leave-one-out cross validation (i.e., it is implemented exactly as
previously described). Parallelization is implemented when setting
<code>parallel = TRUE</code> in <code>local</code>, and the number of
cores to use for parallelization is specified via
<code>ncores</code>.</p>
<p>For <code><a href="../reference/splm.html">splm()</a></code> objects, additional options for the
<code>local</code> method are <code>"covariance"</code> and
<code>"distance"</code>. When the <code>local</code> method is
<code>"covariance"</code>, then a number of observations (specified via
the <code>size</code> argument) having the highest covariance with the
held-out observation are used in the local neighborhood prediction
approach. When the <code>local</code> method is <code>"distance"</code>,
then a number of observations (specified via the <code>size</code>
argument) closest to the held-out observation are used in the local
neighborhood prediction approach. When no random effects are used, no
partition factor is used, and the spatial covariance function is
monotone decreasing, <code>"covariance"</code> and
<code>"distance"</code> are equivalent. The local neighborhood approach
only uses the observations in the local neighborhood of the held-out
observation to perform prediction, and is thus an approximation to the
true solution. Its computational efficiency derives from using
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝚺</mi><mrow><mi>l</mi><mo>,</mo><mi>l</mi></mrow></msub><annotation encoding="application/x-tex">\boldsymbol{\Sigma}_{l, l}</annotation></semantics></math>
(the covariance matrix of the observations in the local neighborhood)
instead of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝚺</mi><annotation encoding="application/x-tex">\boldsymbol{\Sigma}</annotation></semantics></math>
(the covariance matrix of all the observations). Parallelization is
implemented when setting <code>parallel = TRUE</code> in
<code>local</code>, and the number of cores to use for parallelization
is specified via <code>ncores</code>.</p>
</div>
</div>
<div class="section level3">
<h3 id="sec:predict-lm">
<code>predict()</code><a class="anchor" aria-label="anchor" href="#sec:predict-lm"></a>
</h3>
<div class="section level4">
<h4 id="sec:predict-none-lm">
<code>interval = "none"</code><a class="anchor" aria-label="anchor" href="#sec:predict-none-lm"></a>
</h4>
<p>The empirical best linear unbiased predictions (i.e., empirical
Kriging predictor) of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐲</mi><mi>u</mi></msub><annotation encoding="application/x-tex">\mathbf{y}_u</annotation></semantics></math>
are given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover><mi>𝐲</mi><mo accent="true">̇</mo></mover><mi>u</mi></msub><mo>=</mo><msub><mi>𝐗</mi><mi>u</mi></msub><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo>+</mo><msub><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mi>u</mi><mi>o</mi></mrow></msub><msubsup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mi>o</mi><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐲</mi><mi>o</mi></msub><mo>−</mo><msub><mi>𝐗</mi><mi>o</mi></msub><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation}\label{eq:blup}
  \mathbf{\dot{y}}_u =  \mathbf{X}_u \hat{\boldsymbol{\beta}} + \hat{\boldsymbol{\Sigma}}_{uo} \hat{\boldsymbol{\Sigma}}^{-1}_{o} (\mathbf{y}_o - \mathbf{X}_o \hat{\boldsymbol{\beta}}) .
\end{equation}</annotation></semantics></math></p>
<p>This equation sometimes called an empirical universal Kriging
predictor, a Kriging with external drift predictor, or a regression
Kriging predictor.</p>
<p>The covariance matrix of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝐲</mi><mo accent="true">̇</mo></mover><mi>u</mi></msub><annotation encoding="application/x-tex">\mathbf{\dot{y}}_u</annotation></semantics></math><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover><mi>𝚺</mi><mo accent="true">̇</mo></mover><mi>u</mi></msub><mo>=</mo><msub><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mi>u</mi></msub><mo>−</mo><msub><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mi>u</mi><mi>o</mi></mrow></msub><msubsup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mi>o</mi><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><msubsup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mi>u</mi><mi>o</mi></mrow><mi>⊤</mi></msubsup><mo>+</mo><mi>𝐐</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mi>𝐗</mi><mi>o</mi><mi>⊤</mi></msubsup><msubsup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mi>o</mi><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><msub><mi>𝐗</mi><mi>o</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>𝐐</mi><mi>⊤</mi></msup><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation}\label{eq:blup_cov}
  \dot{\boldsymbol{\Sigma}}_u = \hat{\boldsymbol{\Sigma}}_u - \hat{\boldsymbol{\Sigma}}_{uo} \hat{\boldsymbol{\Sigma}}^{-1}_o \hat{\boldsymbol{\Sigma}}^\top_{uo} + \mathbf{Q}(\mathbf{X}_o^\top \hat{\boldsymbol{\Sigma}}_o^{-1} \mathbf{X}_o)^{-1}\mathbf{Q}^\top ,
\end{equation}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐐</mi><mo>=</mo><msub><mi>𝐗</mi><mi>u</mi></msub><mo>−</mo><msub><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mi>u</mi><mi>o</mi></mrow></msub><msubsup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mi>o</mi><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><msub><mi>𝐗</mi><mi>o</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{Q} = \mathbf{X}_u - \hat{\boldsymbol{\Sigma}}_{uo} \hat{\boldsymbol{\Sigma}}^{-1}_o \mathbf{X}_o</annotation></semantics></math>.</p>
<p>When <code>se.fit = TRUE</code>, standard errors are returned by
taking the square root of the diagonal of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝚺</mi><mo accent="true">̇</mo></mover><mi>u</mi></msub><annotation encoding="application/x-tex">\dot{\boldsymbol{\Sigma}}_u</annotation></semantics></math>.</p>
</div>
<div class="section level4">
<h4 id="sec:predict-predict-lm">
<code>interval = "prediction"</code><a class="anchor" aria-label="anchor" href="#sec:predict-predict-lm"></a>
</h4>
<p>The empirical best linear unbiased predictions are returned as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝐲</mi><mo accent="true">̇</mo></mover><mi>u</mi></msub><annotation encoding="application/x-tex">\mathbf{\dot{y}}_u</annotation></semantics></math>.
The (100
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mo>×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math><code>level</code>)% prediction interval for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>u</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>i</mi></msub><annotation encoding="application/x-tex">(y_u)_i</annotation></semantics></math>
is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>y</mi><mo accent="true">̇</mo></mover><mi>u</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>i</mi></msub><mo>±</mo><msup><mi>z</mi><mo>*</mo></msup><msqrt><msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>𝚺</mi><mo accent="true">̇</mo></mover><mi>u</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>i</mi><mo>,</mo><mi>i</mi></mrow></msub></msqrt></mrow><annotation encoding="application/x-tex">(\dot{y}_u)_i \pm z^* \sqrt{(\dot{\boldsymbol{\Sigma}}_u)_{i, i}}</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msqrt><msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>𝚺</mi><mo accent="true">̇</mo></mover><mi>u</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>i</mi><mo>,</mo><mi>i</mi></mrow></msub></msqrt><annotation encoding="application/x-tex">\sqrt{(\dot{\boldsymbol{\Sigma}}_u)_{i, i}}</annotation></semantics></math>
is the standard error of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>y</mi><mo accent="true">̇</mo></mover><mi>u</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>i</mi></msub><annotation encoding="application/x-tex">(\dot{y}_u)_i</annotation></semantics></math>
obtained from <code>se.fit = TRUE</code>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Φ</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>z</mi><mo>*</mo></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>−</mo><mi>α</mi><mi>/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">\Phi(z^*) = 1 - \alpha / 2</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Φ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo>⋅</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Phi(\cdot)</annotation></semantics></math>
is the standard normal (Gaussian) cumulative distribution function,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>1</mn><mo>−</mo></mrow><annotation encoding="application/x-tex">\alpha = 1 -</annotation></semantics></math><code>level</code>, and <code>level</code> is an argument to
<code><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict()</a></code>. The default for <code>level</code> is 0.95,
which corresponds to a
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>z</mi><mo>*</mo></msup><annotation encoding="application/x-tex">z^*</annotation></semantics></math>
of approximately 1.96.</p>
</div>
<div class="section level4">
<h4 id="sec:predict-conf-lm">
<code>interval = "confidence"</code><a class="anchor" aria-label="anchor" href="#sec:predict-conf-lm"></a>
</h4>
<p>The best linear unbiased estimates of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">[</mo><msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>u</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>i</mi></msub><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\text{E}[(y_u)_i]</annotation></semantics></math>
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mo>⋅</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\text{E}(\cdot)</annotation></semantics></math>
denotes expectation) are returned by evaluating
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐗</mi><mi>u</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>i</mi></msub><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover></mrow><annotation encoding="application/x-tex">(\mathbf{X}_u)_i \hat{\boldsymbol{\beta}}</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐗</mi><mi>u</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>i</mi></msub><annotation encoding="application/x-tex">(\mathbf{X}_u)_i</annotation></semantics></math>
is the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>th
row of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐗</mi><mi>u</mi></msub><annotation encoding="application/x-tex">\mathbf{X}_u</annotation></semantics></math>
(i.e., fitted values corresponding to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐗</mi><mi>u</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>i</mi></msub><annotation encoding="application/x-tex">(\mathbf{X}_u)_i</annotation></semantics></math>
are returned). The (100
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mo>×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math><code>level</code>)% confidence interval for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">[</mo><msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>u</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>i</mi></msub><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\text{E}[(y_u)_i]</annotation></semantics></math>
is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐗</mi><mi>u</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>i</mi></msub><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo>±</mo><msup><mi>z</mi><mo>*</mo></msup><msqrt><mrow><msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐗</mi><mi>u</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>i</mi></msub><msup><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mi>𝐗</mi><mi>o</mi><mi>⊤</mi></msubsup><msubsup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mi>o</mi><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><msub><mi>𝐗</mi><mi>o</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msubsup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐗</mi><mi>u</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>i</mi><mi>⊤</mi></msubsup></mrow></msqrt></mrow><annotation encoding="application/x-tex">(\mathbf{X}_u)_i \hat{\boldsymbol{\beta}} \pm z^* \sqrt{(\mathbf{X}_u)_i (\mathbf{X}^\top_o \hat{\boldsymbol{\Sigma}}_o^{-1} \mathbf{X}_o)^{-1} (\mathbf{X}_u)_i^\top}</annotation></semantics></math>
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msqrt><mrow><msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐗</mi><mi>u</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>i</mi></msub><msup><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mi>𝐗</mi><mi>o</mi><mi>⊤</mi></msubsup><msubsup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mi>o</mi><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><msub><mi>𝐗</mi><mi>o</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msubsup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐗</mi><mi>u</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>i</mi><mi>⊤</mi></msubsup></mrow></msqrt><annotation encoding="application/x-tex">\sqrt{(\mathbf{X}_u)_i (\mathbf{X}^\top_o \hat{\boldsymbol{\Sigma}}_o^{-1} \mathbf{X}_o)^{-1} (\mathbf{X}_u)_i^\top}</annotation></semantics></math>
is the standard error of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>y</mi><mo accent="true">̇</mo></mover><mi>u</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>i</mi></msub><annotation encoding="application/x-tex">(\dot{y}_u)_i</annotation></semantics></math>
obtained from <code>se.fit = TRUE</code>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Φ</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>z</mi><mo>*</mo></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>−</mo><mi>α</mi><mi>/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">\Phi(z^*) = 1 - \alpha / 2</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Φ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo>⋅</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Phi(\cdot)</annotation></semantics></math>
is the standard normal (Gaussian) cumulative distribution function,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>1</mn><mo>−</mo></mrow><annotation encoding="application/x-tex">\alpha = 1 -</annotation></semantics></math><code>level</code>, and <code>level</code> is an argument to
<code><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict()</a></code>. The default for <code>level</code> is 0.95,
which corresponds to a
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>z</mi><mo>*</mo></msup><annotation encoding="application/x-tex">z^*</annotation></semantics></math>
of approximately 1.96.</p>
</div>
<div class="section level4">
<h4 id="sec:predict-spautor-lm">
<code>spautor()</code> extra steps<a class="anchor" aria-label="anchor" href="#sec:predict-spautor-lm"></a>
</h4>
<p>For spatial autoregressive models, an extra step is required to
obtain
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mi>o</mi><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><annotation encoding="application/x-tex">\hat{\boldsymbol{\Sigma}}^{-1}_o</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mi>u</mi></msub><annotation encoding="application/x-tex">\hat{\boldsymbol{\Sigma}}_u</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mi>u</mi><mi>o</mi></mrow></msub><annotation encoding="application/x-tex">\hat{\boldsymbol{\Sigma}}_{uo}</annotation></semantics></math>
as they depend on one another through the neighborhood structure of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐲</mi><mi>o</mi></msub><annotation encoding="application/x-tex">\mathbf{y}_o</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐲</mi><mi>u</mi></msub><annotation encoding="application/x-tex">\mathbf{y}_u</annotation></semantics></math>.
Recall that for autoregressive models, it is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">\boldsymbol{\Sigma}^{-1}</annotation></semantics></math>
that is straightforward to obtain, not
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝚺</mi><annotation encoding="application/x-tex">\boldsymbol{\Sigma}</annotation></semantics></math>.</p>
<p>Let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">\boldsymbol{\Sigma}^{-1}</annotation></semantics></math>
be the inverse covariance matrix of the observed and unobserved data,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐲</mi><mi>o</mi></msub><annotation encoding="application/x-tex">\mathbf{y}_o</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐲</mi><mi>u</mi></msub><annotation encoding="application/x-tex">\mathbf{y}_u</annotation></semantics></math>.
One approach to obtain
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝚺</mi><mi>o</mi></msub><annotation encoding="application/x-tex">\boldsymbol{\Sigma}_o</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝚺</mi><mrow><mi>u</mi><mi>o</mi></mrow></msub><annotation encoding="application/x-tex">\boldsymbol{\Sigma}_{uo}</annotation></semantics></math>
is to directly invert
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">\boldsymbol{\Sigma}^{-1}</annotation></semantics></math>
and then subset
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝚺</mi><annotation encoding="application/x-tex">\boldsymbol{\Sigma}</annotation></semantics></math>
appropriately. This inversion can be prohibitive when
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>n</mi><mi>o</mi></msub><mo>+</mo><msub><mi>n</mi><mi>u</mi></msub></mrow><annotation encoding="application/x-tex">n_o + n_u</annotation></semantics></math>
is large. A faster way to obtain
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝚺</mi><mi>o</mi></msub><annotation encoding="application/x-tex">\boldsymbol{\Sigma}_o</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝚺</mi><mrow><mi>u</mi><mi>o</mi></mrow></msub><annotation encoding="application/x-tex">\boldsymbol{\Sigma}_{uo}</annotation></semantics></math>
exists. Represent
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">\boldsymbol{\Sigma}^{-1}</annotation></semantics></math>
blockwise as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><msub><mover><mi>𝚺</mi><mo accent="true">̃</mo></mover><mi>o</mi></msub></mtd><mtd columnalign="center" style="text-align: center"><msubsup><mover><mi>𝚺</mi><mo accent="true">̃</mo></mover><mrow><mi>u</mi><mi>o</mi></mrow><mi>⊤</mi></msubsup></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msub><mover><mi>𝚺</mi><mo accent="true">̃</mo></mover><mrow><mi>u</mi><mi>o</mi></mrow></msub></mtd><mtd columnalign="center" style="text-align: center"><msub><mover><mi>𝚺</mi><mo accent="true">̃</mo></mover><mi>u</mi></msub></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}\label{eq:auto_hw}
  \boldsymbol{\Sigma}^{-1} =
  \begin{bmatrix}
    \tilde{\boldsymbol{\Sigma}}_{o} &amp; \tilde{\boldsymbol{\Sigma}}^{\top}_{uo} \\
    \tilde{\boldsymbol{\Sigma}}_{uo} &amp; \tilde{\boldsymbol{\Sigma}}_{u}
  \end{bmatrix},
\end{equation*}</annotation></semantics></math> where the dimensions of
the blocks match the relevant dimensions of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝚺</mi><annotation encoding="application/x-tex">\boldsymbol{\Sigma}</annotation></semantics></math>.
All of the terms required for prediction can be obtained from this block
representation. <span class="citation">Wolf (1978)</span> shows that
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><msubsup><mi>𝚺</mi><mi>o</mi><mrow><mo>−</mo><mn>1</mn></mrow></msubsup></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><msub><mover><mi>𝚺</mi><mo accent="true">̃</mo></mover><mi>o</mi></msub><mo>−</mo><msubsup><mover><mi>𝚺</mi><mo accent="true">̃</mo></mover><mrow><mi>u</mi><mi>o</mi></mrow><mi>⊤</mi></msubsup><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>𝚺</mi><mo accent="true">̃</mo></mover><mi>u</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msub><mover><mi>𝚺</mi><mo accent="true">̃</mo></mover><mrow><mi>u</mi><mi>o</mi></mrow></msub></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>𝚺</mi><mi>u</mi></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>𝚺</mi><mo accent="true">̃</mo></mover><mi>u</mi></msub><mo>−</mo><msub><mover><mi>𝚺</mi><mo accent="true">̃</mo></mover><mrow><mi>u</mi><mi>o</mi></mrow></msub><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>𝚺</mi><mo accent="true">̃</mo></mover><mi>o</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msubsup><mover><mi>𝚺</mi><mo accent="true">̃</mo></mover><mrow><mi>u</mi><mi>o</mi></mrow><mi>⊤</mi></msubsup><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>𝚺</mi><mrow><mi>u</mi><mi>o</mi></mrow></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mo>−</mo><msub><mi>𝚺</mi><mi>u</mi></msub><msub><mover><mi>𝚺</mi><mo accent="true">̃</mo></mover><mrow><mi>u</mi><mi>o</mi></mrow></msub><msubsup><mover><mi>𝚺</mi><mo accent="true">̃</mo></mover><mi>o</mi><mrow><mo>−</mo><mn>1</mn></mrow></msubsup></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{equation*}\label{eq:hw_forms}
  \begin{split}
    \boldsymbol{\Sigma}^{-1}_o &amp; = \tilde{\boldsymbol{\Sigma}}_{o} - \tilde{\boldsymbol{\Sigma}}^{ \top}_{uo} (\tilde{\boldsymbol{\Sigma}}_{u})^{-1} \tilde{\boldsymbol{\Sigma}}_{uo} \\
    \boldsymbol{\Sigma}_u &amp; = (\tilde{\boldsymbol{\Sigma}}_{u} - \tilde{\boldsymbol{\Sigma}}_{uo} (\tilde{\boldsymbol{\Sigma}}_{o})^{-1} \tilde{\boldsymbol{\Sigma}}^\top_{uo})^{-1} \\
    \boldsymbol{\Sigma}_{uo} &amp; = - \boldsymbol{\Sigma}_u \tilde{\boldsymbol{\Sigma}}_{uo} \tilde{\boldsymbol{\Sigma}}^{-1}_{o}
  \end{split}
\end{equation*}</annotation></semantics></math> Evaluating these
expressions at
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>𝛉</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat{\boldsymbol{\theta}}</annotation></semantics></math>
yields
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mi>o</mi><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><annotation encoding="application/x-tex">\hat{\boldsymbol{\Sigma}}^{-1}_o</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mi>u</mi></msub><annotation encoding="application/x-tex">\hat{\boldsymbol{\Sigma}}_u</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mi>u</mi><mi>o</mi></mrow></msub><annotation encoding="application/x-tex">\hat{\boldsymbol{\Sigma}}_{uo}</annotation></semantics></math>.</p>
<p>A similar result exists for the log determinant of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝚺</mi><mi>o</mi></msub><annotation encoding="application/x-tex">\boldsymbol{\Sigma}_o</annotation></semantics></math>,
which is not required for prediction but is required for restricted
maximum likelihood and maximum likelihood estimation.</p>
</div>
<div class="section level4">
<h4 id="sec:predict-bigdata-lm">Big Data<a class="anchor" aria-label="anchor" href="#sec:predict-bigdata-lm"></a>
</h4>
<p>When the number of observations in the fitted model (observed data)
are large or there are many locations to predict or both, it is often
necessary to implement computationally efficient big data
approximations. Big data approximations are implemented in
<code>spmodel</code> using the <code>local</code> argument to
<code><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict()</a></code>. When the <code>local</code> method is
<code>"all"</code>, all of the fitted model data are used to make
predictions. In this context, computational efficiency is only gained by
parallelizing each prediction. The only available <code>local</code>
method for <code><a href="../reference/spautor.html">spautor()</a></code> fitted models is <code>"all"</code>.
This is because the neighborhood structure of <code><a href="../reference/spautor.html">spautor()</a></code>
fitted models does not permit the subsetting used by the
<code>"covariance"</code> and <code>"distance"</code> methods that we
discuss next.</p>
<p>When the <code>local</code> method is <code>"covariance"</code>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mi>u</mi><mi>o</mi></mrow></msub><annotation encoding="application/x-tex">\hat{\boldsymbol{\Sigma}}_{uo}</annotation></semantics></math>
is computed between the observation being predicted
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐲</mi><mi>u</mi></msub><annotation encoding="application/x-tex">\mathbf{y}_u</annotation></semantics></math>)
and the rest of the observed data. This vector is then ordered and a
number of observations (specified via the <code>size</code> argument)
having the highest covariance with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐲</mi><mi>u</mi></msub><annotation encoding="application/x-tex">\mathbf{y}_u</annotation></semantics></math>
are subset, yielding
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝚺</mi><mo accent="true">̌</mo></mover><mrow><mi>u</mi><mi>o</mi></mrow></msub><annotation encoding="application/x-tex">\check{\boldsymbol{\Sigma}}_{uo}</annotation></semantics></math>,
which has dimension
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>×</mo><mi>s</mi><mi>i</mi><mi>z</mi><mi>e</mi></mrow><annotation encoding="application/x-tex">1 \times size</annotation></semantics></math>.
Then similarly
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mi>o</mi></msub><annotation encoding="application/x-tex">\hat{\boldsymbol{\Sigma}}_o</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐲</mi><mi>o</mi></msub><annotation encoding="application/x-tex">\mathbf{y}_o</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐗</mi><mi>u</mi></msub><annotation encoding="application/x-tex">\mathbf{X}_u</annotation></semantics></math>
are also subset by these <code>size</code> observations, yielding
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝚺</mi><mo accent="true">̌</mo></mover><mi>o</mi></msub><annotation encoding="application/x-tex">\check{\boldsymbol{\Sigma}}_{o}</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝐲</mi><mo accent="true">̌</mo></mover><mi>o</mi></msub><annotation encoding="application/x-tex">\check{\mathbf{y}}_o</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝐗</mi><mo accent="true">̌</mo></mover><mi>u</mi></msub><annotation encoding="application/x-tex">\check{\mathbf{X}}_u</annotation></semantics></math>,
respectively. The previous prediction equations can be evaluated at
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝚺</mi><mo accent="true">̌</mo></mover><mrow><mi>u</mi><mi>o</mi></mrow></msub><annotation encoding="application/x-tex">\check{\boldsymbol{\Sigma}}_{uo}</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝚺</mi><mo accent="true">̌</mo></mover><mi>o</mi></msub><annotation encoding="application/x-tex">\check{\boldsymbol{\Sigma}}_{o}</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝐲</mi><mo accent="true">̌</mo></mover><mi>o</mi></msub><annotation encoding="application/x-tex">\check{\mathbf{y}}_o</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝐗</mi><mo accent="true">̌</mo></mover><mi>u</mi></msub><annotation encoding="application/x-tex">\check{\mathbf{X}}_u</annotation></semantics></math>
(except for the quantity
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mi>𝐗</mi><mi>o</mi><mi>⊤</mi></msubsup><msubsup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mi>o</mi><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><msub><mi>𝐗</mi><mi>o</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">(\mathbf{X}_o^\top \hat{\boldsymbol{\Sigma}}_o^{-1} \mathbf{X}_o)^{-1}</annotation></semantics></math>,
which is evaluated using all the observed data) to yield predictions and
standard errors. When the <code>local</code> method is
<code>"distance"</code>, a similar approach is used except a number of
observations (specified via the <code>size</code> argument) closest (in
terms of Euclidean distance) to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐲</mi><mi>u</mi></msub><annotation encoding="application/x-tex">\mathbf{y}_u</annotation></semantics></math>
are subset instead. When random effects are not used, partition factors
are not used, and the spatial covariance function is monotone
decreasing, <code>"covariance"</code> and <code>"distance"</code> are
equivalent. This approach of subsetting the observed data by the set of
locations closest in covariance or proximity to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐲</mi><mi>u</mi></msub><annotation encoding="application/x-tex">\mathbf{y}_u</annotation></semantics></math>
is known as the local neighborhood approach. As long as
<code>size</code> is relatively small (the default is 100), the local
neighborhood approach is very computationally efficient, mainly because
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mover><mi>𝚺</mi><mo accent="true">̌</mo></mover><mi>o</mi><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><annotation encoding="application/x-tex">\check{\boldsymbol{\Sigma}}_{o}^{-1}</annotation></semantics></math>
is easy to compute. Additional computational efficiency is gained by
parallelizing each prediction.</p>
</div>
<div class="section level4">
<h4 id="sec:rf_pred-lm">
<code>splmRF()</code> and <code>spautorRF()</code><a class="anchor" aria-label="anchor" href="#sec:rf_pred-lm"></a>
</h4>
<p>Random forest spatial residual model predictions are obtained by
combining random forest predictions and spatial linear model predictions
(i.e., Kriging) of the random forest residuals. Formally, the random
forest spatial residual model predictions of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐲</mi><mi>u</mi></msub><annotation encoding="application/x-tex">\mathbf{y}_u</annotation></semantics></math>
are given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover><mi>𝐲</mi><mo accent="true">̇</mo></mover><mi>u</mi></msub><mo>=</mo><msub><mover><mi>𝐲</mi><mo accent="true">̇</mo></mover><mrow><mi>u</mi><mo>,</mo><mi>r</mi><mi>f</mi></mrow></msub><mo>+</mo><msub><mover><mi>𝐞</mi><mo accent="true">̇</mo></mover><mrow><mi>u</mi><mo>,</mo><mi>s</mi><mi>l</mi><mi>m</mi></mrow></msub><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \mathbf{\dot{y}}_u = \mathbf{\dot{y}}_{u, rf} + \mathbf{\dot{e}}_{u, slm},
\end{equation*}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝐲</mi><mo accent="true">̇</mo></mover><mrow><mi>u</mi><mo>,</mo><mi>r</mi><mi>f</mi></mrow></msub><annotation encoding="application/x-tex">\mathbf{\dot{y}}_{u, rf}</annotation></semantics></math>
are the random forest predictions for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐲</mi><mi>u</mi></msub><annotation encoding="application/x-tex">\mathbf{y}_u</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝐞</mi><mo accent="true">̇</mo></mover><mrow><mi>u</mi><mo>,</mo><mi>s</mi><mi>l</mi><mi>m</mi></mrow></msub><annotation encoding="application/x-tex">\mathbf{\dot{e}}_{u, slm}</annotation></semantics></math>
are the spatial linear model predictions of the random forest residuals
for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐲</mi><mi>u</mi></msub><annotation encoding="application/x-tex">\mathbf{y}_u</annotation></semantics></math>.
This process of obtaining predictions is sometimes analogously called
random forest regression Kriging <span class="citation">(Fox, Ver Hoef,
and Olsen 2020)</span>.</p>
<p>Uncertainty quantification in a random forest context has been
studied <span class="citation">(Meinshausen and Ridgeway 2006)</span>
but is not currently available in <code>spmodel</code>. Big data are
accommodated by supplying the <code>local</code> argument to
<code><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict()</a></code>.</p>
</div>
</div>
<div class="section level3">
<h3 id="sec:pr2-lm">
<code>pseudoR2()</code><a class="anchor" aria-label="anchor" href="#sec:pr2-lm"></a>
</h3>
<p>The pseudo R-squared is a generalization of the classical R-squared
from non-spatial linear models. Like the classical R-squared, the pseudo
R-squared measures the proportion of variability in the response
explained by the fixed effects in the fitted model. Unlike the classical
R-squared, the pseudo R-squared can be applied to models whose errors do
not satisfy the iid and constant variance assumption. The pseudo
R-squared is given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mi>R</mi><mn>2</mn><mo>=</mo><mn>1</mn><mo>−</mo><mfrac><mrow><mi>𝒟</mi><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>𝚯</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>𝒟</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>𝚯</mi><mo accent="true">̂</mo></mover><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
PR2 = 1 - \frac{\mathcal{D}(\boldsymbol{\hat{\Theta}})}{\mathcal{D}(\boldsymbol{\hat{\Theta}}_0)}.
\end{equation*}</annotation></semantics></math> For normal (Gaussian)
random errors, the pseudo R-squared is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mi>R</mi><mn>2</mn><mo>=</mo><mn>1</mn><mo>−</mo><mfrac><mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐲</mi><mo>−</mo><mi>𝐗</mi><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mi>⊤</mi></msup><msup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐲</mi><mo>−</mo><mi>𝐗</mi><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐲</mi><mo>−</mo><mover><mi>μ</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mi>⊤</mi></msup><msup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐲</mi><mo>−</mo><mover><mi>μ</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
PR2 = 1 - \frac{(\mathbf{y} - \mathbf{X} \hat{\boldsymbol{\beta}})^\top \hat{\boldsymbol{\Sigma}}^{-1}(\mathbf{y} - \mathbf{X} \hat{\boldsymbol{\beta}})}{(\mathbf{y} - \hat{\mu})^\top \hat{\boldsymbol{\Sigma}}^{-1}(\mathbf{y} - \hat{\mu})},
\end{equation*}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>μ</mi><mo accent="true">̂</mo></mover><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mn>𝟏</mn><mi>⊤</mi></msup><msup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mo>−</mo><mn>1</mn></mrow></msup><mn>𝟏</mn><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mn>𝟏</mn><mi>⊤</mi></msup><msup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐲</mi></mrow><annotation encoding="application/x-tex">\hat{\mu} = (\boldsymbol{1}^\top \hat{\boldsymbol{\Sigma}}^{-1} \boldsymbol{1})^{-1} \boldsymbol{1}^\top \hat{\boldsymbol{\Sigma}}^{-1} \mathbf{y}</annotation></semantics></math>.
For the non-spatial model, the pseudo R-squared reduces to the classical
R-squared, as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mi>R</mi><mn>2</mn><mo>=</mo><mn>1</mn><mo>−</mo><mfrac><mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐲</mi><mo>−</mo><mi>𝐗</mi><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mi>⊤</mi></msup><msup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐲</mi><mo>−</mo><mi>𝐗</mi><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐲</mi><mo>−</mo><mover><mi>μ</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mi>⊤</mi></msup><msup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐲</mi><mo>−</mo><mover><mi>μ</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mo>=</mo><mn>1</mn><mo>−</mo><mfrac><mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐲</mi><mo>−</mo><mi>𝐗</mi><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mi>⊤</mi></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐲</mi><mo>−</mo><mi>𝐗</mi><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐲</mi><mo>−</mo><mover><mi>μ</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mi>⊤</mi></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐲</mi><mo>−</mo><mover><mi>μ</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mo>=</mo><mn>1</mn><mo>−</mo><mfrac><mtext mathvariant="normal">SSE</mtext><mtext mathvariant="normal">SST</mtext></mfrac><mo>=</mo><mi>R</mi><mn>2</mn><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
PR2 = 1 - \frac{(\mathbf{y} - \mathbf{X} \hat{\boldsymbol{\beta}})^\top \hat{\boldsymbol{\Sigma}}^{-1}(\mathbf{y} - \mathbf{X} \hat{\boldsymbol{\beta}})}{(\mathbf{y} - \hat{\mu})^\top \hat{\boldsymbol{\Sigma}}^{-1}(\mathbf{y} - \hat{\mu})}  = 1 - \frac{(\mathbf{y} - \mathbf{X} \hat{\boldsymbol{\beta}})^\top (\mathbf{y} - \mathbf{X} \hat{\boldsymbol{\beta}})}{(\mathbf{y} - \hat{\mu})^\top (\mathbf{y} - \hat{\mu})} = 1 - \frac{\text{SSE}}{\text{SST}} = R2,
\end{equation*}</annotation></semantics></math> where SSE denotes the
error sum of squares and SST denotes the total sum of squares. The
result follows because for a non-spatial model,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝚺</mi><annotation encoding="application/x-tex">\boldsymbol{\Sigma}</annotation></semantics></math>
is proportional to the identity matrix.</p>
<p>The adjusted pseudo r-squared adjusts for additional explanatory
variables and is given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mi>R</mi><mn>2</mn><mi>a</mi><mi>d</mi><mi>j</mi><mo>=</mo><mn>1</mn><mo>−</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>P</mi><mi>R</mi><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow><mfrac><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow><mrow><mi>n</mi><mo>−</mo><mi>p</mi></mrow></mfrac><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
  PR2adj = 1 - (1 - PR2)\frac{n - 1}{n - p}.
\end{equation*}</annotation></semantics></math> If the fitted model does
not have an intercept, the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n - 1</annotation></semantics></math>
term is instead
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>.</p>
</div>
<div class="section level3">
<h3 id="sec:residuals-lm">
<code>residuals()</code><a class="anchor" aria-label="anchor" href="#sec:residuals-lm"></a>
</h3>
<p>Terminology regarding residual names is often conflicting and
confusing. Because of this, we explicitly define the residual options we
use in <code>spmodel</code>. These definitions may be different from
others you have seen in the literature.</p>
<p>When <code>type = "response"</code>, response residuals are returned:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐞</mi><mi>r</mi></msub><mo>=</mo><mi>𝐲</mi><mo>−</mo><mi>𝐗</mi><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
 \mathbf{e}_{r} = \mathbf{y} - \mathbf{X} \hat{\boldsymbol{\beta}}.
\end{equation*}</annotation></semantics></math></p>
<p>When <code>type = "pearson"</code>, Pearson residuals are returned:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐞</mi><mi>p</mi></msub><mo>=</mo><msup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mo>−</mo><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><msub><mi>𝐞</mi><mi>r</mi></msub><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
 \mathbf{e}_{p} = \hat{\boldsymbol{\Sigma}}^{-1/2}\mathbf{e}_{r},
\end{equation*}</annotation></semantics></math> If the errors are normal
(Gaussian), the Pearson residuals should be approximately normally
distributed with mean zero and variance one. The result follows when
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mo>−</mo><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><mo>≈</mo><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\hat{\boldsymbol{\Sigma}}^{-1/2} \approx \boldsymbol{\Sigma}^{-1/2}</annotation></semantics></math>
because
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><msub><mi>𝐞</mi><mi>r</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐞</mi><mi>r</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><mn>𝟎</mn><mo>=</mo><mn>𝟎</mn></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \text{E}(\boldsymbol{\Sigma}^{-1/2} \mathbf{e}_{r}) = \boldsymbol{\Sigma}^{-1/2} \text{E}(\mathbf{e}_{r}) = \boldsymbol{\Sigma}^{-1/2} \boldsymbol{0} = \boldsymbol{0}
\end{equation*}</annotation></semantics></math> and
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><msub><mi>𝐞</mi><mi>r</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐞</mi><mi>r</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>≈</mo><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><mi>𝚺</mi><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><msup><mi>𝚺</mi><mrow><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝚺</mi><mrow><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mi>𝐈</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{equation*}
  \begin{split}
  \text{Cov}(\boldsymbol{\Sigma}^{-1/2} \mathbf{e}_{r}) &amp; = \boldsymbol{\Sigma}^{-1/2} \text{Cov}(\mathbf{e}_{r}) \boldsymbol{\Sigma}^{-1/2} \\
  &amp; \approx \boldsymbol{\Sigma}^{-1/2} \boldsymbol{\Sigma} \boldsymbol{\Sigma}^{-1/2} \\
  &amp; = (\boldsymbol{\Sigma}^{-1/2} \boldsymbol{\Sigma}^{1/2})(\boldsymbol{\Sigma}^{1/2} \boldsymbol{\Sigma}^{-1/2}) \\
  &amp; = \mathbf{I}
  \end{split}
\end{equation*}</annotation></semantics></math></p>
<p>When <code>type = "standardized"</code>, standardized residuals are
returned:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐞</mi><mi>s</mi></msub><mo>=</mo><msub><mi>𝐞</mi><mi>p</mi></msub><mo>⊙</mo><mfrac><mn>1</mn><msqrt><mrow><mn>1</mn><mo>−</mo><mi>d</mi><mi>i</mi><mi>a</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐇</mi><mo>*</mo></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow></msqrt></mfrac><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
 \mathbf{e}_{s} = \mathbf{e}_{p} \odot \frac{1}{\sqrt{1 - diag(\mathbf{H}^*)}},
\end{equation*}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>i</mi><mi>a</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐇</mi><mo>*</mo></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">diag(\mathbf{H}^*)</annotation></semantics></math>
is the diagonal of the spatial hat matrix,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐇</mi><mi>s</mi></msub><mo>≡</mo><msup><mi>𝐗</mi><mo>*</mo></msup><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐗</mi><mrow><mo>*</mo><mi>⊤</mi></mrow></msup><msup><mi>𝐗</mi><mo>*</mo></msup><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>𝐗</mi><mrow><mo>*</mo><mi>⊤</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{H}_s \equiv \mathbf{X}^* (\mathbf{X}^{* \top} \mathbf{X}^*)^{-1} \mathbf{X}^{* \top}</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mo>⊙</mo><annotation encoding="application/x-tex">\odot</annotation></semantics></math>
denotes the Hadmard (element-wise) product. This residual transformation
“standardizes” the Pearson residuals. As such, the standardized
residuals should also have mean zero and variance
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐞</mi><mi>s</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐈</mi><mo>−</mo><msup><mi>𝐇</mi><mo>*</mo></msup><mo stretchy="true" form="postfix">)</mo></mrow><msup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mo>−</mo><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><mi>𝐲</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>≈</mo><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐈</mi><mo>−</mo><msup><mi>𝐇</mi><mo>*</mo></msup><mo stretchy="true" form="postfix">)</mo></mrow><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><mi>𝐲</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐈</mi><mo>−</mo><msup><mi>𝐇</mi><mo>*</mo></msup><mo stretchy="true" form="postfix">)</mo></mrow><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐲</mi><mo stretchy="true" form="postfix">)</mo></mrow><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐈</mi><mo>−</mo><msup><mi>𝐇</mi><mo>*</mo></msup><mo stretchy="true" form="postfix">)</mo></mrow><mi>⊤</mi></msup></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐈</mi><mo>−</mo><msup><mi>𝐇</mi><mo>*</mo></msup><mo stretchy="true" form="postfix">)</mo></mrow><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><mi>𝚺</mi><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐈</mi><mo>−</mo><msup><mi>𝐇</mi><mo>*</mo></msup><mo stretchy="true" form="postfix">)</mo></mrow><mi>⊤</mi></msup></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐈</mi><mo>−</mo><msup><mi>𝐇</mi><mo>*</mo></msup><mo stretchy="true" form="postfix">)</mo></mrow><mi>𝐈</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐈</mi><mo>−</mo><msup><mi>𝐇</mi><mo>*</mo></msup><mo stretchy="true" form="postfix">)</mo></mrow><mi>⊤</mi></msup></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐈</mi><mo>−</mo><msup><mi>𝐇</mi><mo>*</mo></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{equation*}
  \begin{split}
  \text{Cov}(\mathbf{e}_{s}) &amp; = \text{Cov}((\mathbf{I} - \mathbf{H}^*) \hat{\boldsymbol{\Sigma}}^{-1/2}\mathbf{y}) \\
  &amp; \approx \text{Cov}((\mathbf{I} - \mathbf{H}^*) \boldsymbol{\Sigma}^{-1/2}\mathbf{y}) \\
  &amp; = (\mathbf{I} - \mathbf{H}^*) \boldsymbol{\Sigma}^{-1/2} \text{Cov}(\mathbf{y}) \boldsymbol{\Sigma}^{-1/2}(\mathbf{I} - \mathbf{H}^*)^\top \\
  &amp; = (\mathbf{I} - \mathbf{H}^*) \boldsymbol{\Sigma}^{-1/2} \boldsymbol{\Sigma} \boldsymbol{\Sigma}^{-1/2}(\mathbf{I} - \mathbf{H}^*)^\top \\
  &amp; = (\mathbf{I} - \mathbf{H}^*) \mathbf{I} (\mathbf{I} - \mathbf{H}^*)^\top \\
  &amp; = (\mathbf{I} - \mathbf{H}^*),
  \end{split}
\end{equation*}</annotation></semantics></math> because
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐈</mi><mo>−</mo><msup><mi>𝐇</mi><mo>*</mo></msup><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(\mathbf{I} - \mathbf{H}^*)</annotation></semantics></math>
is symmetric and idempotent. Note that the average value of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>i</mi><mi>a</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐇</mi><mo>*</mo></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">diag(\mathbf{H}^*)</annotation></semantics></math>
is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mi>/</mi><mi>n</mi></mrow><annotation encoding="application/x-tex">p / n</annotation></semantics></math>,
so
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐈</mi><mo>−</mo><msup><mi>𝐇</mi><mo>*</mo></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>≈</mo><mi>𝐈</mi></mrow><annotation encoding="application/x-tex">(\mathbf{I} - \mathbf{H}^*) \approx \mathbf{I}</annotation></semantics></math>
for large sample sizes.</p>
</div>
<div class="section level3">
<h3 id="sec:spmod">
<code>spautor()</code> and <code>splm()</code><a class="anchor" aria-label="anchor" href="#sec:spmod"></a>
</h3>
<p>Next we discuss technical details for the <code><a href="../reference/spautor.html">spautor()</a></code> and
<code><a href="../reference/splm.html">splm()</a></code> functions. Many of the details for the two functions
are the same, though occasional differences are noted in the following
subsection headers. Specifically, <code><a href="../reference/spautor.html">spautor()</a></code> and
<code><a href="../reference/splm.html">splm()</a></code> are for different data types and use different
covariance functions. <code><a href="../reference/spautor.html">spautor()</a></code> is for spatial linear
models with areal data (i.e., spatial autoregressive models) and
<code><a href="../reference/splm.html">splm()</a></code> is for spatial linear models with point-referenced
data (i.e., geostatistical models). There are also a few features
<code><a href="../reference/splm.html">splm()</a></code> has that <code><a href="../reference/spautor.html">spautor()</a></code> does not:
semivariogram-based estimation, random effects, anisotropy, and big data
approximations.</p>
<div class="section level4">
<h4 id="sec:spautor-fn">
<code>spautor()</code> Spatial Covariance Functions<a class="anchor" aria-label="anchor" href="#sec:spautor-fn"></a>
</h4>
<p>For areal data, the covariance matrix depends on the specification of
a neighborhood structure among the observations. Observations with at
least one neighbor (not including itself) are called “connected”
observations. Observations with no neighbors are called “unconnected”
observations. The autoregressive spatial covariance matrix can be
defined as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝚺</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><msubsup><mi>σ</mi><mrow><mi>d</mi><mi>e</mi></mrow><mn>2</mn></msubsup><mi>𝐑</mi></mtd><mtd columnalign="center" style="text-align: center"><mn>𝟎</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>𝟎</mn></mtd><mtd columnalign="center" style="text-align: center"><msubsup><mi>σ</mi><mi>ξ</mi><mn>2</mn></msubsup><mi>𝐈</mi></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow><mo>+</mo><msubsup><mi>σ</mi><mrow><mi>i</mi><mi>e</mi></mrow><mn>2</mn></msubsup><mi>𝐈</mi><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \boldsymbol{\Sigma} =
  \begin{bmatrix}
    \sigma^2_{de} \mathbf{R} &amp; \mathbf{0} \\
    \mathbf{0} &amp; \sigma^2_{\xi} \mathbf{I}
  \end{bmatrix}
  + \sigma^2_{ie} \mathbf{I},
\end{equation*}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>σ</mi><mrow><mi>d</mi><mi>e</mi></mrow><mn>2</mn></msubsup><annotation encoding="application/x-tex">\sigma^2_{de}</annotation></semantics></math><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mo>≥</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(\geq 0)</annotation></semantics></math>
is the spatially dependent (correlated) variance for the connected
observations,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐑</mi><annotation encoding="application/x-tex">\mathbf{R}</annotation></semantics></math>
is a matrix that describes the spatial dependence for the connected
observations,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>σ</mi><mi>ξ</mi><mn>2</mn></msubsup><annotation encoding="application/x-tex">\sigma^2_{\xi}</annotation></semantics></math><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mo>≥</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(\geq 0)</annotation></semantics></math>
is the independent (not correlated) variance for the unconnected
observations, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>σ</mi><mrow><mi>i</mi><mi>e</mi></mrow><mn>2</mn></msubsup><annotation encoding="application/x-tex">\sigma^2_{ie}</annotation></semantics></math><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mo>≥</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(\geq 0)</annotation></semantics></math>
is the independent (not correlated) variance for all observations. As
seen, the connected and unconnected observations are allowed different
variances. The total variance for connected observations is then
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>σ</mi><mrow><mi>d</mi><mi>e</mi></mrow><mn>2</mn></msubsup><mo>+</mo><msubsup><mi>σ</mi><mrow><mi>i</mi><mi>e</mi></mrow><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">\sigma^2_{de} + \sigma^2_{ie}</annotation></semantics></math>
and the total variance for unconnected observations is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>σ</mi><mi>ξ</mi><mn>2</mn></msubsup><mo>+</mo><msubsup><mi>σ</mi><mrow><mi>i</mi><mi>e</mi></mrow><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">\sigma^2_{\xi} + \sigma^2_{ie}</annotation></semantics></math>.
<code>spmodel</code> accommodates two spatial covariances: conditional
autoregressive (CAR) and simultaneous autoregressive (SAR), both of
which have their
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐑</mi><annotation encoding="application/x-tex">\mathbf{R}</annotation></semantics></math>
forms provided in the following table.</p>
<table class="table">
<caption>The forms of R for each spatial covariance type available in
spautor()</caption>
<colgroup>
<col width="45%">
<col width="54%">
</colgroup>
<thead><tr class="header">
<th>Spatial covariance type</th>
<th>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐑</mi><annotation encoding="application/x-tex">\mathbf{R}</annotation></semantics></math>
functional form</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="monospace">"𝚌𝚊𝚛"</mtext><annotation encoding="application/x-tex">\texttt{"car"}</annotation></semantics></math></td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐈</mi><mo>−</mo><mi>ϕ</mi><mi>𝐖</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐌</mi></mrow><annotation encoding="application/x-tex">(\mathbf{I} - \phi\mathbf{W})^{-1}\mathbf{M}</annotation></semantics></math></td>
</tr>
<tr class="even">
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="monospace">"𝚜𝚊𝚛"</mtext><annotation encoding="application/x-tex">\texttt{"sar"}</annotation></semantics></math></td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mrow><mo stretchy="true" form="prefix">[</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐈</mi><mo>−</mo><mi>ϕ</mi><mi>𝐖</mi><mo stretchy="true" form="postfix">)</mo></mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐈</mi><mo>−</mo><mi>ϕ</mi><mi>𝐖</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>⊤</mi></msup><mo stretchy="true" form="postfix">]</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">[(\mathbf{I} - \phi\mathbf{W})(\mathbf{I} - \phi\mathbf{W})^\top]^{-1}</annotation></semantics></math></td>
</tr>
</tbody>
</table>
<p>For both CAR and SAR covariance functions,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐑</mi><annotation encoding="application/x-tex">\mathbf{R}</annotation></semantics></math>
depends on similar quantities:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐈</mi><annotation encoding="application/x-tex">\mathbf{I}</annotation></semantics></math>,
an identity matrix;
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ϕ</mi><annotation encoding="application/x-tex">\phi</annotation></semantics></math>,
a range parameter, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐖</mi><annotation encoding="application/x-tex">\mathbf{W}</annotation></semantics></math>,
a matrix that defines the neighborhood structure. Often
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐖</mi><annotation encoding="application/x-tex">\mathbf{W}</annotation></semantics></math>
is symmetric but it need not be. Valid values for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ϕ</mi><annotation encoding="application/x-tex">\phi</annotation></semantics></math>
are in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mi>/</mi><msub><mi>λ</mi><mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub><mo>,</mo><mn>1</mn><mi>/</mi><msub><mi>λ</mi><mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(1 / \lambda_{min}, 1 / \lambda_{max})</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>λ</mi><mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub><annotation encoding="application/x-tex">\lambda_{min}</annotation></semantics></math>
is the minimum eigenvalue of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐖</mi><annotation encoding="application/x-tex">\mathbf{W}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>λ</mi><mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub><annotation encoding="application/x-tex">\lambda_{max}</annotation></semantics></math>
is the maximum eigenvalue of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐖</mi><annotation encoding="application/x-tex">\mathbf{W}</annotation></semantics></math><span class="citation">(Ver Hoef et al. 2018)</span>. For SAR covariance
functions,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>λ</mi><mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub><annotation encoding="application/x-tex">\lambda_{min}</annotation></semantics></math>
must be negative and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>λ</mi><mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub><annotation encoding="application/x-tex">\lambda_{max}</annotation></semantics></math>
must be positive. For CAR covariances functions, a matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐌</mi><annotation encoding="application/x-tex">\mathbf{M}</annotation></semantics></math>
matrix must be provided that satisfies the CAR symmetry condition, which
enforces the symmetry of the covariance matrix. The CAR symmetry
condition states
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><msub><mi>𝐖</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><msub><mi>𝐌</mi><mrow><mi>i</mi><mi>i</mi></mrow></msub></mfrac><mo>=</mo><mfrac><msub><mi>𝐖</mi><mrow><mi>j</mi><mi>i</mi></mrow></msub><msub><mi>𝐌</mi><mrow><mi>j</mi><mi>j</mi></mrow></msub></mfrac></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \frac{\mathbf{W}_{ij}}{\mathbf{M}_{ii}} = \frac{\mathbf{W}_{ji}}{\mathbf{M}_{jj}}
\end{equation*}</annotation></semantics></math> for all
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>
index rows or columns. When
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐖</mi><annotation encoding="application/x-tex">\mathbf{W}</annotation></semantics></math>
is symmetric,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐌</mi><annotation encoding="application/x-tex">\mathbf{M}</annotation></semantics></math>
is often taken to be the identity matrix.</p>
<p>The default in <code>spmodel</code> is to row-standardize
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐖</mi><annotation encoding="application/x-tex">\mathbf{W}</annotation></semantics></math>
by dividing each element by its respective row sum, which decreases
variance. If row-standardization is not used for a CAR model, the
default in <code>spmodel</code> for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐌</mi><annotation encoding="application/x-tex">\mathbf{M}</annotation></semantics></math>
is the identity matrix.</p>
</div>
<div class="section level4">
<h4 id="sec:splm-fn">
<code>splm()</code> Spatial Covariance Functions<a class="anchor" aria-label="anchor" href="#sec:splm-fn"></a>
</h4>
<p>For point-referenced data, the spatial covariance is given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>σ</mi><mrow><mi>d</mi><mi>e</mi></mrow><mn>2</mn></msubsup><mi>𝐑</mi><mo>+</mo><msubsup><mi>σ</mi><mrow><mi>i</mi><mi>e</mi></mrow><mn>2</mn></msubsup><mi>𝐈</mi><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
\sigma^2_{de}\mathbf{R} + \sigma^2_{ie} \mathbf{I},
\end{equation*}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>σ</mi><mrow><mi>d</mi><mi>e</mi></mrow><mn>2</mn></msubsup><annotation encoding="application/x-tex">\sigma^2_{de}</annotation></semantics></math><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mo>≥</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(\geq 0)</annotation></semantics></math>
is the spatially dependent (correlated) variance,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐑</mi><annotation encoding="application/x-tex">\mathbf{R}</annotation></semantics></math>
is a spatial correlation matrix,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>σ</mi><mrow><mi>i</mi><mi>e</mi></mrow><mn>2</mn></msubsup><annotation encoding="application/x-tex">\sigma^2_{ie}</annotation></semantics></math><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mo>≥</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(\geq 0)</annotation></semantics></math>
is the spatially independent (not correlated) variance, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐈</mi><annotation encoding="application/x-tex">\mathbf{I}</annotation></semantics></math>
is an identity matrix. The
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐑</mi><annotation encoding="application/x-tex">\mathbf{R}</annotation></semantics></math>
matrix always depends on a range parameter,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ϕ</mi><annotation encoding="application/x-tex">\phi</annotation></semantics></math><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mo>&gt;</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(&gt; 0)</annotation></semantics></math>,
that controls the behavior of the covariance function with distance. For
some covariance functions, the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐑</mi><annotation encoding="application/x-tex">\mathbf{R}</annotation></semantics></math>
matrix depends on an additional parameter that we call the “extra”
parameter. The following table shows the parametric form for all
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐑</mi><annotation encoding="application/x-tex">\mathbf{R}</annotation></semantics></math>
matrices available in <code><a href="../reference/splm.html">splm()</a></code>. The range parameter is
denoted as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ϕ</mi><annotation encoding="application/x-tex">\phi</annotation></semantics></math>,
the distance is denoted as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation></semantics></math>,
the distance divided by the range parameter
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mi>/</mi><mi>ϕ</mi></mrow><annotation encoding="application/x-tex">h / \phi</annotation></semantics></math>)
is denoted as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>η</mi><annotation encoding="application/x-tex">\eta</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ℐ</mi><mo stretchy="false" form="prefix">{</mo><mo>⋅</mo><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">\mathcal{I}\{\cdot\}</annotation></semantics></math>
is an indicator function equal to one when the argument occurs and zero
otherwise, and the extra parameter is denoted as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ξ</mi><annotation encoding="application/x-tex">\xi</annotation></semantics></math>
(when relevant).</p>
<table class="table">
<caption>The forms of R for each spatial covariance type available in
splm(). All spatial covariance functions are valid in two dimensions
except the triangular and cosine functions, which are only valid in one
dimension.</caption>
<colgroup>
<col width="56%">
<col width="43%">
</colgroup>
<thead><tr class="header">
<th>Spatial Covariance Type</th>
<th>R Functional Form</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="monospace">"𝚎𝚡𝚙𝚘𝚗𝚎𝚗𝚝𝚒𝚊𝚕"</mtext><annotation encoding="application/x-tex">\texttt{"exponential"}</annotation></semantics></math></td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>e</mi><mrow><mo>−</mo><mi>η</mi></mrow></msup><annotation encoding="application/x-tex">e^{-\eta}</annotation></semantics></math></td>
</tr>
<tr class="even">
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="monospace">"𝚜𝚙𝚑𝚎𝚛𝚒𝚌𝚊𝚕"</mtext><annotation encoding="application/x-tex">\texttt{"spherical"}</annotation></semantics></math></td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mn>1.5</mn><mi>η</mi><mo>+</mo><mn>0.5</mn><msup><mi>η</mi><mn>3</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow><mi>ℐ</mi><mo stretchy="false" form="prefix">{</mo><mi>h</mi><mo>≤</mo><mi>ϕ</mi><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">(1 - 1.5\eta + 0.5\eta^3)\mathcal{I}\{h \leq \phi \}</annotation></semantics></math></td>
</tr>
<tr class="odd">
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="monospace">"𝚐𝚊𝚞𝚜𝚜𝚒𝚊𝚗"</mtext><annotation encoding="application/x-tex">\texttt{"gaussian"}</annotation></semantics></math></td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>e</mi><mrow><mo>−</mo><msup><mi>η</mi><mn>2</mn></msup></mrow></msup><annotation encoding="application/x-tex">e^{-\eta^2}</annotation></semantics></math></td>
</tr>
<tr class="even">
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="monospace">"𝚝𝚛𝚒𝚊𝚗𝚐𝚞𝚕𝚊𝚛"</mtext><annotation encoding="application/x-tex">\texttt{"triangular"}</annotation></semantics></math></td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>η</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>ℐ</mi><mo stretchy="false" form="prefix">{</mo><mi>h</mi><mo>≤</mo><mi>ϕ</mi><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">(1 - \eta)\mathcal{I}\{h \leq \phi \}</annotation></semantics></math></td>
</tr>
<tr class="odd">
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="monospace">"𝚌𝚒𝚛𝚌𝚞𝚕𝚊𝚛"</mtext><annotation encoding="application/x-tex">\texttt{"circular"}</annotation></semantics></math></td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mfrac><mn>2</mn><mi>π</mi></mfrac><mrow><mo stretchy="true" form="prefix">[</mo><mi>m</mi><msqrt><mrow><mn>1</mn><mo>−</mo><msup><mi>m</mi><mn>2</mn></msup></mrow></msqrt><mo>+</mo><mi>s</mi><mi>i</mi><msup><mi>n</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo stretchy="false" form="prefix">{</mo><mi>m</mi><mo stretchy="false" form="postfix">}</mo><mo stretchy="true" form="postfix">]</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mi>ℐ</mi><mo stretchy="false" form="prefix">{</mo><mi>h</mi><mo>≤</mo><mi>ϕ</mi><mo stretchy="false" form="postfix">}</mo><mo>,</mo><mi>m</mi><mo>=</mo><mi>m</mi><mi>i</mi><mi>n</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>η</mi><mo>,</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">(1 - \frac{2}{\pi}[m\sqrt{1 - m^2} + sin^{-1}\{m\}])\mathcal{I}\{h \leq \phi \}, m = min(\eta, 1)</annotation></semantics></math></td>
</tr>
<tr class="even">
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="monospace">"𝚌𝚞𝚋𝚒𝚌"</mtext><annotation encoding="application/x-tex">\texttt{"cubic"}</annotation></semantics></math></td>
<td>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mn>7</mn><msup><mi>η</mi><mn>2</mn></msup><mo>+</mo><mn>8.75</mn><msup><mi>η</mi><mn>3</mn></msup><mo>−</mo><mn>3.5</mn><msup><mi>η</mi><mn>5</mn></msup><mo>+</mo><mn>0.75</mn><msup><mi>η</mi><mn>7</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow><mi>ℐ</mi><mo stretchy="false" form="prefix">{</mo><mi>h</mi><mo>≤</mo><mi>ϕ</mi><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">(1 - 7\eta^2 + 8.75\eta^3 - 3.5\eta^5 + 0.75 \eta^7)\mathcal{I}\{h \leq \phi \}</annotation></semantics></math>
\</td>
</tr>
<tr class="odd">
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="monospace">"𝚙𝚎𝚗𝚝𝚊𝚜𝚙𝚑𝚎𝚛𝚒𝚌𝚊𝚕"</mtext><annotation encoding="application/x-tex">\texttt{"pentaspherical"}</annotation></semantics></math></td>
<td>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mn>1.875</mn><mi>η</mi><mo>+</mo><mn>1.250</mn><msup><mi>η</mi><mn>3</mn></msup><mo>−</mo><mn>0.375</mn><msup><mi>η</mi><mn>5</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow><mi>ℐ</mi><mo stretchy="false" form="prefix">{</mo><mi>h</mi><mo>≤</mo><mi>ϕ</mi><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">(1 - 1.875\eta + 1.250\eta^3 - 0.375\eta^5)\mathcal{I}\{h \leq \phi \}</annotation></semantics></math>
\</td>
</tr>
<tr class="even">
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="monospace">"𝚌𝚘𝚜𝚒𝚗𝚎"</mtext><annotation encoding="application/x-tex">\texttt{"cosine"}</annotation></semantics></math></td>
<td>$ cos() $</td>
</tr>
<tr class="odd">
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="monospace">"𝚠𝚊𝚟𝚎"</mtext><annotation encoding="application/x-tex">\texttt{"wave"}</annotation></semantics></math></td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>s</mi><mi>i</mi><mi>n</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>η</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mi>η</mi></mfrac><mi>ℐ</mi><mo stretchy="false" form="prefix">{</mo><mi>h</mi><mo>&gt;</mo><mn>0</mn><mo stretchy="false" form="postfix">}</mo><mo>+</mo><mi>ℐ</mi><mo stretchy="false" form="prefix">{</mo><mi>h</mi><mo>=</mo><mn>0</mn><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">\frac{sin(\eta)}{\eta}\mathcal{I}\{h &gt; 0 \} + \mathcal{I}\{h = 0 \}</annotation></semantics></math></td>
</tr>
<tr class="even">
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="monospace">"𝚓𝚋𝚎𝚜𝚜𝚎𝚕"</mtext><annotation encoding="application/x-tex">\texttt{"jbessel"}</annotation></semantics></math></td>
<td>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>B</mi><mi>j</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mi>ϕ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><msub><mi>B</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">B_j(h\phi), B_j</annotation></semantics></math>
is Bessel-J</td>
</tr>
<tr class="odd">
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="monospace">"𝚐𝚛𝚊𝚟𝚒𝚝𝚢"</mtext><annotation encoding="application/x-tex">\texttt{"gravity"}</annotation></semantics></math></td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>+</mo><msup><mi>η</mi><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><annotation encoding="application/x-tex">(1 + \eta^2)^{-1/2}</annotation></semantics></math></td>
</tr>
<tr class="even">
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="monospace">"𝚛𝚚𝚞𝚊𝚍"</mtext><annotation encoding="application/x-tex">\texttt{"rquad"}</annotation></semantics></math></td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>+</mo><msup><mi>η</mi><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">(1 + \eta^2)^{-1}</annotation></semantics></math></td>
</tr>
<tr class="odd">
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="monospace">"𝚖𝚊𝚐𝚗𝚎𝚝𝚒𝚌"</mtext><annotation encoding="application/x-tex">\texttt{"magnetic"}</annotation></semantics></math></td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>+</mo><msup><mi>η</mi><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>3</mn><mi>/</mi><mn>2</mn></mrow></msup><annotation encoding="application/x-tex">(1 + \eta^2)^{-3/2}</annotation></semantics></math></td>
</tr>
<tr class="even">
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="monospace">"𝚖𝚊𝚝𝚎𝚛𝚗"</mtext><annotation encoding="application/x-tex">\texttt{"matern"}</annotation></semantics></math></td>
<td>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><msup><mn>2</mn><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>ξ</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mrow><mi>Γ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>ξ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><msup><mi>α</mi><mi>ξ</mi></msup><msub><mi>B</mi><mi>k</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>α</mi><mo>,</mo><mi>ξ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>α</mi><mo>=</mo><msqrt><mrow><mn>2</mn><mi>ξ</mi><mi>η</mi></mrow></msqrt><mo>,</mo><msub><mi>B</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\frac{2^{(1 - \xi)}}{\Gamma(\xi)} \alpha^\xi B_k(\alpha, \xi), \alpha = \sqrt{2\xi \eta}, B_k</annotation></semantics></math>
is Bessel-K with order
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ξ</mi><annotation encoding="application/x-tex">\xi</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ξ</mi><mo>∈</mo><mrow><mo stretchy="true" form="prefix">[</mo><mn>1</mn><mi>/</mi><mn>5</mn><mo>,</mo><mn>5</mn><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\xi \in [1/5, 5]</annotation></semantics></math>
</td>
</tr>
<tr class="odd">
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="monospace">"𝚌𝚊𝚞𝚌𝚑𝚢"</mtext><annotation encoding="application/x-tex">\texttt{"cauchy"}</annotation></semantics></math></td>
<td>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>+</mo><msup><mi>η</mi><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mi>ξ</mi></mrow></msup><annotation encoding="application/x-tex">(1 + \eta^2)^{-\xi}</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ξ</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\xi &gt; 0</annotation></semantics></math>
</td>
</tr>
<tr class="even">
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="monospace">"𝚙𝚎𝚡𝚙𝚘𝚗𝚎𝚗𝚝𝚒𝚊𝚕"</mtext><annotation encoding="application/x-tex">\texttt{"pexponential"}</annotation></semantics></math></td>
<td>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>e</mi><mi>x</mi><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo>−</mo><msup><mi>h</mi><mi>ξ</mi></msup><mi>/</mi><mi>ϕ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">exp(-h^\xi / \phi)</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ξ</mi><mo>∈</mo><mo stretchy="false" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>2</mn><mo stretchy="false" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">\xi \in (0, 2]</annotation></semantics></math>
</td>
</tr>
<tr class="odd">
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="monospace">"𝚗𝚘𝚗𝚎"</mtext><annotation encoding="application/x-tex">\texttt{"none"}</annotation></semantics></math></td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>0</mn><annotation encoding="application/x-tex">0</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</div>
<div class="section level4">
<h4 id="sec:estimation-lm">Model-fitting<a class="anchor" aria-label="anchor" href="#sec:estimation-lm"></a>
</h4>
<div class="section level5">
<h5 id="sec:estimation-lik-lm">Likelihood-based Estimation (<code>estmethod = "reml"</code> or
<code>estmethod = "ml"</code>)<a class="anchor" aria-label="anchor" href="#sec:estimation-lik-lm"></a>
</h5>
<p>Minus twice a profiled (by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛃</mi><annotation encoding="application/x-tex">\boldsymbol{\beta}</annotation></semantics></math>)
Gaussian log-likelihood is given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mn>2</mn><msub><mo>ℓ</mo><mi>p</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝛉</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">|</mo><mi>𝚺</mi><mo stretchy="true" form="postfix">|</mo></mrow><mo>+</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐲</mi><mo>−</mo><mi>𝐗</mi><mover><mi>𝛃</mi><mo accent="true">̃</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mi>⊤</mi></msup><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐲</mi><mo>−</mo><mi>𝐗</mi><mover><mi>𝛃</mi><mo accent="true">̃</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>n</mi><mo>ln</mo><mrow><mn>2</mn><mi>π</mi></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation}\label{eq:ml-lik}
  -2\ell_p(\boldsymbol{\theta}) = \ln{|\boldsymbol{\Sigma}|} + (\mathbf{y} - \mathbf{X} \tilde{\boldsymbol{\beta}})^\top \boldsymbol{\Sigma}^{-1} (\mathbf{y} - \mathbf{X} \tilde{\boldsymbol{\beta}}) + n \ln{2\pi},
\end{equation}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>𝛃</mi><mo accent="true">̃</mo></mover><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐗</mi><mi>⊤</mi></msup><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐗</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>𝐗</mi><mi>⊤</mi></msup><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐲</mi></mrow><annotation encoding="application/x-tex">\tilde{\boldsymbol{\beta}} = (\mathbf{X}^\top \mathbf{\Sigma}^{-1} \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{\Sigma}^{-1} \mathbf{y}</annotation></semantics></math>.
Minimizing this equation yields
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝛉</mi><mo accent="true">̂</mo></mover><mrow><mi>m</mi><mi>l</mi></mrow></msub><annotation encoding="application/x-tex">\boldsymbol{\hat{\theta}}_{ml}</annotation></semantics></math>,
the maximum likelihood estimates for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛉</mi><annotation encoding="application/x-tex">\boldsymbol{\theta}</annotation></semantics></math>.
Then a closed form solution exists for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mrow><mi>m</mi><mi>l</mi></mrow></msub><annotation encoding="application/x-tex">\boldsymbol{\hat{\beta}}_{ml}</annotation></semantics></math>,
the maximum likelihood estimates for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛃</mi><annotation encoding="application/x-tex">\boldsymbol{\beta}</annotation></semantics></math>:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mrow><mi>m</mi><mi>l</mi></mrow></msub><mo>=</mo><msub><mover><mi>𝛃</mi><mo accent="true">̃</mo></mover><mrow><mi>m</mi><mi>l</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\boldsymbol{\hat{\beta}}_{ml} = \tilde{\boldsymbol{\beta}}_{ml}</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝛃</mi><mo accent="true">̃</mo></mover><mrow><mi>m</mi><mi>l</mi></mrow></msub><annotation encoding="application/x-tex">\tilde{\boldsymbol{\beta}}_{ml}</annotation></semantics></math>
is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>𝛃</mi><mo accent="true">̃</mo></mover><annotation encoding="application/x-tex">\tilde{\boldsymbol{\beta}}</annotation></semantics></math>
evaluated at
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝛉</mi><mo accent="true">̂</mo></mover><mrow><mi>m</mi><mi>l</mi></mrow></msub><annotation encoding="application/x-tex">\boldsymbol{\hat{\theta}}_{ml}</annotation></semantics></math>.
To reduce bias in that variances of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mrow><mi>m</mi><mi>l</mi></mrow></msub><annotation encoding="application/x-tex">\boldsymbol{\hat{\beta}}_{ml}</annotation></semantics></math>
that can occur due to the simultaneous estimation of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛃</mi><annotation encoding="application/x-tex">\boldsymbol{\beta}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛉</mi><annotation encoding="application/x-tex">\boldsymbol{\theta}</annotation></semantics></math>,
restricted maximum likelihood estimation (REML) <span class="citation">(Patterson and Thompson 1971; Harville 1977; Wolfinger,
Tobias, and Sall 1994)</span> has been shown to be better than maximum
likelihood estimation. Integrating
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛃</mi><annotation encoding="application/x-tex">\boldsymbol{\beta}</annotation></semantics></math>
out of a Gaussian likelihood yields the restricted Gaussian likelihood.
Minus twice a restricted Gaussian log-likelihood is given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mn>2</mn><msub><mo>ℓ</mo><mi>R</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝛉</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>−</mo><mn>2</mn><msub><mo>ℓ</mo><mi>p</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝛉</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">|</mo><msup><mi>𝐗</mi><mi>⊤</mi></msup><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐗</mi><mo stretchy="true" form="postfix">|</mo></mrow><mo>−</mo><mi>p</mi><mo>ln</mo><mrow><mn>2</mn><mi>π</mi></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation}\label{eq:reml-lik}
  -2\ell_R(\boldsymbol{\theta}) = -2\ell_p(\boldsymbol{\theta})  + \ln{|\mathbf{X}^\top \boldsymbol{\Sigma}^{-1} \mathbf{X}|} - p \ln{2\pi} ,
\end{equation}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>
equals the dimension of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛃</mi><annotation encoding="application/x-tex">\boldsymbol{\beta}</annotation></semantics></math>.
Minimizing this equation yields
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝛉</mi><mo accent="true">̂</mo></mover><mrow><mi>r</mi><mi>e</mi><mi>m</mi><mi>l</mi></mrow></msub><annotation encoding="application/x-tex">\boldsymbol{\hat{\theta}}_{reml}</annotation></semantics></math>,
the restricted maximum likelihood estimates for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛉</mi><annotation encoding="application/x-tex">\boldsymbol{\theta}</annotation></semantics></math>.
Then a closed for solution exists for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mrow><mi>r</mi><mi>e</mi><mi>m</mi><mi>l</mi></mrow></msub><annotation encoding="application/x-tex">\boldsymbol{\hat{\beta}}_{reml}</annotation></semantics></math>,
the restricted maximum likelihood estimates for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛃</mi><annotation encoding="application/x-tex">\boldsymbol{\beta}</annotation></semantics></math>:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mrow><mi>r</mi><mi>e</mi><mi>m</mi><mi>l</mi></mrow></msub><mo>=</mo><msub><mover><mi>𝛃</mi><mo accent="true">̃</mo></mover><mrow><mi>r</mi><mi>e</mi><mi>m</mi><mi>l</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\boldsymbol{\hat{\beta}}_{reml} = \tilde{\boldsymbol{\beta}}_{reml}</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝛃</mi><mo accent="true">̃</mo></mover><mrow><mi>r</mi><mi>e</mi><mi>m</mi><mi>l</mi></mrow></msub><annotation encoding="application/x-tex">\tilde{\boldsymbol{\beta}}_{reml}</annotation></semantics></math>
is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>𝛃</mi><mo accent="true">̃</mo></mover><annotation encoding="application/x-tex">\tilde{\boldsymbol{\beta}}</annotation></semantics></math>
evaluated at
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝛉</mi><mo accent="true">̂</mo></mover><mrow><mi>r</mi><mi>e</mi><mi>m</mi><mi>l</mi></mrow></msub><annotation encoding="application/x-tex">\boldsymbol{\hat{\theta}}_{reml}</annotation></semantics></math>.</p>
<p>The covariance matrix can often be written as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝚺</mi><mo>=</mo><msup><mi>σ</mi><mn>2</mn></msup><msup><mi>𝚺</mi><mo>*</mo></msup></mrow><annotation encoding="application/x-tex">\boldsymbol{\Sigma} = \sigma^2 \boldsymbol{\Sigma}^*</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>σ</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\sigma^2</annotation></semantics></math>
is the overall variance and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>𝚺</mi><mo>*</mo></msup><annotation encoding="application/x-tex">\boldsymbol{\Sigma}^*</annotation></semantics></math>
is a covariance matrix that depends on parameter vector
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>𝛉</mi><mo>*</mo></msup><annotation encoding="application/x-tex">\boldsymbol{\theta}^*</annotation></semantics></math>
with one less dimension than
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛉</mi><annotation encoding="application/x-tex">\boldsymbol{\theta}</annotation></semantics></math>.
Then the overall variance,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>σ</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\sigma^2</annotation></semantics></math>,
can be profiled out of the previous likelihood equation. This reduces
the number of parameters requiring optimization by one, which can
dramatically reduce estimation time. Further profiling out
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>σ</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\sigma^2</annotation></semantics></math>
yields
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mn>2</mn><msubsup><mo>ℓ</mo><mi>p</mi><mo>*</mo></msubsup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝛉</mi><mo>*</mo></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">|</mo><msup><mi>𝚺</mi><mo mathvariant="bold">*</mo></msup><mo stretchy="true" form="postfix">|</mo></mrow><mo>+</mo><mi>n</mi><mo>ln</mo><mrow><mo stretchy="true" form="prefix">[</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐲</mi><mo>−</mo><mi>𝐗</mi><mover><mi>𝛃</mi><mo accent="true">̃</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mi>⊤</mi></msup><msup><mi>𝚺</mi><mrow><mo>*</mo><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐲</mi><mo>−</mo><mi>𝐗</mi><mover><mi>𝛃</mi><mo accent="true">̃</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mo>+</mo><mi>n</mi><mo>+</mo><mi>n</mi><mo>ln</mo><mrow><mn>2</mn><mi>π</mi><mi>/</mi><mi>n</mi></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}\label{eq:ml-plik}
  -2\ell_p^*(\boldsymbol{\theta}^*) = \ln{|\boldsymbol{\Sigma^*}|} + n\ln[(\mathbf{y} - \mathbf{X} \tilde{\boldsymbol{\beta}})^\top \boldsymbol{\Sigma}^{* -1} (\mathbf{y} - \mathbf{X} \tilde{\boldsymbol{\beta}})] + n + n\ln{2\pi / n}.
\end{equation*}</annotation></semantics></math> After finding
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mover><mi>𝛉</mi><mo accent="true">̂</mo></mover><mrow><mi>m</mi><mi>l</mi></mrow><mo>*</mo></msubsup><annotation encoding="application/x-tex">\hat{\boldsymbol{\theta}}^*_{ml}</annotation></semantics></math>,
a closed form solution for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mover><mi>σ</mi><mo accent="true">̂</mo></mover><mrow><mi>m</mi><mi>l</mi></mrow><mn>2</mn></msubsup><annotation encoding="application/x-tex">\hat{\sigma}^2_{ml}</annotation></semantics></math>
exists:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mover><mi>σ</mi><mo accent="true">̂</mo></mover><mrow><mi>m</mi><mi>l</mi></mrow><mn>2</mn></msubsup><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐲</mi><mo>−</mo><mi>𝐗</mi><mover><mi>𝛃</mi><mo accent="true">̃</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mi>⊤</mi></msup><msup><mi>𝚺</mi><mrow><mo>*</mo><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐲</mi><mo>−</mo><mi>𝐗</mi><mover><mi>𝛃</mi><mo accent="true">̃</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mi>/</mi><mi>n</mi></mrow><annotation encoding="application/x-tex">\hat{\sigma}^2_{ml} = [(\mathbf{y} - \mathbf{X} \boldsymbol{\tilde{\beta}})^\top \mathbf{\Sigma}^{* -1} (\mathbf{y} - \mathbf{X} \tilde{\boldsymbol{\beta}})] / n</annotation></semantics></math>.
Then
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mover><mi>𝛉</mi><mo accent="true">̂</mo></mover><mrow><mi>m</mi><mi>l</mi></mrow><mo>*</mo></msubsup><annotation encoding="application/x-tex">\boldsymbol{\hat{\theta}}^*_{ml}</annotation></semantics></math>
is combined with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mover><mi>σ</mi><mo accent="true">̂</mo></mover><mrow><mi>m</mi><mi>l</mi></mrow><mn>2</mn></msubsup><annotation encoding="application/x-tex">\hat{\sigma}^2_{ml}</annotation></semantics></math>
to yield
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝛉</mi><mo accent="true">̂</mo></mover><mrow><mi>m</mi><mi>l</mi></mrow></msub><annotation encoding="application/x-tex">\boldsymbol{\hat{\theta}}_{ml}</annotation></semantics></math>
and subsequently
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mrow><mi>m</mi><mi>l</mi></mrow></msub><annotation encoding="application/x-tex">\boldsymbol{\hat{\beta}}_{ml}</annotation></semantics></math>.
A similar result holds for restricted maximum likelihood estimation.
Further profiling out
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>σ</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\sigma^2</annotation></semantics></math>
yields
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mn>2</mn><msubsup><mo>ℓ</mo><mi>R</mi><mo>*</mo></msubsup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝚯</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">|</mo><msup><mi>𝚺</mi><mo>*</mo></msup><mo stretchy="true" form="postfix">|</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo>−</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>ln</mo><mrow><mo stretchy="true" form="prefix">[</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐲</mi><mo>−</mo><mi>𝐗</mi><mover><mi>𝛃</mi><mo accent="true">̃</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mi>⊤</mi></msup><msup><mi>𝚺</mi><mrow><mo>*</mo><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐲</mi><mo>−</mo><mi>𝐗</mi><mover><mi>𝛃</mi><mo accent="true">̃</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mo>+</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">|</mo><msup><mi>𝐗</mi><mi>⊤</mi></msup><msup><mi>𝚺</mi><mrow><mo>*</mo><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐗</mi><mo stretchy="true" form="postfix">|</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo>−</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo>−</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>ln</mo><mn>2</mn><mi>π</mi><mi>/</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo>−</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}\label{eq:reml-plik}
  -2\ell_R^*(\boldsymbol{\Theta}) = \ln{|\boldsymbol{\Sigma}^*|} + (n - p)\ln[(\mathbf{y} - \mathbf{X} \tilde{\boldsymbol{\beta}})^\top \boldsymbol{\Sigma}^{* -1} (\mathbf{y} - \mathbf{X} \tilde{\boldsymbol{\beta}})] + \ln{|\mathbf{X}^\top \boldsymbol{\Sigma}^{* -1} \mathbf{X}|} + (n - p) + (n - p)\ln2\pi / (n - p).
\end{equation*}</annotation></semantics></math> After finding
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mover><mi>𝛉</mi><mo accent="true">̂</mo></mover><mrow><mi>r</mi><mi>e</mi><mi>m</mi><mi>l</mi></mrow><mo>*</mo></msubsup><annotation encoding="application/x-tex">\hat{\boldsymbol{\theta}}^*_{reml}</annotation></semantics></math>,
a closed form solution for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mover><mi>σ</mi><mo accent="true">̂</mo></mover><mrow><mi>r</mi><mi>e</mi><mi>m</mi><mi>l</mi></mrow><mn>2</mn></msubsup><annotation encoding="application/x-tex">\hat{\sigma}^2_{reml}</annotation></semantics></math>
exists:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mover><mi>σ</mi><mo accent="true">̂</mo></mover><mrow><mi>r</mi><mi>e</mi><mi>m</mi><mi>l</mi></mrow><mn>2</mn></msubsup><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐲</mi><mo>−</mo><mi>𝐗</mi><mover><mi>𝛃</mi><mo accent="true">̃</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mi>⊤</mi></msup><msup><mi>𝚺</mi><mrow><mo>*</mo><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐲</mi><mo>−</mo><mi>𝐗</mi><mover><mi>𝛃</mi><mo accent="true">̃</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mi>/</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo>−</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\hat{\sigma}^2_{reml} = [(\mathbf{y} - \mathbf{X} \boldsymbol{\tilde{\beta}})^\top \mathbf{\Sigma}^{* -1} (\mathbf{y} - \mathbf{X} \tilde{\boldsymbol{\beta}})] / (n - p)</annotation></semantics></math>.
Then
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mover><mi>𝛉</mi><mo accent="true">̂</mo></mover><mrow><mi>r</mi><mi>e</mi><mi>m</mi><mi>l</mi></mrow><mo>*</mo></msubsup><annotation encoding="application/x-tex">\boldsymbol{\hat{\theta}}^*_{reml}</annotation></semantics></math>
is combined with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mover><mi>σ</mi><mo accent="true">̂</mo></mover><mrow><mi>r</mi><mi>e</mi><mi>m</mi><mi>l</mi></mrow><mn>2</mn></msubsup><annotation encoding="application/x-tex">\hat{\sigma}^2_{reml}</annotation></semantics></math>
to yield
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝛉</mi><mo accent="true">̂</mo></mover><mrow><mi>r</mi><mi>e</mi><mi>m</mi><mi>l</mi></mrow></msub><annotation encoding="application/x-tex">\boldsymbol{\hat{\theta}}_{reml}</annotation></semantics></math>
and subsequently
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mrow><mi>r</mi><mi>e</mi><mi>m</mi><mi>l</mi></mrow></msub><annotation encoding="application/x-tex">\boldsymbol{\hat{\beta}}_{reml}</annotation></semantics></math>.
For more on profiling Gaussian likelihoods, see <span class="citation">Wolfinger, Tobias, and Sall (1994)</span>.</p>
<p>Both maximum likelihood and restricted maximum likelihood estimation
rely on the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">n \times n</annotation></semantics></math>
covariance matrix inverse. Inverting an
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">n \times n</annotation></semantics></math>
matrix is an enormous computational demand that scales cubically with
the sample size. For this reason, maximum likelihood and restricted
maximum likelihood estimation have historically been infeasible to
implement in their standard form with data larger than a few thousand
observations. This motivates the use for big data approaches.</p>
</div>
<div class="section level5">
<h5 id="sec:estimation-sv-lm">Semivariogram-based Estimation (<code>splm()</code> only)<a class="anchor" aria-label="anchor" href="#sec:estimation-sv-lm"></a>
</h5>
<p>An alternative approach to likelihood-based estimation is
semivariogram-based estimation. The semivariogram of a constant-mean
process
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐲</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math>
is the expectation of half of the squared difference between two
observations
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation></semantics></math>
distance apart. More formally, the semivariogram is denoted
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\gamma(h)</annotation></semantics></math>
and defined as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">[</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mi>y</mi><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mo stretchy="true" form="postfix">]</mo></mrow><mi>/</mi><mn>2</mn><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}\label{eq:sv}
  \gamma(h) = \text{E}[(y_i - y_j)^2] / 2 ,
\end{equation*}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation></semantics></math>
is the Euclidean distance between the locations of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding="application/x-tex">y_i</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>y</mi><mi>j</mi></msub><annotation encoding="application/x-tex">y_j</annotation></semantics></math>.
When the process
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐲</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math>
is second-order stationary, the semivariogram and covariance function
are intimately connected:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>σ</mi><mn>2</mn></msup><mo>−</mo><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\gamma(h) = \sigma^2 - \text{Cov}(h)</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>σ</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\sigma^2</annotation></semantics></math>
is the overall variance and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\text{Cov}(h)</annotation></semantics></math>
is the covariance function evaluated at
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation></semantics></math>.
As such, the semivariogram and covariance function rely on the same
parameter vector
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛉</mi><annotation encoding="application/x-tex">\boldsymbol{\theta}</annotation></semantics></math>.
Both of the semivariogram approaches described next are more
computationally efficient than restricted maximum likelihood and maximum
likelihood estimation because the major computational burden of the
semivariogram approaches (calculations based on squared differences
among pairs) scales quadratically with the sample size (i.e., not the
cubed sample size like the likelihood-based approaches).</p>
<div class="section level6">
<h6 id="sec:estimation-sv-cwls-lm">Weighted Least Squares (<code>estmethod = "sv-wls"</code>)<a class="anchor" aria-label="anchor" href="#sec:estimation-sv-cwls-lm"></a>
</h6>
<p>The empirical semivariogram is a moment-based estimate of the
semivariogram denoted by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>γ</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\hat{\gamma}(h)</annotation></semantics></math>.
It is defined as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>γ</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mrow><mn>2</mn><mrow><mo stretchy="true" form="prefix">|</mo><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">|</mo></mrow></mrow></mfrac><munder><mo>∑</mo><mrow><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></munder><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mi>y</mi><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \hat{\gamma}(h) = \frac{1}{2|N(h)|} \sum_{N(h)} (y_i - y_j)^2, 
\end{equation*}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">N(h)</annotation></semantics></math>
is the set of observations in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐲</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math>
that are
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation></semantics></math>
distance units apart (distance classes) and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">|</mo><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">|</mo></mrow><annotation encoding="application/x-tex">|N(h)|</annotation></semantics></math>
is the cardinality of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">N(h)</annotation></semantics></math><span class="citation">(Cressie 1993)</span>. One criticism of the
empirical semivariogram is that distance bins and cutoffs tend to be
arbitrarily chosen (i.e., not chosen according to some statistical
criteria).</p>
<p><span class="citation">Cressie (1985)</span> proposed estimating
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛉</mi><annotation encoding="application/x-tex">\boldsymbol{\theta}</annotation></semantics></math>
by minimizing an objective function that involves
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\gamma(h)</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>γ</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\hat{\gamma}(h)</annotation></semantics></math>
and is based on a weighted least squares criterion. This criterion is
defined as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><munder><mo>∑</mo><mi>i</mi></munder><msub><mi>w</mi><mi>i</mi></msub><msup><mrow><mo stretchy="true" form="prefix">[</mo><mover><mi>γ</mi><mo accent="true">̂</mo></mover><msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>i</mi></msub><mo>−</mo><mi>γ</mi><msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>i</mi></msub><mo stretchy="true" form="postfix">]</mo></mrow><mn>2</mn></msup><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation}\label{eq:svwls}
  \sum_i w_i [\hat{\gamma}(h)_i - \gamma(h)_i]^2,
\end{equation}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>w</mi><mi>i</mi></msub><annotation encoding="application/x-tex">w_i</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>γ</mi><mo accent="true">̂</mo></mover><msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\hat{\gamma}(h)_i</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi><msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\gamma(h)_i</annotation></semantics></math>
are the weights, empirical semivariogram, and semivariogram for the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>th
distance class, respectively. Minimizing this loss function yields
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝛉</mi><mo accent="true">̂</mo></mover><mrow><mi>w</mi><mi>l</mi><mi>s</mi></mrow></msub><annotation encoding="application/x-tex">\boldsymbol{\hat{\theta}}_{wls}</annotation></semantics></math>,
the semivariogram weighted least squares estimate of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛉</mi><annotation encoding="application/x-tex">\boldsymbol{\theta}</annotation></semantics></math>.
After estimating
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛉</mi><annotation encoding="application/x-tex">\boldsymbol{\theta}</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛃</mi><annotation encoding="application/x-tex">\boldsymbol{\beta}</annotation></semantics></math>
estimates are constructed using (empirical) generalized least squares:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mrow><mi>w</mi><mi>l</mi><mi>s</mi></mrow></msub><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐗</mi><mi>⊤</mi></msup><msup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐗</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>𝐗</mi><mi>⊤</mi></msup><msup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐲</mi></mrow><annotation encoding="application/x-tex">\boldsymbol{\hat{\beta}}_{wls} = (\mathbf{X}^\top \hat{\mathbf{\Sigma}}^{-1} \mathbf{X})^{-1} \mathbf{X}^\top \hat{\mathbf{\Sigma}}^{-1} \mathbf{y}</annotation></semantics></math>.</p>
<p><span class="citation">Cressie (1985)</span> recommends setting
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>=</mo><mrow><mo stretchy="true" form="prefix">|</mo><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">|</mo></mrow><mi>/</mi><mi>γ</mi><msubsup><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>i</mi><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">w_i = |N(h)| / \gamma(h)_i^2</annotation></semantics></math>,
which gives more weight to distance classes with more observations
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">|</mo><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">|</mo></mrow><annotation encoding="application/x-tex">|N(h)|</annotation></semantics></math>)
and shorter distances
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mi>/</mi><mi>γ</mi><msubsup><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>i</mi><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">1 / \gamma(h)_i^2</annotation></semantics></math>).
The default in <code>spmodel</code> is to use these
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>w</mi><mi>i</mi></msub><annotation encoding="application/x-tex">w_i</annotation></semantics></math>,
known as Cressie weights, though several other options for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>w</mi><mi>i</mi></msub><annotation encoding="application/x-tex">w_i</annotation></semantics></math>
exist and are available via the <code>weights</code> argument. The
following table contains all
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>w</mi><mi>i</mi></msub><annotation encoding="application/x-tex">w_i</annotation></semantics></math>
available via the <code>weights</code> argument.</p>
<table class="table">
<caption>Table of values for the weights argument in splm() when
estmethod = “sv-wls”.</caption>
<colgroup>
<col width="36%">
<col width="31%">
<col width="31%">
</colgroup>
<thead><tr class="header">
<th>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>w</mi><mi>i</mi></msub><annotation encoding="application/x-tex">w_i</annotation></semantics></math>
Name</th>
<th>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>w</mi><mi>i</mi></msub><annotation encoding="application/x-tex">w_i</annotation></semantics></math>
Form</th>
<th><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="monospace">𝚠𝚎𝚒𝚐𝚑𝚝 =</mtext><annotation encoding="application/x-tex">\texttt{weight =}</annotation></semantics></math></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Cressie</td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">|</mo><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">|</mo></mrow><mi>/</mi><mi>γ</mi><msubsup><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>i</mi><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">|N(h)| / \gamma(h)_i^2</annotation></semantics></math></td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="monospace">"𝚌𝚛𝚎𝚜𝚜𝚒𝚎"</mtext><annotation encoding="application/x-tex">\texttt{"cressie"}</annotation></semantics></math></td>
</tr>
<tr class="even">
<td>Cressie (Denominator) Root</td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">|</mo><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">|</mo></mrow><mi>/</mi><mi>γ</mi><msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">|N(h)| / \gamma(h)_i</annotation></semantics></math></td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="monospace">"𝚌𝚛𝚎𝚜𝚜𝚒𝚎-𝚍𝚛"</mtext><annotation encoding="application/x-tex">\texttt{"cressie-dr"}</annotation></semantics></math></td>
</tr>
<tr class="odd">
<td>Cressie No Pairs</td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mi>/</mi><mi>γ</mi><msubsup><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>i</mi><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">1 / \gamma(h)_i^2</annotation></semantics></math></td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="monospace">"𝚌𝚛𝚎𝚜𝚜𝚒𝚎-𝚗𝚘𝚙𝚊𝚒𝚛𝚜"</mtext><annotation encoding="application/x-tex">\texttt{"cressie-nopairs"}</annotation></semantics></math></td>
</tr>
<tr class="even">
<td>Cressie (Denominator) Root No Pairs</td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mi>/</mi><mi>γ</mi><msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">1 / \gamma(h)_i</annotation></semantics></math></td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="monospace">"𝚌𝚛𝚎𝚜𝚜𝚒𝚎-𝚍𝚛-𝚗𝚘𝚙𝚊𝚒𝚛𝚜"</mtext><annotation encoding="application/x-tex">\texttt{"cressie-dr-nopairs"}</annotation></semantics></math></td>
</tr>
<tr class="odd">
<td>Pairs</td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">|</mo><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">|</mo></mrow><annotation encoding="application/x-tex">|N(h)|</annotation></semantics></math></td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="monospace">𝚙𝚊𝚒𝚛𝚜"</mtext><annotation encoding="application/x-tex">\texttt{pairs"}</annotation></semantics></math></td>
</tr>
<tr class="even">
<td>Pairs Inverse Distance</td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">|</mo><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">|</mo></mrow><mi>/</mi><msup><mi>h</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">|N(h)| / h^2</annotation></semantics></math></td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="monospace">"𝚙𝚊𝚒𝚛𝚜-𝚒𝚗𝚟𝚍"</mtext><annotation encoding="application/x-tex">\texttt{"pairs-invd"}</annotation></semantics></math></td>
</tr>
<tr class="odd">
<td>Pairs Inverse (Root) Distance</td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">|</mo><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">|</mo></mrow><mi>/</mi><mi>h</mi></mrow><annotation encoding="application/x-tex">|N(h)| / h</annotation></semantics></math></td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="monospace">"𝚙𝚊𝚒𝚛𝚜-𝚒𝚗𝚟𝚛𝚍"</mtext><annotation encoding="application/x-tex">\texttt{"pairs-invrd"}</annotation></semantics></math></td>
</tr>
<tr class="even">
<td>Ordinary Least Squares</td>
<td>1</td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="monospace">"𝚘𝚕𝚜"</mtext><annotation encoding="application/x-tex">\texttt{"ols"}</annotation></semantics></math></td>
</tr>
</tbody>
</table>
<p>The number of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">N(h)</annotation></semantics></math>
classes and the maximum distance for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation></semantics></math>
are specified by passing the <code>bins</code> and <code>cutoff</code>
arguments to <code><a href="../reference/splm.html">splm()</a></code> (these arguments are passed via
<code>...</code> to <code><a href="../reference/esv.html">esv()</a></code>). The default value for
<code>bins</code> is 15 and the default value for <code>cutoff</code> is
half the maximum distance of the spatial domain’s bounding box.</p>
<p>Recall that the semivariogram is defined for a constant-mean process.
Generally,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐲</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math>
does not necessarily have a constant mean so the empirical semivariogram
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝛉</mi><mo accent="true">̂</mo></mover><mrow><mi>w</mi><mi>l</mi><mi>s</mi></mrow></msub><annotation encoding="application/x-tex">\boldsymbol{\hat{\theta}}_{wls}</annotation></semantics></math>
are typically constructed using the residuals from an ordinary least
squares regression of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐲</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math>
on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐗</mi><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics></math>.
These ordinary least squares residuals are assumed to have mean
zero.</p>
</div>
<div class="section level6">
<h6 id="sec:estimation-sv-cl-lm">Composite Likelihood (<code>estmethod = "sv-cl"</code>)<a class="anchor" aria-label="anchor" href="#sec:estimation-sv-cl-lm"></a>
</h6>
<p>Composite likelihood approaches involve constructing likelihoods
based on conditional or marginal events for which likelihoods are
available and then adding together these individual components.
Composite likelihoods are attractive because they behave very similar to
likelihoods but are easier to handle, both from a theoretical and from a
computational perspective. <span class="citation">Curriero and Lele
(1999)</span> derive a particular composite likelihood for estimating
semivariogram parameters. The negative log of this composite likelihood,
denoted
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">CL</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\text{CL}(h)</annotation></semantics></math>,
is given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">CL</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></munderover><munder><mo>∑</mo><mrow><mi>j</mi><mo>&gt;</mo><mi>i</mi></mrow></munder><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mi>y</mi><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mrow><mn>2</mn><mi>γ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mo>+</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>γ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\begin{equation}\label{eq:svcl}
  \text{CL}(h) = \sum_{i = 1}^{n - 1} \sum_{j &gt; i} \left( \frac{(y_i - y_j)^2}{2\gamma(h)} + \ln(\gamma(h)) \right)
\end{equation}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\gamma(h)</annotation></semantics></math>
is the semivariogram. Minimizing this loss function yields
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝛉</mi><mo accent="true">̂</mo></mover><mrow><mi>c</mi><mi>l</mi></mrow></msub><annotation encoding="application/x-tex">\boldsymbol{\hat{\theta}}_{cl}</annotation></semantics></math>,
the semivariogram composite likelihood estimates of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛉</mi><annotation encoding="application/x-tex">\boldsymbol{\theta}</annotation></semantics></math>.
After estimating
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛉</mi><annotation encoding="application/x-tex">\boldsymbol{\theta}</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛃</mi><annotation encoding="application/x-tex">\boldsymbol{\beta}</annotation></semantics></math>
estimates are constructed using (empirical) generalized least squares:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mrow><mi>c</mi><mi>l</mi></mrow></msub><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐗</mi><mi>⊤</mi></msup><msup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐗</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>𝐗</mi><mi>⊤</mi></msup><msup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐲</mi></mrow><annotation encoding="application/x-tex">\boldsymbol{\hat{\beta}}_{cl} = (\mathbf{X}^\top \hat{\mathbf{\Sigma}}^{-1} \mathbf{X})^{-1} \mathbf{X}^\top \hat{\mathbf{\Sigma}}^{-1} \mathbf{y}</annotation></semantics></math>.</p>
<p>An advantage of the composite likelihood approach to semivariogram
estimation is that it does not require arbitrarily specifying empirical
semivariogram bins and cutoffs. It does tend to be more computationally
demanding than weighted least squares, however. The composite likelihood
is constructed from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mfrac linethickness="0"><mi>n</mi><mn>2</mn></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\binom{n}{2}</annotation></semantics></math>
pairs for a sample size
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>,
whereas the weighted least squares approach only requires calculating
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mfrac linethickness="0"><mrow><mo stretchy="true" form="prefix">|</mo><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">|</mo></mrow><mn>2</mn></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\binom{|N(h)|}{2}</annotation></semantics></math>
pairs for each distance bin
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">N(h)</annotation></semantics></math>.
As with the weighted least squares approach, the composite likelihood
approach requires a constant-mean process, so typically the residuals
from an ordinary least squares regression of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐲</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math>
on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐗</mi><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics></math>
are used to estimate
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛉</mi><annotation encoding="application/x-tex">\boldsymbol{\theta}</annotation></semantics></math>.</p>
</div>
</div>
</div>
<div class="section level4">
<h4 id="sec:optim-lm">Optimization<a class="anchor" aria-label="anchor" href="#sec:optim-lm"></a>
</h4>
<p>Parameter estimation is performed using <code><a href="https://rdrr.io/r/stats/optim.html" class="external-link">stats::optim()</a></code>.
The default estimation method is Nelder-Mead <span class="citation">(Nelder and Mead 1965)</span> and the stopping
criterion is a relative convergence tolerance (<code>reltol</code>) of
.0001. If only one parameter requires estimation (on the profiled scale
if relevant), the Brent algorithm is instead used <span class="citation">(Brent 1971)</span>. Arguments to <code><a href="https://rdrr.io/r/stats/optim.html" class="external-link">optim()</a></code>
are passed via <code>...</code> to <code><a href="../reference/splm.html">splm()</a></code> and
<code><a href="../reference/spautor.html">spautor()</a></code>. For example, the default estimation method and
convergence criteria are overridden by passing <code>method</code> and
<code>control</code>, respectively, to <code><a href="../reference/splm.html">splm()</a></code> and
<code><a href="../reference/spautor.html">spautor()</a></code>. If the <code>lower</code> and <code>upper</code>
arguments to <code><a href="https://rdrr.io/r/stats/optim.html" class="external-link">optim()</a></code> are specified in <code><a href="../reference/splm.html">splm()</a></code>
and <code><a href="../reference/spautor.html">spautor()</a></code> to be passed to <code><a href="https://rdrr.io/r/stats/optim.html" class="external-link">optim()</a></code>, they
are ignored, as optimization for all parameters is generally
unconstrained. Initial values for <code><a href="https://rdrr.io/r/stats/optim.html" class="external-link">optim()</a></code> are found using
the grid search described next.</p>
<div class="section level5">
<h5 id="sec:grid-lm">Grid Search<a class="anchor" aria-label="anchor" href="#sec:grid-lm"></a>
</h5>
<p><code>spmodel</code> uses a grid search to find suitable initial
values for use in optimization. For spatial linear models without random
effects, the spatially dependent variance
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>σ</mi><mrow><mi>d</mi><mi>e</mi></mrow><mn>2</mn></msubsup><annotation encoding="application/x-tex">\sigma^2_{de}</annotation></semantics></math>)
and spatially independent variance
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>σ</mi><mrow><mi>i</mi><mi>e</mi></mrow><mn>2</mn></msubsup><annotation encoding="application/x-tex">\sigma^2_{ie}</annotation></semantics></math>)
parameters are given “low”, “medium”, and “high” values. The sample
variance of a non-spatial linear model is slightly inflated by a factor
of 1.2 (non-spatial models can underestimate the variance when there is
spatial dependence) and these “low”, “medium”, and “high” values
correspond to 10%, 50%, and 90% of the inflated sample variance. Only
combinations of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>σ</mi><mrow><mi>d</mi><mi>e</mi></mrow><mn>2</mn></msubsup><annotation encoding="application/x-tex">\sigma^2_{de}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>σ</mi><mrow><mi>i</mi><mi>e</mi></mrow><mn>2</mn></msubsup><annotation encoding="application/x-tex">\sigma^2_{ie}</annotation></semantics></math>
whose proportions sum to 100% are considered. The range
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ϕ</mi><annotation encoding="application/x-tex">\phi</annotation></semantics></math>)
and extra
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ξ</mi><annotation encoding="application/x-tex">\xi</annotation></semantics></math>)
parameters are given “low” and “high” values that are unique to each
spatial covariance function. For example, when using an exponential
covariance function, the “low” value of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ϕ</mi><annotation encoding="application/x-tex">\phi</annotation></semantics></math>
is one-half the diagonal of the domain’s bounding box divided by three.
This particular value is chosen so that the effective range (the
distance at which the covariance is approximately zero), which equals
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mi>ϕ</mi></mrow><annotation encoding="application/x-tex">3\phi</annotation></semantics></math>
for the exponential covariance function, is is reached at one-half the
diagonal of the domain’s bounding box. Analogously, the “high” value of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ϕ</mi><annotation encoding="application/x-tex">\phi</annotation></semantics></math>
is three-halves the diagonal of the domain’s bounding box divided by
three. The anisotropy rotation parameter
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>)
is given six values that correspond to 0,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi><mi>/</mi><mn>6</mn></mrow><annotation encoding="application/x-tex">\pi/6</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi>π</mi><mi>/</mi><mn>6</mn></mrow><annotation encoding="application/x-tex">2\pi/6</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn><mi>π</mi><mi>/</mi><mn>6</mn></mrow><annotation encoding="application/x-tex">4\pi/6</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>5</mn><mi>π</mi><mi>/</mi><mn>6</mn></mrow><annotation encoding="application/x-tex">5\pi/6</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>π</mi><annotation encoding="application/x-tex">\pi</annotation></semantics></math>
radians. The anisotropy scale parameter
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>)
is given “low”, “medium”, and “high” values that correspond to scaling
factors of 0.25, 0.75, and 1. Note that the anisotropy parameters are
only used during grid searches for point-referenced data.</p>
<p>The crossing of all appropriate parameter values is considered. If
initial values are used for a parameter, the initial value replaces all
values of the parameter in this crossing. Duplicate crossings are then
omitted. The parameter configuration that yields the smallest value of
the objective function is then used as an initial value for
optimization. Suppose the inflated sample variance is 10, the
exponential covariance is used assuming isotropy, and the diagonal of
the bounding box is 180 distance units. The parameter configurations
evaluated are shown in the following table.</p>
<table class="table">
<caption>Grid search parameter configurations for an isotropic
exponential spatial covariance with inflated sample variance equal to 10
and diagonal of the bounding box equal to 180 distance units.</caption>
<thead><tr class="header">
<th><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>σ</mi><mrow><mi>d</mi><mi>e</mi></mrow><mn>2</mn></msubsup><annotation encoding="application/x-tex">\sigma^2_{de}</annotation></semantics></math></th>
<th><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>σ</mi><mrow><mi>i</mi><mi>e</mi></mrow><mn>2</mn></msubsup><annotation encoding="application/x-tex">\sigma^2_{ie}</annotation></semantics></math></th>
<th><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ϕ</mi><annotation encoding="application/x-tex">\phi</annotation></semantics></math></th>
<th><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></th>
<th><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>9</td>
<td>1</td>
<td>15</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="even">
<td>1</td>
<td>9</td>
<td>15</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="odd">
<td>5</td>
<td>5</td>
<td>15</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="even">
<td>9</td>
<td>1</td>
<td>45</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="odd">
<td>1</td>
<td>9</td>
<td>45</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="even">
<td>5</td>
<td>5</td>
<td>15</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>For spatial linear models with random effects, the same approach is
used to create a crossing of spatial covariance parameters. A separate
approach is used to create a set of random effect variances. The random
effect variances are similarly first grouped by proportions. The first
combination is such that the first random effect variance is given 90%
of variance, and the remaining 10% is spread out evenly among the
remaining random effect variances. The second combination is such that
the second random effect variance is given 90% of the variance, and the
remaining 10% is spread out evenly among the remaining random effect
variances. And so on and so forth. These combinations ascertain whether
one random effect dominates variability. A final grouping is lastly
considered: all 100% of variance is spread out evenly among all random
effects.</p>
<p>When finding parameter values
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>σ</mi><mrow><mi>d</mi><mi>e</mi></mrow><mn>2</mn></msubsup><annotation encoding="application/x-tex">\sigma^2_{de}</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>σ</mi><mrow><mi>i</mi><mi>e</mi></mrow><mn>2</mn></msubsup><annotation encoding="application/x-tex">\sigma^2_{ie}</annotation></semantics></math>,
and the random effect variances
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>σ</mi><msub><mi>u</mi><mi>i</mi></msub><mn>2</mn></msubsup><annotation encoding="application/x-tex">\sigma^2_{u_i}</annotation></semantics></math>
for the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>th
random effect), three scenarios are considered. In the first scenario,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>σ</mi><mrow><mi>d</mi><mi>e</mi></mrow><mn>2</mn></msubsup><annotation encoding="application/x-tex">\sigma^2_{de}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>σ</mi><mrow><mi>i</mi><mi>e</mi></mrow><mn>2</mn></msubsup><annotation encoding="application/x-tex">\sigma^2_{ie}</annotation></semantics></math>
get 90% of the inflated sample variance and the random effect variances
get 10%. In this scenario, only the random effect grouping where the
variance is evenly spread out is considered. This is because the random
effect variances are already contributing little to the overall
variability, so performing additional objective function evaluations is
unnecessary. In the second scenario, the random effects get 90% of the
inflated sample variances and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>σ</mi><mrow><mi>d</mi><mi>e</mi></mrow><mn>2</mn></msubsup><annotation encoding="application/x-tex">\sigma^2_{de}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>σ</mi><mrow><mi>i</mi><mi>e</mi></mrow><mn>2</mn></msubsup><annotation encoding="application/x-tex">\sigma^2_{ie}</annotation></semantics></math>
get 10%. Similarly in this scenario, only the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>σ</mi><mrow><mi>d</mi><mi>e</mi></mrow><mn>2</mn></msubsup><annotation encoding="application/x-tex">\sigma^2_{de}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>σ</mi><mrow><mi>i</mi><mi>e</mi></mrow><mn>2</mn></msubsup><annotation encoding="application/x-tex">\sigma^2_{ie}</annotation></semantics></math>
grouping where the variance is evenly spread out is considered. Also in
this scenario, only the lowest value for <code>range</code> and
<code>extra</code> are used. In the third scenario, the 50% of the
inflated sample variance is given to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>σ</mi><mrow><mi>d</mi><mi>e</mi></mrow><mn>2</mn></msubsup><annotation encoding="application/x-tex">\sigma^2_{de}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>σ</mi><mrow><mi>i</mi><mi>e</mi></mrow><mn>2</mn></msubsup><annotation encoding="application/x-tex">\sigma^2_{ie}</annotation></semantics></math>
and 50% to the random effects. In this scenario, the only parameter
combination considered is the case where variances are evenly spread out
among
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>σ</mi><mrow><mi>d</mi><mi>e</mi></mrow><mn>2</mn></msubsup><annotation encoding="application/x-tex">\sigma^2_{de}</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>σ</mi><mrow><mi>i</mi><mi>e</mi></mrow><mn>2</mn></msubsup><annotation encoding="application/x-tex">\sigma^2_{ie}</annotation></semantics></math>,
and the random effect variances. Together, there are parameter
configurations where the spatial variability dominates (scenario 1), the
random variability dominates (scenario 2), and where there is an even
contribution from spatial and random variability. The parameter
configuration that minimizes the objective function is then used as an
initial value for optimization. Recall that random effects are only used
with restricted maximum likelihood or maximum likelihood estimation, so
the objective function is always a likelihood.</p>
<p>Suppose the inflated sample variance is 10, the exponential
covariance is used assuming isotropy, the diagonal of the bounding box
is 180 distance units, and there are two random effects. The parameter
configurations evaluated are shown in the following table.</p>
<table style="width:100%;" class="table">
<caption>Grid search parameter configurations for an isotropic
exponential spatial covariance with two random effects, inflated sample
variance equal to 10, and diagonal of the bounding box equal to 180
distance units.</caption>
<colgroup>
<col width="14%">
<col width="14%">
<col width="14%">
<col width="14%">
<col width="14%">
<col width="14%">
<col width="14%">
</colgroup>
<thead><tr class="header">
<th><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>σ</mi><mrow><mi>d</mi><mi>e</mi></mrow><mn>2</mn></msubsup><annotation encoding="application/x-tex">\sigma^2_{de}</annotation></semantics></math></th>
<th><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>σ</mi><mrow><mi>i</mi><mi>e</mi></mrow><mn>2</mn></msubsup><annotation encoding="application/x-tex">\sigma^2_{ie}</annotation></semantics></math></th>
<th><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ϕ</mi><annotation encoding="application/x-tex">\phi</annotation></semantics></math></th>
<th><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></th>
<th><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math></th>
<th><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>σ</mi><mrow><mi>u</mi><mn>1</mn></mrow><mn>2</mn></msubsup><annotation encoding="application/x-tex">\sigma^2_{u1}</annotation></semantics></math></th>
<th><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>σ</mi><mrow><mi>u</mi><mn>2</mn></mrow><mn>2</mn></msubsup><annotation encoding="application/x-tex">\sigma^2_{u2}</annotation></semantics></math></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>8.1</td>
<td>0.9</td>
<td>15</td>
<td>0</td>
<td>1</td>
<td>0.5</td>
<td>0.5</td>
</tr>
<tr class="even">
<td>0.9</td>
<td>8.1</td>
<td>15</td>
<td>0</td>
<td>1</td>
<td>0.5</td>
<td>0.5</td>
</tr>
<tr class="odd">
<td>4.5</td>
<td>4.5</td>
<td>15</td>
<td>0</td>
<td>1</td>
<td>0.5</td>
<td>0.5</td>
</tr>
<tr class="even">
<td>8.1</td>
<td>0.9</td>
<td>45</td>
<td>0</td>
<td>1</td>
<td>0.5</td>
<td>0.5</td>
</tr>
<tr class="odd">
<td>0.9</td>
<td>8.1</td>
<td>45</td>
<td>0</td>
<td>1</td>
<td>0.5</td>
<td>0.5</td>
</tr>
<tr class="even">
<td>4.5</td>
<td>4.5</td>
<td>45</td>
<td>0</td>
<td>1</td>
<td>0.5</td>
<td>0.5</td>
</tr>
<tr class="odd">
<td>0.5</td>
<td>0.5</td>
<td>15</td>
<td>0</td>
<td>1</td>
<td>8.1</td>
<td>0.9</td>
</tr>
<tr class="even">
<td>0.5</td>
<td>0.5</td>
<td>15</td>
<td>0</td>
<td>1</td>
<td>0.9</td>
<td>8.1</td>
</tr>
<tr class="odd">
<td>0.5</td>
<td>0.5</td>
<td>15</td>
<td>0</td>
<td>1</td>
<td>4.5</td>
<td>4.5</td>
</tr>
<tr class="even">
<td>2.5</td>
<td>2.5</td>
<td>15</td>
<td>0</td>
<td>1</td>
<td>2.5</td>
<td>2.5</td>
</tr>
<tr class="odd">
<td>2.5</td>
<td>2.5</td>
<td>45</td>
<td>0</td>
<td>1</td>
<td>2.5</td>
<td>2.5</td>
</tr>
</tbody>
</table>
<p>This grid search approach balances a thorough exploration of the
parameter space with computational efficiency, as each objective
function evaluation can be computationally expensive.</p>
</div>
</div>
<div class="section level4">
<h4 id="sec:testing-lm">Hypothesis Testing<a class="anchor" aria-label="anchor" href="#sec:testing-lm"></a>
</h4>
<p>The hypothesis tests for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat{\boldsymbol{\beta}}</annotation></semantics></math>
returned by <code><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary()</a></code> or <code><a href="https://generics.r-lib.org/reference/tidy.html" class="external-link">tidy()</a></code> of an
<code>splm</code> or <code>spautor</code> object are asymptotic z-tests
based on the normal (Gaussian) distribution (Wald tests). The null
hypothesis for the test associated with each
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>β</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><annotation encoding="application/x-tex">\hat{\beta}_i</annotation></semantics></math>
is that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>β</mi><mi>i</mi></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\beta_i = 0</annotation></semantics></math>.
Then the test statistic is given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>z</mi><mo accent="true">̃</mo></mover><mo>=</mo><mfrac><msub><mover><mi>β</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><mrow><mtext mathvariant="normal">SE</mtext><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>β</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \tilde{z} = \frac{\hat{\beta}_i}{\text{SE}(\hat{\beta}_i)},
\end{equation*}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">SE</mtext><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>β</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\text{SE}(\hat{\beta}_i)</annotation></semantics></math>
is the standard error of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>β</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><annotation encoding="application/x-tex">\hat{\beta}_i</annotation></semantics></math>,
which equals the square root of the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>th
diagonal element of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐗</mi><mi>⊤</mi></msup><msup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐗</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">(\mathbf{X}^\top \hat{\boldsymbol{\Sigma}}^{-1} \mathbf{X})^{-1}</annotation></semantics></math>.
The p-value is given by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>*</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>Φ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mo stretchy="true" form="prefix">|</mo><mover><mi>z</mi><mo accent="true">̃</mo></mover><mo stretchy="true" form="postfix">|</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">2 * (1 - \Phi(|\tilde{z}|))</annotation></semantics></math>,
which corresponds to an equal-tailed, two-sided hypothesis test of level
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Φ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo>⋅</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Phi(\cdot)</annotation></semantics></math>
denotes the standard normal (Gaussian) cumulative distribution function
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">|</mo><mo>⋅</mo><mo stretchy="true" form="postfix">|</mo></mrow><annotation encoding="application/x-tex">|\cdot|</annotation></semantics></math>
denotes the absolute value.</p>
</div>
<div class="section level4">
<h4 id="sec:random-lm">Random Effects (<code>splm()</code> only and <code>"reml"</code> or
<code>"ml"</code> <code>estmethod</code> only)<a class="anchor" aria-label="anchor" href="#sec:random-lm"></a>
</h4>
<p>The random effects contribute directly to the covariance through
their design matrices. Let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐮</mi><annotation encoding="application/x-tex">\mathbf{u}</annotation></semantics></math>
be a mean-zero random effect column vector of length
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>n</mi><mi>u</mi></msub><annotation encoding="application/x-tex">n_u</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>n</mi><mi>u</mi></msub><annotation encoding="application/x-tex">n_u</annotation></semantics></math>
is the number of levels of the random effect, with design matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐙</mi><mi>u</mi></msub><annotation encoding="application/x-tex">\mathbf{Z}_u</annotation></semantics></math>.
Then
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐙</mi><mi>u</mi></msub><mi>𝐮</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mi>𝐙</mi><mi>u</mi></msub><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐮</mi><mo stretchy="true" form="postfix">)</mo></mrow><msubsup><mi>𝐙</mi><mi>u</mi><mi>⊤</mi></msubsup></mrow><annotation encoding="application/x-tex">\text{Cov}(\mathbf{Z}_u\mathbf{u}) = \mathbf{Z}_u \text{Cov}(\mathbf{u})\mathbf{Z}_u^\top</annotation></semantics></math>.
Because each element of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐮</mi><annotation encoding="application/x-tex">\mathbf{u}</annotation></semantics></math>
is independent of one another, this reduces to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐙</mi><mi>u</mi></msub><mi>𝐮</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msubsup><mi>σ</mi><mi>u</mi><mn>2</mn></msubsup><msub><mi>𝐙</mi><mi>u</mi></msub><msubsup><mi>𝐙</mi><mi>u</mi><mi>⊤</mi></msubsup></mrow><annotation encoding="application/x-tex">\text{Cov}(\mathbf{Z}_u\mathbf{u}) = \sigma^2_u \mathbf{Z}_u \mathbf{Z}_u^\top</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>σ</mi><mi>u</mi><mn>2</mn></msubsup><annotation encoding="application/x-tex">\sigma^2_u</annotation></semantics></math>
is the variance parameter corresponding to the random effect (i.e., the
random effect variance parameter).</p>
<p>The
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐙</mi><annotation encoding="application/x-tex">\mathbf{Z}</annotation></semantics></math>
matrices index the levels of the random effect.
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐙</mi><annotation encoding="application/x-tex">\mathbf{Z}</annotation></semantics></math>
has dimension
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><msub><mi>n</mi><mi>u</mi></msub></mrow><annotation encoding="application/x-tex">n \times n_u</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
is the sample size. Each row of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐙</mi><annotation encoding="application/x-tex">\mathbf{Z}</annotation></semantics></math>
corresponds to an observation and each column to a level of the random
effect. For example, suppose we have
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">n = 4</annotation></semantics></math>
observations, so
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐲</mi><mo>=</mo><mo stretchy="false" form="prefix">{</mo><msub><mi>y</mi><mn>1</mn></msub><mo>,</mo><msub><mi>y</mi><mn>2</mn></msub><mo>,</mo><msub><mi>y</mi><mn>3</mn></msub><mo>,</mo><msub><mi>y</mi><mn>4</mn></msub><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">\mathbf{y} = \{y_1, y_2, y_3, y_4\}</annotation></semantics></math>.
Also suppose that the random effect
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐮</mi><annotation encoding="application/x-tex">\mathbf{u}</annotation></semantics></math>
has two levels and that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>y</mi><mn>1</mn></msub><annotation encoding="application/x-tex">y_1</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>y</mi><mn>4</mn></msub><annotation encoding="application/x-tex">y_4</annotation></semantics></math>
are in the first level and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>y</mi><mn>2</mn></msub><annotation encoding="application/x-tex">y_2</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>y</mi><mn>3</mn></msub><annotation encoding="application/x-tex">y_3</annotation></semantics></math>
are in the second level. For random intercepts, each element of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐙</mi><annotation encoding="application/x-tex">\mathbf{Z}</annotation></semantics></math>
is one if the observation is in the appropriate level of the random
effect and zero otherwise. So it follows that
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐙</mi><mi>𝐮</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>u</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>u</mi><mn>2</mn></msub></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
\mathbf{Z}\mathbf{u} = 
\begin{bmatrix}
1 &amp; 0 \\
0 &amp; 1 \\
0 &amp; 1 \\
1 &amp; 0
\end{bmatrix}
\begin{bmatrix}
u_1 \\
u_2
\end{bmatrix},
\end{equation*}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>u</mi><mn>1</mn></msub><annotation encoding="application/x-tex">u_1</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>u</mi><mn>2</mn></msub><annotation encoding="application/x-tex">u_2</annotation></semantics></math>
are the random intercepts for the first and second levels of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐮</mi><annotation encoding="application/x-tex">\mathbf{u}</annotation></semantics></math>,
respectively. For random slopes, each element of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐙</mi><annotation encoding="application/x-tex">\mathbf{Z}</annotation></semantics></math>
equals the value of an auxiliary variable,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐤</mi><annotation encoding="application/x-tex">\mathbf{k}</annotation></semantics></math>,
if the observation is in the appropriate level of the random effect and
zero otherwise. So if
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐤</mi><mo>=</mo><mo stretchy="false" form="prefix">{</mo><mn>2</mn><mo>,</mo><mn>7</mn><mo>,</mo><mn>5</mn><mo>,</mo><mn>4</mn><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">\mathbf{k} = \{2, 7, 5, 4 \}</annotation></semantics></math>
it follows that
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐙</mi><mi>𝐮</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>2</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>7</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>5</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>4</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>u</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>u</mi><mn>2</mn></msub></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
\mathbf{Z}\mathbf{u} = 
\begin{bmatrix}
2 &amp; 0 \\
0 &amp; 7 \\
0 &amp; 5 \\
4 &amp; 0
\end{bmatrix}
\begin{bmatrix}
u_1 \\
u_2
\end{bmatrix},
\end{equation*}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>u</mi><mn>1</mn></msub><annotation encoding="application/x-tex">u_1</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>u</mi><mn>2</mn></msub><annotation encoding="application/x-tex">u_2</annotation></semantics></math>
are the random slopes for the first and second levels of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐮</mi><annotation encoding="application/x-tex">\mathbf{u}</annotation></semantics></math>,
respectively. If a random slope is included in the model, it is common
for the auxiliary variable to be a column in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐗</mi><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics></math>,
the fixed effects design matrix (i.e., also a fixed effect). Denote this
column as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐱</mi><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math>.
Here
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛃</mi><annotation encoding="application/x-tex">\boldsymbol{\beta}</annotation></semantics></math>
captures the average effect of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐱</mi><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math>
on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐲</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math>
(accounting for other explanatory variables) and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐮</mi><annotation encoding="application/x-tex">\mathbf{u}</annotation></semantics></math>
captures a subject-specific effect of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐱</mi><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math>
on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐲</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math>.
So for a subject in the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>th
level of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐮</mi><annotation encoding="application/x-tex">\mathbf{u}</annotation></semantics></math>,
the average increase in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>
associated with a one-unit increase
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>
is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi><mo>+</mo><msub><mi>u</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\beta + u_i</annotation></semantics></math>.</p>
<p>The <code>sv-wls</code> and <code>sv-cl</code> estimation methods do
not use a likelihood, and thus, they do not allow for the estimation of
random effects in <code>spmodel</code>.</p>
</div>
<div class="section level4">
<h4 id="sec:anisotropy-lm">Anisotropy (<code>splm()</code> only)<a class="anchor" aria-label="anchor" href="#sec:anisotropy-lm"></a>
</h4>
<p>An isotropic spatial covariance function behaves similarly in all
directions (i.e., is independent of direction) as a function of
distance. An anisotropic spatial covariance function does not behave
similarly in all directions as a function of distance. The following
figure shows ellipses for an isotropic and anisotropic spatial
covariance function centered at the origin (a distance of zero). The
black outline of each ellipse is a level curve of equal correlation. The
left ellipse (a circle) represents an isotropic covariance function. The
distance at which the correlation between two observations lays on the
level curve is the same in all directions. The right ellipse represents
an anisotropic covariance function. The distance at which the
correlation between two observations lays on the level curve is
different in different directions.</p>
<div class="figure">
<img src="technical_files/figure-html/anisotropy-1.png" alt="In the left figure, the ellipse of an isotropic spatial covariance function centered at the origin is shown. In the right figure, the ellipse of an anisotropic spatial covariance function centered at the origin is shown. The black outline of each ellipse is a level curve of equal correlation. " width="50%"><img src="technical_files/figure-html/anisotropy-2.png" alt="In the left figure, the ellipse of an isotropic spatial covariance function centered at the origin is shown. In the right figure, the ellipse of an anisotropic spatial covariance function centered at the origin is shown. The black outline of each ellipse is a level curve of equal correlation. " width="50%"><p class="caption">
In the left figure, the ellipse of an isotropic spatial covariance
function centered at the origin is shown. In the right figure, the
ellipse of an anisotropic spatial covariance function centered at the
origin is shown. The black outline of each ellipse is a level curve of
equal correlation.
</p>
</div>
<p>To accommodate spatial anisotropy, the original coordinates must be
transformed such that the transformed coordinates yield an isotropic
spatial covariance. This transformation involves a rotation and a
scaling. Consider a set of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>
coordinates that should be transformed into
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>x</mi><mo>*</mo></msup><annotation encoding="application/x-tex">x^*</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>y</mi><mo>*</mo></msup><annotation encoding="application/x-tex">y^*</annotation></semantics></math>
coordinates. This transformation is formally defined as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><msup><mi>x</mi><mo>*</mo></msup></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msup><mi>y</mi><mo>*</mo></msup></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>1</mn><mi>/</mi><mi>S</mi></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mo>cos</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>α</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="center" style="text-align: center"><mo>sin</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>α</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mo>−</mo><mo>sin</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>α</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="center" style="text-align: center"><mo>cos</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>α</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mi>x</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>y</mi></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \begin{bmatrix}
    x^* \\
    y^*
  \end{bmatrix} = 
  \begin{bmatrix}
    1 &amp; 0 \\
    0 &amp; 1 / S
  \end{bmatrix}
  \begin{bmatrix}
    \cos(\alpha) &amp; \sin(\alpha) \\
    -\sin(\alpha) &amp; \cos(\alpha)
  \end{bmatrix}  
  \begin{bmatrix}
    x \\
    y
  \end{bmatrix}.
\end{equation*}</annotation></semantics></math> The original coordinates
are first multiplied by the rotation matrix, which rotates the
coordinates clockwise by angle
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>.
They are then multiplied by the scaling matrix, which scales the minor
axis of the spatial covariance ellipse by the reciprocal of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>.
The transformed coordinates are then used to compute distances and the
resulting spatial covariances. This type of anisotropy is more formally
known as “geometric” anisotropy because it involves a geometric
transformation of the coordinates. The following figure shows this
process step-by-step.</p>
<div class="figure">
<img src="technical_files/figure-html/anisotropy2-1.png" alt="In the left figure, the ellipse of an anisotropic spatial covariance function centered at the origin is shown. The blue lines represent the original axes and the red lines the transformed axes. The solid lines represent the x-axes and the dotted lines the y-axes. Note that the solid, red line is the major axis of the ellpise and the dashed, red line is the minor axis of the ellipse. In the center figure, the ellipse has been rotated clockwise by the rotate parameter so the major axis is the transformed x-axis and the minor axis is the transformed y-axis. In the right figure, the minor axis of the ellipse has been scaled by the reciprocal of the scale parameter so that the ellipse becomes a circle, which corresponds to an isotropic spatial covariance function. The transformed coordinates are then used to compute distances and spatial covariances." width="33%"><img src="technical_files/figure-html/anisotropy2-2.png" alt="In the left figure, the ellipse of an anisotropic spatial covariance function centered at the origin is shown. The blue lines represent the original axes and the red lines the transformed axes. The solid lines represent the x-axes and the dotted lines the y-axes. Note that the solid, red line is the major axis of the ellpise and the dashed, red line is the minor axis of the ellipse. In the center figure, the ellipse has been rotated clockwise by the rotate parameter so the major axis is the transformed x-axis and the minor axis is the transformed y-axis. In the right figure, the minor axis of the ellipse has been scaled by the reciprocal of the scale parameter so that the ellipse becomes a circle, which corresponds to an isotropic spatial covariance function. The transformed coordinates are then used to compute distances and spatial covariances." width="33%"><img src="technical_files/figure-html/anisotropy2-3.png" alt="In the left figure, the ellipse of an anisotropic spatial covariance function centered at the origin is shown. The blue lines represent the original axes and the red lines the transformed axes. The solid lines represent the x-axes and the dotted lines the y-axes. Note that the solid, red line is the major axis of the ellpise and the dashed, red line is the minor axis of the ellipse. In the center figure, the ellipse has been rotated clockwise by the rotate parameter so the major axis is the transformed x-axis and the minor axis is the transformed y-axis. In the right figure, the minor axis of the ellipse has been scaled by the reciprocal of the scale parameter so that the ellipse becomes a circle, which corresponds to an isotropic spatial covariance function. The transformed coordinates are then used to compute distances and spatial covariances." width="33%"><p class="caption">
In the left figure, the ellipse of an anisotropic spatial covariance
function centered at the origin is shown. The blue lines represent the
original axes and the red lines the transformed axes. The solid lines
represent the x-axes and the dotted lines the y-axes. Note that the
solid, red line is the major axis of the ellpise and the dashed, red
line is the minor axis of the ellipse. In the center figure, the ellipse
has been rotated clockwise by the rotate parameter so the major axis is
the transformed x-axis and the minor axis is the transformed y-axis. In
the right figure, the minor axis of the ellipse has been scaled by the
reciprocal of the scale parameter so that the ellipse becomes a circle,
which corresponds to an isotropic spatial covariance function. The
transformed coordinates are then used to compute distances and spatial
covariances.
</p>
</div>
<p>Anisotropy parameters
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>)
can be estimated in <code>spmodel</code> using restricted maximum
likelihood or maximum likelihood. Estimating anisotropy can be
challenging. First, we need to restrict the parameter space so that the
two parameters are identifiable (there is a unique parameter set for
each possible outcome). We restricted
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>
to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">[</mo><mn>0</mn><mo>,</mo><mi>π</mi><mo stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[0, \pi]</annotation></semantics></math>
radians due to symmetry of the covariance ellipse at rotations
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>+</mo><mi>j</mi><mi>π</mi></mrow><annotation encoding="application/x-tex">\alpha + j \pi</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>
is any integer. We also restricted
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[0, 1]</annotation></semantics></math>
because we have defined
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
as the scaling factor for the length of the minor axis relative to the
major axis – otherwise it would not be clear whether
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
refers to the minor or major axis. Given this restricted parameter
space, there is still an issue of local maxima, particularly at rotation
parameters near zero, which have a rotation very close to rotation
parameter
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>π</mi><annotation encoding="application/x-tex">\pi</annotation></semantics></math>,
but zero is far from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>π</mi><annotation encoding="application/x-tex">\pi</annotation></semantics></math>
in the parameter space. To address the local maxima problem, each
optimization iteration actually involves two likelihood evaluations –
one for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>
and another for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">|</mo><mi>π</mi><mo>−</mo><mi>α</mi><mo stretchy="true" form="postfix">|</mo></mrow><annotation encoding="application/x-tex">|\pi - \alpha|</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">|</mo><mo>⋅</mo><mo stretchy="true" form="postfix">|</mo></mrow><annotation encoding="application/x-tex">|\cdot|</annotation></semantics></math>
denotes absolute value. Thus one likelihood evaluation is always in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">[</mo><mn>0</mn><mo>,</mo><mi>π</mi><mi>/</mi><mn>2</mn><mo stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[0, \pi/2]</annotation></semantics></math>
radians and another in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">[</mo><mi>π</mi><mi>/</mi><mn>2</mn><mo>,</mo><mi>π</mi><mo stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[\pi/2, \pi]</annotation></semantics></math>
radians, exploring different quadrants of the parameter space and
allowing optimization to test solutions near zero and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>π</mi><annotation encoding="application/x-tex">\pi</annotation></semantics></math>
simultaneously.</p>
<p>Anisotropy parameters cannot be estimated in <code>spmodel</code>
when <code>estmethod</code> is <code>sv-wls</code> or
<code>sv-cl</code>. However, known anisotropy parameters for these
estimation methods can be specified via <code>spcov_initial</code> and
incorporated into estimation of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛉</mi><annotation encoding="application/x-tex">\boldsymbol{\theta}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛃</mi><annotation encoding="application/x-tex">\boldsymbol{\beta}</annotation></semantics></math>.
Anisotropy is not defined for areal data given its (binary) neighborhood
structure.</p>
</div>
<div class="section level4">
<h4 id="sec:partition-lm">Partition Factors<a class="anchor" aria-label="anchor" href="#sec:partition-lm"></a>
</h4>
<p>A partition factor is a factor (or categorical) variable in which
observations from different levels of the partition factor are assumed
uncorrelated. A partition matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐏</mi><annotation encoding="application/x-tex">\mathbf{P}</annotation></semantics></math>
of dimension
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">n \times n</annotation></semantics></math>
can be constructed to represent the partition factor. The
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mi>j</mi></mrow><annotation encoding="application/x-tex">ij</annotation></semantics></math>th
element of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐏</mi><annotation encoding="application/x-tex">\mathbf{P}</annotation></semantics></math>
equals one if the observation in the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>th
row and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>th
column are from the same level of the partition factor and zero
otherwise. Then the initial covariance matrix (ignoring the partition
factor) is updated by taking the Hadmard (element-wise) product with the
partition matrix:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝚺</mi><mrow><mi>u</mi><mi>p</mi><mi>d</mi><mi>a</mi><mi>t</mi><mi>e</mi><mi>d</mi></mrow></msub><mo>=</mo><msub><mi>𝚺</mi><mrow><mi>i</mi><mi>n</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi></mrow></msub><mo>⊙</mo><mi>𝐏</mi><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
 \boldsymbol{\Sigma}_{updated} = \boldsymbol{\Sigma}_{initial} \odot \mathbf{P},
\end{equation*}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mo>⊙</mo><annotation encoding="application/x-tex">\odot</annotation></semantics></math>
indicates the Hadmard product. Partition factors impose a block
structure in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝚺</mi><annotation encoding="application/x-tex">\boldsymbol{\Sigma}</annotation></semantics></math>,
which allows for efficient computation of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">\boldsymbol{\Sigma}^{-1}</annotation></semantics></math>
used for estimation and prediction.</p>
<p>When computing the empirical semivariogram using <code><a href="../reference/esv.html">esv()</a></code>,
semivariances are ignored when observations are from different levels of
the partition factor. For the <code>sv-wls</code> and <code>sv-cl</code>
estimation methods, semivariances are ignored when observations are from
different levels of the partition factor.</p>
</div>
<div class="section level4">
<h4 id="sec:bigdata-lm">Big Data (<code>splm()</code> only)<a class="anchor" aria-label="anchor" href="#sec:bigdata-lm"></a>
</h4>
<p>Big data model-fitting is accommodated in <code>spmodel</code> using
a “local” spatial indexing (SPIN) approach <span class="citation">(Ver
Hoef, Dumelle, et al. 2023)</span>. Suppose there are
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>
unique indexes, and each observation has one of the indexes. Then
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝚺</mi><annotation encoding="application/x-tex">\boldsymbol{\Sigma}</annotation></semantics></math>
can be represented blockwise as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝚺</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>𝚺</mi><mrow><mn>1</mn><mo>,</mo><mn>1</mn></mrow></msub></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>𝚺</mi><mrow><mn>1</mn><mo>,</mo><mn>2</mn></mrow></msub></mtd><mtd columnalign="center" style="text-align: center"><mi>…</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>…</mi></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>𝚺</mi><mrow><mn>1</mn><mo>,</mo><mi>m</mi></mrow></msub></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>𝚺</mi><mrow><mn>2</mn><mo>,</mo><mn>1</mn></mrow></msub></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>𝚺</mi><mrow><mn>2</mn><mo>,</mo><mn>2</mn></mrow></msub></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>𝚺</mi><mrow><mn>2</mn><mo>,</mo><mn>3</mn></mrow></msub></mtd><mtd columnalign="center" style="text-align: center"><mi>…</mi></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>𝚺</mi><mrow><mn>2</mn><mo>,</mo><mi>m</mi></mrow></msub></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>⋮</mi></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>𝚺</mi><mrow><mn>3</mn><mo>,</mo><mn>2</mn></mrow></msub></mtd><mtd columnalign="center" style="text-align: center"><mo>⋱</mo></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>𝚺</mi><mrow><mn>3</mn><mo>,</mo><mn>4</mn></mrow></msub></mtd><mtd columnalign="center" style="text-align: center"><mi>⋮</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>⋮</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>⋮</mi></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>𝚺</mi><mrow><mn>4</mn><mo>,</mo><mn>3</mn></mrow></msub></mtd><mtd columnalign="center" style="text-align: center"><mo>⋱</mo></mtd><mtd columnalign="center" style="text-align: center"><mi>⋮</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>𝚺</mi><mrow><mi>m</mi><mo>,</mo><mn>1</mn></mrow></msub></mtd><mtd columnalign="center" style="text-align: center"><mi>…</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>…</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>…</mi></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>𝚺</mi><mrow><mi>m</mi><mo>,</mo><mi>m</mi></mrow></msub></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation}\label{eq:full_cov}
  \boldsymbol{\Sigma} = 
  \begin{bmatrix}
  \boldsymbol{\Sigma}_{1,1} &amp; \boldsymbol{\Sigma}_{1,2} &amp; \ldots &amp; \ldots &amp; \boldsymbol{\Sigma}_{1,m} \\
  \boldsymbol{\Sigma}_{2,1} &amp; \boldsymbol{\Sigma}_{2,2} &amp; \boldsymbol{\Sigma}_{2,3} &amp; \ldots &amp; \boldsymbol{\Sigma}_{2,m} \\
  \vdots &amp; \boldsymbol{\Sigma}_{3,2} &amp; \ddots &amp; \boldsymbol{\Sigma}_{3,4} &amp; \vdots \\
  \vdots &amp; \vdots &amp; \boldsymbol{\Sigma}_{4,3} &amp; \ddots &amp; \vdots \\
  \boldsymbol{\Sigma}_{m,1} &amp; \ldots &amp; \ldots &amp; \ldots &amp; \boldsymbol{\Sigma}_{m, m}
  \end{bmatrix},
\end{equation}</annotation></semantics></math> To perform estimation for
big data, observations with the same index value are assumed independent
of observations with different index values, yielding a “big-data”
covariance matrix given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝚺</mi><mrow><mi>b</mi><mi>d</mi></mrow></msub><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>𝚺</mi><mrow><mn>1</mn><mo>,</mo><mn>1</mn></mrow></msub></mtd><mtd columnalign="center" style="text-align: center"><mn>𝟎</mn></mtd><mtd columnalign="center" style="text-align: center"><mi>…</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>…</mi></mtd><mtd columnalign="center" style="text-align: center"><mn>𝟎</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>𝟎</mn></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>𝚺</mi><mrow><mn>2</mn><mo>,</mo><mn>2</mn></mrow></msub></mtd><mtd columnalign="center" style="text-align: center"><mn>𝟎</mn></mtd><mtd columnalign="center" style="text-align: center"><mi>…</mi></mtd><mtd columnalign="center" style="text-align: center"><mn>𝟎</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>⋮</mi></mtd><mtd columnalign="center" style="text-align: center"><mn>𝟎</mn></mtd><mtd columnalign="center" style="text-align: center"><mo>⋱</mo></mtd><mtd columnalign="center" style="text-align: center"><mn>𝟎</mn></mtd><mtd columnalign="center" style="text-align: center"><mi>⋮</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>⋮</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>⋮</mi></mtd><mtd columnalign="center" style="text-align: center"><mn>𝟎</mn></mtd><mtd columnalign="center" style="text-align: center"><mo>⋱</mo></mtd><mtd columnalign="center" style="text-align: center"><mi>⋮</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>𝟎</mn></mtd><mtd columnalign="center" style="text-align: center"><mi>…</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>…</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>…</mi></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>𝚺</mi><mrow><mi>m</mi><mo>,</mo><mi>m</mi></mrow></msub></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation}\label{eq:bd_cov}
  \boldsymbol{\Sigma}_{bd} = 
  \begin{bmatrix}
  \boldsymbol{\Sigma}_{1,1} &amp; \boldsymbol{0} &amp; \ldots &amp; \ldots &amp; \boldsymbol{0} \\
  \boldsymbol{0} &amp; \boldsymbol{\Sigma}_{2,2} &amp; \boldsymbol{0} &amp; \ldots &amp; \boldsymbol{0} \\
  \vdots &amp; \boldsymbol{0} &amp; \ddots &amp; \boldsymbol{0} &amp; \vdots \\
  \vdots &amp; \vdots &amp; \boldsymbol{0} &amp; \ddots &amp; \vdots \\
  \boldsymbol{0} &amp; \ldots &amp; \ldots &amp; \ldots &amp; \boldsymbol{\Sigma}_{m, m}
  \end{bmatrix},
\end{equation}</annotation></semantics></math> Estimation then proceeds
using
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝚺</mi><mrow><mi>b</mi><mi>d</mi></mrow></msub><annotation encoding="application/x-tex">\boldsymbol{\Sigma}_{bd}</annotation></semantics></math>
instead of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝚺</mi><annotation encoding="application/x-tex">\boldsymbol{\Sigma}</annotation></semantics></math>.
When computing the empirical semivariogram, semivariances are ignored
when observations have different local indexes. For the
<code>sv-wls</code> and <code>sv-cl</code> estimation methods,
semivariances are ignored when observations have different local
indexes. Via this equation, it can be seen that the local index acts as
a partition factor separate from the partition factor explicitly defined
by <code>partition_factor</code>.</p>
<p>spmodel allows for custom local indexes to be passed to
<code><a href="../reference/splm.html">splm()</a></code>. If a custom local index is not passed, the local
index is determined using the <code>"random"</code> or
<code>"kmeans"</code> method. The <code>"random"</code> method assigns
observations to indexes randomly based on the number of groups desired.
The <code>"kmeans"</code> method uses k-means clustering <span class="citation">(MacQueen 1967)</span> on the x-coordinates and
y-coordinates to assign observations to indexes (based on the number of
clusters (groups) desired).</p>
<p>The estimate of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛃</mi><annotation encoding="application/x-tex">\boldsymbol{\beta}</annotation></semantics></math>
when using
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝚺</mi><mrow><mi>b</mi><mi>d</mi></mrow></msub><annotation encoding="application/x-tex">\boldsymbol{\Sigma}_{bd}</annotation></semantics></math>
is given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mrow><mi>b</mi><mi>d</mi></mrow></msub><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐗</mi><mi>⊤</mi></msup><msubsup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mi>b</mi><mi>d</mi></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mi>𝐗</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>𝐗</mi><mi>⊤</mi></msup><msubsup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mi>b</mi><mi>d</mi></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mi>𝐲</mi><mo>=</mo><msubsup><mi>𝐓</mi><mrow><mi>x</mi><mi>x</mi></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><msub><mi>𝐭</mi><mrow><mi>x</mi><mi>y</mi></mrow></msub><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation}\label{eq:beta_bd}
  \hat{\boldsymbol{\beta}}_{bd} = (\mathbf{X}^\top \boldsymbol{\hat{\Sigma}}^{-1}_{bd}\mathbf{X})^{-1}\mathbf{X}^\top \boldsymbol{\hat{\Sigma}}^{-1}_{bd} \mathbf{y} = \mathbf{T}^{-1}_{xx}\mathbf{t}_{xy},
\end{equation}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐓</mi><mrow><mi>x</mi><mi>x</mi></mrow></msub><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup><msubsup><mi>𝐗</mi><mi>i</mi><mi>⊤</mi></msubsup><msubsup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mi>i</mi><mo>,</mo><mi>i</mi></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><msub><mi>𝐗</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{T}_{xx} = \sum_{i = 1}^m \mathbf{X}_i^\top \boldsymbol{\hat{\Sigma}}^{-1}_{i, i}\mathbf{X}_i</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐭</mi><mrow><mi>x</mi><mi>y</mi></mrow></msub><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup><msubsup><mi>𝐗</mi><mi>i</mi><mi>⊤</mi></msubsup><msubsup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mi>i</mi><mo>,</mo><mi>i</mi></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><msub><mi>𝐲</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{t}_{xy} = \sum_{i = 1}^m \mathbf{X}_i^\top \hat{\boldsymbol{\Sigma}}^{-1}_{i, i} \mathbf{y}_i</annotation></semantics></math>.
Note that in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mrow><mi>b</mi><mi>d</mi></mrow></msub><annotation encoding="application/x-tex">\hat{\boldsymbol{\beta}}_{bd}</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐗</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\mathbf{X}_i</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐲</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\mathbf{y}_i</annotation></semantics></math>
are the subsets of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐗</mi><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐲</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math>,
respectively, for the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>th
local index. This estimator acts as a pooled estimator of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛃</mi><annotation encoding="application/x-tex">\boldsymbol{\beta}</annotation></semantics></math>
across the indexes.</p>
<p><code>spmodel</code> has four approaches for estimating the
covariance matrix of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mrow><mi>b</mi><mi>d</mi></mrow></msub><annotation encoding="application/x-tex">\hat{\boldsymbol{\beta}}_{bd}</annotation></semantics></math>.
The choice is determined by the <code>var_adjust</code> argument to
<code>local</code>. The first approach is implements no adjustment
(<code>var_adjust = "none"</code>) and simply uses
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>𝐓</mi><mrow><mi>x</mi><mi>x</mi></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><annotation encoding="application/x-tex">\mathbf{T}_{xx}^{-1}</annotation></semantics></math>,
which is the covariance matrix of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mrow><mi>b</mi><mi>d</mi></mrow></msub><annotation encoding="application/x-tex">\hat{\boldsymbol{\beta}}_{bd}</annotation></semantics></math>
using
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝚺</mi><mrow><mi>b</mi><mi>d</mi></mrow></msub><annotation encoding="application/x-tex">\boldsymbol{\Sigma}_{bd}</annotation></semantics></math>.
While computationally efficient, this approach ignores the covariance
across indexes. It can be shown that the covariance of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mrow><mi>b</mi><mi>d</mi></mrow></msub><annotation encoding="application/x-tex">\hat{\boldsymbol{\beta}}_{bd}</annotation></semantics></math>
using
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝚺</mi><annotation encoding="application/x-tex">\boldsymbol{\Sigma}</annotation></semantics></math>,
the full covariance matrix, is given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>𝐓</mi><mrow><mi>x</mi><mi>x</mi></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mo>+</mo><msubsup><mi>𝐓</mi><mrow><mi>x</mi><mi>x</mi></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><msub><mi>𝐖</mi><mrow><mi>x</mi><mi>x</mi></mrow></msub><msubsup><mi>𝐓</mi><mrow><mi>x</mi><mi>x</mi></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation}\label{eq:var_theo}
  \mathbf{T}_{xx}^{-1} + \mathbf{T}_{xx}^{-1} \mathbf{W}_{xx}\mathbf{T}_{xx}^{-1},
\end{equation}</annotation></semantics></math> where
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐖</mi><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>m</mi><mo>−</mo><mn>1</mn></mrow></munderover><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐗</mi><mi>⊤</mi></msup><msubsup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mi>i</mi><mo>,</mo><mi>i</mi></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><msub><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><msubsup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mi>j</mi><mo>,</mo><mi>j</mi></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><msub><mi>𝐗</mi><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐗</mi><mi>⊤</mi></msup><msubsup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mi>i</mi><mo>,</mo><mi>i</mi></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><msub><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><msubsup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mi>j</mi><mo>,</mo><mi>j</mi></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><msub><mi>𝐗</mi><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>⊤</mi></msup></mrow><annotation encoding="application/x-tex">\begin{equation*}
\mathbf{W} = \sum_{i = 1}^{m - 1} \sum_{j = i + 1}^m (\mathbf{X}^\top \hat{\boldsymbol{\Sigma}}^{-1}_{i, i} \hat{\boldsymbol{\Sigma}}_{i, j} \hat{\boldsymbol{\Sigma}}^{-1}_{j, j} \mathbf{X}_j) + (\mathbf{X}^\top \hat{\boldsymbol{\Sigma}}^{-1}_{i, i} \hat{\boldsymbol{\Sigma}}_{i, j} \hat{\boldsymbol{\Sigma}}^{-1}_{j, j} \mathbf{X}_j)^\top
\end{equation*}</annotation></semantics></math> This equation can be
viewed as the sum of the unadjusted covariance matrix of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mrow><mi>b</mi><mi>d</mi></mrow></msub><annotation encoding="application/x-tex">\hat{\boldsymbol{\beta}}_{bd}</annotation></semantics></math>
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>𝐓</mi><mrow><mi>x</mi><mi>x</mi></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><annotation encoding="application/x-tex">\mathbf{T}_{xx}^{-1}</annotation></semantics></math>)
and a correction that incorporates the covariance across indexes
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>𝐓</mi><mrow><mi>x</mi><mi>x</mi></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><msub><mi>𝐖</mi><mrow><mi>x</mi><mi>x</mi></mrow></msub><msubsup><mi>𝐓</mi><mrow><mi>x</mi><mi>x</mi></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup></mrow><annotation encoding="application/x-tex">\mathbf{T}_{xx}^{-1} \mathbf{W}_{xx}\mathbf{T}_{xx}^{-1}</annotation></semantics></math>).
This adjustment is known as the “theoretically-correct”
(<code>var_adjust = "theoretical"</code>) adjustment because it uses
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝚺</mi><annotation encoding="application/x-tex">\boldsymbol{\Sigma}</annotation></semantics></math>.
The theoretical adjustment is the default adjustment in
<code>spmodel</code> because it is theoretically correct, but it is the
most computationally expensive adjustment. Two alternative adjustments
are also provided, and while not equal to the theoretical adjustment,
they are easier to compute. They are the empirical
(<code>var_adjust = "empirical"</code>) and pooled
(<code>var_adjust = "pooled"</code>) adjustments. The empirical
adjustment is given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mrow><mi>m</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>m</mi><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><mo>−</mo><msub><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mrow><mi>b</mi><mi>d</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><mo>−</mo><msub><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mrow><mi>b</mi><mi>d</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>⊤</mi></msup><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
\frac{1}{m(m -1)} \sum_{i = 1}^m (\boldsymbol{\hat{\beta}}_i - \boldsymbol{\hat{\beta}}_{bd})(\boldsymbol{\hat{\beta}}_i - \boldsymbol{\hat{\beta}}_{bd})^\top,
\end{equation*}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐗</mi><mi>⊤</mi></msup><msup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐗</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msubsup><mi>𝐗</mi><mi>i</mi><mi>⊤</mi></msubsup><msubsup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mi>i</mi><mo>,</mo><mi>i</mi></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><msub><mi>𝐲</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\boldsymbol{\hat{\beta}}_i = (\mathbf{X}^\top \hat{\boldsymbol{\Sigma}}^{-1} \mathbf{X})^{-1}\mathbf{X}_i^\top \hat{\boldsymbol{\Sigma}}^{-1}_{i, i} \mathbf{y}_i</annotation></semantics></math>.
A similar adjustment could use
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mi>𝐗</mi><mi>i</mi><mi>⊤</mi></msubsup><msubsup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mi>i</mi><mo>,</mo><mi>i</mi></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><msub><mi>𝐗</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msub><mi>𝐗</mi><mi>i</mi></msub><msubsup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mi>i</mi><mo>,</mo><mi>i</mi></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><msub><mi>𝐲</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\boldsymbol{\hat{\beta}}_i = (\mathbf{X}_i^\top \hat{\boldsymbol{\Sigma}}^{-1}_{i, i} \mathbf{X}_i)^{-1}\mathbf{X}_i \hat{\boldsymbol{\Sigma}}^{-1}_{i, i} \mathbf{y}_i</annotation></semantics></math>,
which more closely resembles a composite likelihood approach. This
approach is sensitive to the presence of at least one singularity in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>𝐗</mi><mi>i</mi><mi>⊤</mi></msubsup><msubsup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mi>i</mi><mo>,</mo><mi>i</mi></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><msub><mi>𝐗</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{X}_i^\top \hat{\boldsymbol{\Sigma}}^{-1}_{i, i} \mathbf{X}_i</annotation></semantics></math>,
in which case the variance adjustment cannot be computed. The
<code>"pooled"</code> variance adjustment is given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><msup><mi>m</mi><mn>2</mn></msup></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msup><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mi>𝐗</mi><mi>i</mi><mi>⊤</mi></msubsup><msubsup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mi>i</mi><mo>,</mo><mi>i</mi></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><msub><mi>𝐗</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
\frac{1}{m^2} \sum_{i = 1}^m (\mathbf{X}^\top_i \hat{\boldsymbol{\Sigma}}^{-1}_{i, i} \mathbf{X}_i)^{-1}.
\end{equation*}</annotation></semantics></math> Note that the pooled
variance adjustment cannot be computed if any
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>𝐗</mi><mi>i</mi><mi>⊤</mi></msubsup><msubsup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mi>i</mi><mo>,</mo><mi>i</mi></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><msub><mi>𝐗</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{X}_i^\top \hat{\boldsymbol{\Sigma}}^{-1}_{i, i} \mathbf{X}_i</annotation></semantics></math>
are singular.</p>
</div>
</div>
<div class="section level3">
<h3 id="sec:rf-lm">
<code>splmRF()</code> and <code>spautorRF()</code><a class="anchor" aria-label="anchor" href="#sec:rf-lm"></a>
</h3>
<p><code><a href="../reference/splmRF.html">splmRF()</a></code> and <code><a href="../reference/spautorRF.html">spautorRF()</a></code> fit random forest
spatial residual models designed for prediction. These models are fit by
combining aspects of random forest and spatial linear modeling. First, a
random forest model <span class="citation">(Breiman 2001; James et al.
2013)</span> is fit using the <code>ranger</code> <strong>R</strong>
package <span class="citation">(Wright and Ziegler 2015)</span>. Then
random forest fitted values are obtained for each data observation and
used to compute a residual (by subtracting the fitted value from the
observed value). Then an intercept-only spatial linear model is fit to
these residuals:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐞</mi><mrow><mi>r</mi><mi>f</mi></mrow></msub><mo>=</mo><msub><mi>β</mi><mn>0</mn></msub><mo>+</mo><mi>𝛕</mi><mo>+</mo><mi>𝛜</mi><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \mathbf{e}_{rf} = \beta_0 + \mathbf{\tau} + \mathbf{\epsilon},
\end{equation*}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐞</mi><mrow><mi>r</mi><mi>f</mi></mrow></msub><annotation encoding="application/x-tex">\mathbf{e}_{rf}</annotation></semantics></math>
are the random forest residuals. Random forest spatial residual models
can significantly improve predictive accuracy for new data compared to
standard random forest models by formally incorporating spatial
covariance in the random forest residuals <span class="citation">(Fox,
Ver Hoef, and Olsen 2020)</span>.</p>
<p>Different estimation methods, different spatial covariance functions,
fixing spatial covariance parameter values, random effects, anisotropy,
partition factors, and big data are accommodated in the spatial linear
model portion of the random forest spatial residual models by supplying
their respective named arguments to <code><a href="../reference/splmRF.html">splmRF()</a></code> and
<code><a href="../reference/spautorRF.html">spautorRF()</a></code>.</p>
</div>
<div class="section level3">
<h3 id="sec:sprnorm">
<code>sprnorm()</code><a class="anchor" aria-label="anchor" href="#sec:sprnorm"></a>
</h3>
<p>Spatial normal (Gaussian) random variables are simulated by taking
the sum of a fixed mean and random errors. The random errors have mean
zero and covariance matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝚺</mi><annotation encoding="application/x-tex">\boldsymbol{\Sigma}</annotation></semantics></math>.
A realization of the random errors is obtained from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>𝚺</mi><mrow><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><mi>𝐞</mi></mrow><annotation encoding="application/x-tex">\boldsymbol{\Sigma}^{1/2} \mathbf{e}</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐞</mi><annotation encoding="application/x-tex">\mathbf{e}</annotation></semantics></math>
is a normal random variable with mean zero and covariance matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐈</mi><annotation encoding="application/x-tex">\mathbf{I}</annotation></semantics></math>.
Then the spatial normal random variable equals
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐲</mi><mo>=</mo><mi>𝛍</mi><mo>+</mo><msup><mi>𝚺</mi><mrow><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><mi>𝐞</mi><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
 \mathbf{y} = \boldsymbol{\mu} + \boldsymbol{\Sigma}^{1/2} \mathbf{e},
\end{equation*}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛍</mi><annotation encoding="application/x-tex">\boldsymbol{\mu}</annotation></semantics></math>
is the fixed mean. It follows that
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐲</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mi>𝛍</mi><mo>+</mo><msup><mi>𝚺</mi><mrow><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐞</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>𝛍</mi></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐲</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝚺</mi><mrow><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><mi>𝐞</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>𝚺</mi><mrow><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐞</mi><mo stretchy="true" form="postfix">)</mo></mrow><msup><mi>𝚺</mi><mrow><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><mo>=</mo><msup><mi>𝚺</mi><mrow><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><msup><mi>𝚺</mi><mrow><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><mo>=</mo><mi>𝚺</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{equation*}
  \begin{split}
  \text{E}(\mathbf{y}) &amp; = \boldsymbol{\mu} + \boldsymbol{\Sigma}^{1/2} \text{E}(\mathbf{e}) = \boldsymbol{\mu} \\
  \text{Cov}(\mathbf{y}) &amp; = \text{Cov}(\boldsymbol{\Sigma}^{1/2} \mathbf{e}) = \boldsymbol{\Sigma}^{1/2} \text{Cov}(\mathbf{e}) \boldsymbol{\Sigma}^{1/2} = \boldsymbol{\Sigma}^{1/2} \boldsymbol{\Sigma}^{1/2} = \boldsymbol{\Sigma}
  \end{split}
\end{equation*}</annotation></semantics></math></p>
</div>
<div class="section level3">
<h3 id="sec:vcov-lm">
<code>vcov()</code><a class="anchor" aria-label="anchor" href="#sec:vcov-lm"></a>
</h3>
<p><code><a href="https://rdrr.io/r/stats/vcov.html" class="external-link">vcov()</a></code> returns the variance-covariance matrix of
estimated parameters. Currently, <code><a href="https://rdrr.io/r/stats/vcov.html" class="external-link">vcov()</a></code> only returns the
variance-covariance matrix of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat{\boldsymbol{\beta}}</annotation></semantics></math>,
the fixed effects. The variance-covariance matrix of the fixed effects
is given by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐗</mi><mi>⊤</mi></msup><msup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐗</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">(\mathbf{X}^\top \hat{\boldsymbol{\Sigma}}^{-1} \mathbf{X})^{-1}</annotation></semantics></math>.</p>
</div>
</div>
<div class="section level2">
<h2 id="sec:spglms">Spatial Generalized Linear Models<a class="anchor" aria-label="anchor" href="#sec:spglms"></a>
</h2>
<p>When building spatial linear models, the response vector
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐲</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math>
is typically assumed Gaussian (given
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐗</mi><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics></math>).
Relaxing this assumption on the distribution of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐲</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math>
yields a rich class of spatial generalized linear models that can
describe binary data, proportion data, count data, and skewed data that
is parameterized as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝛍</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>𝛈</mi><mo>=</mo><mi>𝐗</mi><mi>𝛃</mi><mo>+</mo><mi>𝛕</mi><mo>+</mo><mi>𝛜</mi><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation}\label{eq:spglm}
g(\boldsymbol{\mu}) = \boldsymbol{\eta} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\tau} + \boldsymbol{\epsilon},
\end{equation}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo>⋅</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">g(\cdot)</annotation></semantics></math>
is called a link function,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛍</mi><annotation encoding="application/x-tex">\boldsymbol{\mu}</annotation></semantics></math>
is the mean of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐲</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math>,
and the remaining terms
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐗</mi><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛃</mi><annotation encoding="application/x-tex">\boldsymbol{\beta}</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛕</mi><annotation encoding="application/x-tex">\boldsymbol{\tau}</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛜</mi><annotation encoding="application/x-tex">\boldsymbol{\epsilon}</annotation></semantics></math>
represent the same quantities as for the spatial linear models. The link
function,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo>⋅</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">g(\cdot)</annotation></semantics></math>,
“links” a function of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛍</mi><annotation encoding="application/x-tex">\boldsymbol{\mu}</annotation></semantics></math>
to the linear term
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛈</mi><annotation encoding="application/x-tex">\boldsymbol{\eta}</annotation></semantics></math>
, denoted here as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐗</mi><mi>𝛃</mi><mo>+</mo><mi>𝛕</mi><mo>+</mo><mi>𝛜</mi></mrow><annotation encoding="application/x-tex">\mathbf{X} \boldsymbol{\beta} + \boldsymbol{\tau} + \boldsymbol{\epsilon}</annotation></semantics></math>,
which is familiar from spatial linear models. Note that the linking of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛍</mi><annotation encoding="application/x-tex">\boldsymbol{\mu}</annotation></semantics></math>
to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛈</mi><annotation encoding="application/x-tex">\boldsymbol{\eta}</annotation></semantics></math>
applies element-wise to each vector. Each link function
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo>⋅</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">g(\cdot)</annotation></semantics></math>
has a corresponding inverse link function,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>g</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mo>⋅</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">g^{-1}(\cdot)</annotation></semantics></math>.
The inverse link function “links” a function of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛈</mi><annotation encoding="application/x-tex">\boldsymbol{\eta}</annotation></semantics></math>
to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛍</mi><annotation encoding="application/x-tex">\boldsymbol{\mu}</annotation></semantics></math>.
Notice that for spatial generalized linear models, we are not modeling
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐲</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math>
directly as we do for spatial linear models, but rather we are modeling
a function of the mean of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐲</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math>.
Also notice that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛈</mi><annotation encoding="application/x-tex">\boldsymbol{\eta}</annotation></semantics></math>
is unconstrained but
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛍</mi><annotation encoding="application/x-tex">\boldsymbol{\mu}</annotation></semantics></math>
is usually constrained in some way (e.g., positive).</p>
<p>The model
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝛍</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>𝛈</mi><mo>=</mo><mi>𝐗</mi><mi>𝛃</mi><mo>+</mo><mi>𝛕</mi><mo>+</mo><mi>𝛜</mi></mrow><annotation encoding="application/x-tex">g(\boldsymbol{\mu}) = \boldsymbol{\eta} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\tau} + \boldsymbol{\epsilon}</annotation></semantics></math>
is called the spatial generalized linear model. <code>spmodel</code>
allows fitting of spatial generalized linear models when
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐲</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math>
is a binomial (or Bernoulli), beta, Poisson, negative binomial, gamma,
or inverse Gaussian random vector via the Laplace approximation and
restricted maximum likelihood estimation or maximum likelihood
estimation – <span class="citation">Ver Hoef, Blagg, et al.
(2023)</span> provide further details. For binomial and beta
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐲</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math>,
the logit link function is defined as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝛍</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mi>𝛍</mi><mrow><mn>1</mn><mo>−</mo><mi>𝛍</mi></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>𝛈</mi></mrow><annotation encoding="application/x-tex">g(\boldsymbol{\mu}) = \ln(\frac{\boldsymbol{\mu}}{1 - \boldsymbol{\mu}}) = \boldsymbol{\eta}</annotation></semantics></math>,
and the inverse logit link function is defined as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>g</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝛈</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mrow><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝛈</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mn>1</mn><mo>+</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝛈</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mo>=</mo><mi>𝛍</mi></mrow><annotation encoding="application/x-tex">g^{-1}(\boldsymbol{\eta}) = \frac{\exp(\boldsymbol{\eta})}{1 + \exp(\boldsymbol{\eta})} = \boldsymbol{\mu}</annotation></semantics></math>.
For Poisson, negative binomial, gamma, and inverse Gaussian
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐲</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math>,
the log link function is defined as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝛍</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝛍</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>𝛈</mi></mrow><annotation encoding="application/x-tex">g(\boldsymbol{\mu}) = log(\boldsymbol{\mu}) = \boldsymbol{\eta}</annotation></semantics></math>,
and the inverse log link function is defined as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>g</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝛈</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝛈</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>𝛍</mi></mrow><annotation encoding="application/x-tex">g^{-1}(\boldsymbol{\eta}) = \exp(\boldsymbol{\eta}) = \boldsymbol{\mu}</annotation></semantics></math>.
Full parameterizations of these distributions are given later.</p>
<p>For spatial linear models, one can marginalize over
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛃</mi><annotation encoding="application/x-tex">\boldsymbol{\beta}</annotation></semantics></math>
and the random components to obtain an explicit distribution of only the
data
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐲</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math>)
and covariance parameters
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛉</mi><annotation encoding="application/x-tex">\boldsymbol{\theta}</annotation></semantics></math>)
– this is the REML likelihood. For spatial generalized linear models,
this marginalization is more challenging. First define
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐰</mi><mo>=</mo><mi>𝐗</mi><mi>𝛃</mi><mo>+</mo><mi>𝛕</mi><mo>+</mo><mi>𝛜</mi></mrow><annotation encoding="application/x-tex">\mathbf{w} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\tau} + \boldsymbol{\epsilon}</annotation></semantics></math>.
Our goal is to marginalize the joint distribution of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐲</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐰</mi><annotation encoding="application/x-tex">\mathbf{w}</annotation></semantics></math>
over
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐰</mi><annotation encoding="application/x-tex">\mathbf{w}</annotation></semantics></math>
(and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛃</mi><annotation encoding="application/x-tex">\boldsymbol{\beta}</annotation></semantics></math>)
to obtain a distribution of only the data
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐲</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math>),
a dispersion parameter
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>φ</mi><annotation encoding="application/x-tex">\varphi</annotation></semantics></math>),
and covariance parameters
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛉</mi><annotation encoding="application/x-tex">\boldsymbol{\theta}</annotation></semantics></math>).
To accomplish this feat, we use a hierarchical construction that treats
the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐰</mi><annotation encoding="application/x-tex">\mathbf{w}</annotation></semantics></math>
as latent (i.e., unobserved) variables and then use the Laplace
approximation to perform integration. We briefly describe this approach
next, but it is described in full detail in <span class="citation">Ver
Hoef, Blagg, et al. (2023)</span>.</p>
<p>The marginal distribution of interest can be written hierarchically
as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>𝐲</mi><mo stretchy="false" form="prefix">|</mo><mi>𝐗</mi><mo>,</mo><mi>φ</mi><mo>,</mo><mi>𝛉</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><msub><mo>∫</mo><mi>𝐰</mi></msub><mrow><mo stretchy="true" form="prefix">[</mo><mi>𝐲</mi><mo stretchy="false" form="prefix">|</mo><msup><mi>g</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐰</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>φ</mi><mo stretchy="true" form="postfix">]</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>𝐰</mi><mo stretchy="false" form="prefix">|</mo><mi>𝐗</mi><mo>,</mo><mi>𝛉</mi><mo stretchy="true" form="postfix">]</mo></mrow><mi>d</mi><mi>𝐰</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
  [\mathbf{y}|\mathbf{X}, \varphi, \boldsymbol{\theta}] = \int_\mathbf{w} [\mathbf{y} | g^{-1}(\mathbf{w}), \varphi] [\mathbf{w} | \mathbf{X}, \boldsymbol{\theta}] d\mathbf{w} .
\end{equation*}</annotation></semantics></math> The term
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">[</mo><mi>𝐲</mi><mo stretchy="false" form="prefix">|</mo><msup><mi>g</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐰</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>φ</mi><mo stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[\mathbf{y}|g^{-1}(\mathbf{w}), \varphi]</annotation></semantics></math>
is likelihood of the generalized linear model with mean function
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>g</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐰</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">g^{-1}(\mathbf{w})</annotation></semantics></math>,
and the term
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">[</mo><mi>𝐰</mi><mo stretchy="false" form="prefix">|</mo><mi>𝐗</mi><mo>,</mo><mi>𝛉</mi><mo stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[\mathbf{w}|\mathbf{X}, \boldsymbol{\theta}]</annotation></semantics></math>
is the restricted log likelihood for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐰</mi><annotation encoding="application/x-tex">\mathbf{w}</annotation></semantics></math>
given the covariance parameters.</p>
<p>Next define
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mo>ℓ</mo><mi>𝐰</mi></msub><mo>=</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mo stretchy="true" form="prefix">[</mo><mi>𝐲</mi><mo stretchy="false" form="prefix">|</mo><msup><mi>g</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐰</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>φ</mi><mo stretchy="true" form="postfix">]</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>𝐰</mi><mo stretchy="false" form="prefix">|</mo><mi>𝐗</mi><mo>,</mo><mi>𝛉</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\ell_\mathbf{w} = \ln([\mathbf{y} | g^{-1}(\mathbf{w}), \varphi] [\mathbf{w} | \mathbf{X}, \boldsymbol{\theta}])</annotation></semantics></math>.
Let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐠</mi><annotation encoding="application/x-tex">\mathbf{g}</annotation></semantics></math>
be the gradient vector where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>g</mi><mi>i</mi></msub><mo>=</mo><mfrac><mrow><mi>∂</mi><msub><mo>ℓ</mo><mi>𝐰</mi></msub></mrow><mrow><mi>∂</mi><msub><mi>w</mi><mi>i</mi></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">g_i = \frac{\partial \ell_\mathbf{w}}{\partial w_i}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐆</mi><annotation encoding="application/x-tex">\mathbf{G}</annotation></semantics></math>
be the Hessian matrix with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mi>j</mi></mrow><annotation encoding="application/x-tex">ij</annotation></semantics></math>th
element
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>G</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>=</mo><mfrac><mrow><msup><mi>∂</mi><mn>2</mn></msup><msub><mo>ℓ</mo><mi>𝐰</mi></msub></mrow><mrow><mi>∂</mi><msub><mi>w</mi><mi>i</mi></msub><mi>∂</mi><msub><mi>w</mi><mi>j</mi></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">G_{i, j} = \frac{\partial^2 \ell_\mathbf{w}}{\partial w_i \partial w_j}</annotation></semantics></math>.
Using a multivariate Taylor series expansion around some point
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐚</mi><annotation encoding="application/x-tex">\mathbf{a}</annotation></semantics></math>,
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mo>∫</mo><mi>𝐰</mi></msub><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mo>ℓ</mo><mi>𝐰</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>d</mi><mi>𝐰</mi><mo>≈</mo><msub><mo>∫</mo><mi>𝐰</mi></msub><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mo>ℓ</mo><mi>𝐚</mi></msub><mo>+</mo><msup><mi>𝐠</mi><mi>⊤</mi></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐰</mi><mo>−</mo><mi>𝐚</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐰</mi><mo>−</mo><mi>𝐚</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>⊤</mi></msup><mi>𝐆</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐰</mi><mo>−</mo><mi>𝐚</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mi>d</mi><mi>𝐰</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \int_\mathbf{w} \exp(\ell_\mathbf{w}) d\mathbf{w} \approx \int_\mathbf{w} \exp(\ell_\mathbf{a} + \mathbf{g}^\top(\mathbf{w} - \mathbf{a}) + \frac{1}{2}(\mathbf{w} - \mathbf{a})^\top \mathbf{G} (\mathbf{w} - \mathbf{a})) d\mathbf{w}.
\end{equation*}</annotation></semantics></math> If
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐚</mi><annotation encoding="application/x-tex">\mathbf{a}</annotation></semantics></math>
is the value at which
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐠</mi><mo>=</mo><mn>𝟎</mn></mrow><annotation encoding="application/x-tex">\mathbf{g} = \mathbf{0}</annotation></semantics></math>,
then
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mo>∫</mo><mi>𝐰</mi></msub><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mo>ℓ</mo><mi>𝐰</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>d</mi><mi>𝐰</mi><mo>≈</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mo>ℓ</mo><mi>𝐚</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><msub><mo>∫</mo><mi>𝐰</mi></msub><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mo>−</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐰</mi><mo>−</mo><mi>𝐚</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>⊤</mi></msup><mrow><mo stretchy="true" form="prefix">(</mo><mo>−</mo><mi>𝐆</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐰</mi><mo>−</mo><mi>𝐚</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mi>d</mi><mi>𝐰</mi><mo>=</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mo>ℓ</mo><mi>𝐚</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mi>π</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>n</mi><mi>/</mi><mn>2</mn></mrow></msup><msup><mrow><mo stretchy="true" form="prefix">|</mo><mo>−</mo><msub><mi>𝐆</mi><mi>a</mi></msub><mo stretchy="true" form="postfix">|</mo></mrow><mrow><mo>−</mo><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \int_\mathbf{w} \exp(\ell_\mathbf{w}) d\mathbf{w} \approx \exp(\ell_\mathbf{a}) \int_\mathbf{w} \exp \left(-\frac{1}{2} (\mathbf{w} - \mathbf{a})^\top (- \mathbf{G})(\mathbf{w} - \mathbf{a}) \right) d\mathbf{w} = \exp(\ell_\mathbf{a}) (2 \pi)^{n/2} |-\mathbf{G}_a|^{-1/2}, 
\end{equation*}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐆</mi><mi>a</mi></msub><annotation encoding="application/x-tex">\mathbf{G}_a</annotation></semantics></math>
is the Hessian evaluated at
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐚</mi><annotation encoding="application/x-tex">\mathbf{a}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">|</mo><mo>⋅</mo><mo stretchy="true" form="postfix">|</mo></mrow><annotation encoding="application/x-tex">|\cdot|</annotation></semantics></math>
is the determinant operator. The previous result follows from the
normalizing constant of a Gaussian distribution with kernel
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐰</mi><mo>−</mo><mi>𝐚</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>⊤</mi></msup><msup><mrow><mo stretchy="true" form="prefix">[</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mo>−</mo><mi>𝐆</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo stretchy="true" form="postfix">]</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐰</mi><mo>−</mo><mi>𝐚</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\exp(\frac{1}{2} (\mathbf{w} - \mathbf{a})^\top [(- \mathbf{G})^{-1}]^{-1}(\mathbf{w} - \mathbf{a}))</annotation></semantics></math>.
Finally, we arrive at
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mo>∫</mo><mi>𝐰</mi></msub><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mo>ℓ</mo><mi>𝐰</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>d</mi><mi>𝐰</mi><mo>≈</mo><mrow><mo stretchy="true" form="prefix">[</mo><mi>𝐲</mi><mrow><mo stretchy="true" form="prefix">|</mo><msup><mi>g</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐚</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>φ</mi><mo stretchy="false" form="postfix">]</mo><mo stretchy="false" form="prefix">[</mo><mi>𝐚</mi><mo stretchy="true" form="postfix">|</mo></mrow><mi>𝐗</mi><mo>,</mo><mi>𝛉</mi><mo stretchy="true" form="postfix">]</mo></mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mi>π</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>n</mi><mi>/</mi><mn>2</mn></mrow></msup><msup><mrow><mo stretchy="true" form="prefix">|</mo><mo>−</mo><msub><mi>𝐆</mi><mi>a</mi></msub><mo stretchy="true" form="postfix">|</mo></mrow><mrow><mo>−</mo><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><mo stretchy="false" form="prefix">|</mo><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \int_\mathbf{w} \exp(\ell_\mathbf{w}) d\mathbf{w} \approx [\mathbf{y} | g^{-1}(\mathbf{a}), \varphi] [\mathbf{a} | \mathbf{X}, \boldsymbol{\theta}] (2 \pi)^{n/2} |-\mathbf{G}_a|^{-1/2}|,
\end{equation*}</annotation></semantics></math> which is a distribution
that has been marginalized over the latent
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐰</mi><annotation encoding="application/x-tex">\mathbf{w}</annotation></semantics></math>
and depends only on the data, a dispersion parameter, and the covariance
parameters. The approach we outlined to solve this integral is known as
the Laplace approximation.</p>
<div class="section level3">
<h3 id="sec:aic-glm">
<code>AIC()</code> and <code>AICc()</code><a class="anchor" aria-label="anchor" href="#sec:aic-glm"></a>
</h3>
<p><code><a href="https://rdrr.io/r/stats/AIC.html" class="external-link">AIC()</a></code> and <code><a href="../reference/AIC.spmodel.html">AICc()</a></code> for spatial generalized
linear models is defined the same as for spatial linear models.</p>
</div>
<div class="section level3">
<h3 id="sec:anova-glm">
<code>anova()</code><a class="anchor" aria-label="anchor" href="#sec:anova-glm"></a>
</h3>
<p><code><a href="https://rdrr.io/r/stats/anova.html" class="external-link">anova()</a></code> for spatial generalized linear models is defined
the same as for spatial linear models.</p>
</div>
<div class="section level3">
<h3 id="sec:auroc">
<code>AUROC()</code><a class="anchor" aria-label="anchor" href="#sec:auroc"></a>
</h3>
<p><code><a href="../reference/AUROC.html">AUROC()</a></code> is the area under the receiver operating
characteristic curve and is relevant for binomial (i.e., logistic)
regression models where each response represents a single success or
failure (i.e.., is binary). The AUROC ranges from zero to one and is a
measure of the model’s classification accuracy averaged over all
possible threshold values. More formally, it represents the probability
that a randomly chosen success (datum value of one) has a larger fitted
value than the fitted value of a randomly chosen failure (datum value of
zero), with an adjustment for ties in the fitted values <span class="citation">(Muschelli III 2020)</span>. <code><a href="../reference/AUROC.html">AUROC()</a></code> in
<code>spmodel</code> leverages the <code>auc()</code> function in
<code>pROC</code> <span class="citation">Robin et al. (2011)</span>. For
more on the AUROC, see <span class="citation">Hanley and McNeil
(1982)</span> and <span class="citation">Fawcett (2006)</span>.</p>
</div>
<div class="section level3">
<h3 id="sec:bic-glm">
<code>BIC()</code><a class="anchor" aria-label="anchor" href="#sec:bic-glm"></a>
</h3>
<p><code><a href="https://rdrr.io/r/stats/AIC.html" class="external-link">BIC()</a></code> for spatial generalized linear models is defined
the same as for spatial linear models.</p>
</div>
<div class="section level3">
<h3 id="sec:coef-glm">
<code>coef()</code><a class="anchor" aria-label="anchor" href="#sec:coef-glm"></a>
</h3>
<p><code><a href="https://rdrr.io/r/stats/coef.html" class="external-link">coef()</a></code> for spatial generalized linear models is defined
the same as for spatial linear models.</p>
</div>
<div class="section level3">
<h3 id="sec:confint-glm">
<code>confint()</code><a class="anchor" aria-label="anchor" href="#sec:confint-glm"></a>
</h3>
<p><code><a href="https://rdrr.io/r/stats/confint.html" class="external-link">confint()</a></code> for spatial generalized linear models is
defined the same as for spatial linear models.</p>
</div>
<div class="section level3">
<h3 id="sec:cooks-glm">
<code>cooks.distance()</code><a class="anchor" aria-label="anchor" href="#sec:cooks-glm"></a>
</h3>
<p>The Cook’s distance is defined as the standard generalized linear
model Cook’s distance after conditioning on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐰</mi><annotation encoding="application/x-tex">\mathbf{w}</annotation></semantics></math>.
That is, after conditioning on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐰</mi><annotation encoding="application/x-tex">\mathbf{w}</annotation></semantics></math>,
the Cook’s distance is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><msubsup><mi>𝐞</mi><mi>p</mi><mn>2</mn></msubsup><mi>p</mi></mfrac><mo>⊙</mo><mi>d</mi><mi>i</mi><mi>a</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐇</mi><mi>c</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>⊙</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>−</mo><mi>d</mi><mi>i</mi><mi>a</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐇</mi><mi>c</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation}
\frac{\mathbf{e}_p^2}{p} \odot diag(\mathbf{H}_c) \odot \frac{1}{1 - diag(\mathbf{H}_c)},
\end{equation}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>𝐞</mi><mi>p</mi><mn>2</mn></msubsup><annotation encoding="application/x-tex">\mathbf{e}_p^2</annotation></semantics></math>
are the Pearson residuals,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>i</mi><mi>a</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐇</mi><mi>c</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">diag(\mathbf{H}_c)</annotation></semantics></math>
is the diagonal of the leverage (hat) matrix conditional on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐰</mi><annotation encoding="application/x-tex">\mathbf{w}</annotation></semantics></math>
and given by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐇</mi><mi>c</mi></msub><mo>=</mo><msub><mi>𝐗</mi><mi>v</mi></msub><msup><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mi>𝐗</mi><mi>v</mi><mi>⊤</mi></msubsup><msub><mi>𝐗</mi><mi>v</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msubsup><mi>𝐗</mi><mi>v</mi><mi>⊤</mi></msubsup></mrow><annotation encoding="application/x-tex">\mathbf{H}_c = \mathbf{X}_v (\mathbf{X}_v^\top\mathbf{X}_v)^{-1} \mathbf{X}_v^\top</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐗</mi><mi>v</mi></msub><mo>=</mo><msup><mi>𝐕</mi><mrow><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><mi>𝐗</mi></mrow><annotation encoding="application/x-tex">\mathbf{X}_v = \mathbf{V}^{1/2}\mathbf{X}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐕</mi><annotation encoding="application/x-tex">\mathbf{V}</annotation></semantics></math>
is a diagonal matrix with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>th
diagonal element equal to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">Var</mtext><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\text{Var}(w_i)</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mo>⊙</mo><annotation encoding="application/x-tex">\odot</annotation></semantics></math>
denotes the Hadmard (element-wise) product.</p>
</div>
<div class="section level3">
<h3 id="sec:deviance-glm">
<code>deviance()</code><a class="anchor" aria-label="anchor" href="#sec:deviance-glm"></a>
</h3>
<p>The deviance is defined conditional on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐰</mi><annotation encoding="application/x-tex">\mathbf{w}</annotation></semantics></math>
(i.e., we find the deviance of the conditional model). It is derived by
taking the deviance definitions from each generalized linear models
distribution (defined later) and evaluating
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>μ</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\mu_i</annotation></semantics></math>
at
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>w</mi><mi>i</mi></msub><annotation encoding="application/x-tex">w_i</annotation></semantics></math>.</p>
</div>
<div class="section level3">
<h3 id="sec:fitted-glm">
<code>fitted()</code><a class="anchor" aria-label="anchor" href="#sec:fitted-glm"></a>
</h3>
<p>The fitted values on the link scale (<code>type = "link"</code>) are
given by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐰</mi><annotation encoding="application/x-tex">\mathbf{w}</annotation></semantics></math>.
The fitted values on the response scale (<code>type = "response"</code>)
are given by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>g</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐰</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">g^{-1}(\mathbf{w})</annotation></semantics></math>.
The fitted values for the spatial random errors
(<code>type = "spcov"</code>) and random effects
(<code>type = "randcov"</code>) are derived similarly as for spatial
linear models but treat
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐰</mi><annotation encoding="application/x-tex">\mathbf{w}</annotation></semantics></math>
as the response instead of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐲</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math>
(and are on the link scale).</p>
</div>
<div class="section level3">
<h3 id="sec:hatvalues-glm">
<code>hatvalues()</code><a class="anchor" aria-label="anchor" href="#sec:hatvalues-glm"></a>
</h3>
<p>The leverage (hat) matrix is obtained by finding the standard
generalized linear model leverage (hat) matrix after conditioning on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐰</mi><annotation encoding="application/x-tex">\mathbf{w}</annotation></semantics></math>.
That is, after conditioning on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐰</mi><annotation encoding="application/x-tex">\mathbf{w}</annotation></semantics></math>
the leverage (hat) matrix,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐇</mi><mi>c</mi></msub><annotation encoding="application/x-tex">\mathbf{H}_c</annotation></semantics></math>,
is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐇</mi><mi>c</mi></msub><mo>=</mo><msub><mi>𝐗</mi><mi>v</mi></msub><msup><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mi>𝐗</mi><mi>v</mi><mi>⊤</mi></msubsup><msub><mi>𝐗</mi><mi>v</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msubsup><mi>𝐗</mi><mi>v</mi><mi>⊤</mi></msubsup><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation}
  \mathbf{H}_c = \mathbf{X}_v (\mathbf{X}_v^\top\mathbf{X}_v)^{-1} \mathbf{X}_v^\top,
\end{equation}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐗</mi><mi>v</mi></msub><mo>=</mo><msup><mi>𝐕</mi><mrow><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><mi>𝐗</mi></mrow><annotation encoding="application/x-tex">\mathbf{X}_v = \mathbf{V}^{1/2}\mathbf{X}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐕</mi><annotation encoding="application/x-tex">\mathbf{V}</annotation></semantics></math>
is a diagonal matrix with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>th
diagonal element equal to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">Var</mtext><mrow><mo stretchy="true" form="prefix">(</mo><msub><mtext mathvariant="normal">w</mtext><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\text{Var}(\text{w}_i)</annotation></semantics></math>.</p>
</div>
<div class="section level3">
<h3 id="sec:loglik-glm">
<code>logLik()</code><a class="anchor" aria-label="anchor" href="#sec:loglik-glm"></a>
</h3>
<p><code><a href="https://rdrr.io/r/stats/logLik.html" class="external-link">logLik()</a></code> for spatial generalized linear models is
defined the same as for spatial linear models, in that the
log-likelihood is returned. The log-likelihood for spatial generalized
linear models is given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mo>ℓ</mo><mi>p</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>φ</mi><mo>,</mo><mi>𝛉</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mo stretchy="true" form="prefix">[</mo><mi>𝐲</mi><mo stretchy="false" form="prefix">|</mo><msup><mi>g</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐚</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>φ</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mo stretchy="true" form="prefix">[</mo><mi>𝐚</mi><mo stretchy="false" form="prefix">|</mo><mi>𝐗</mi><mo>,</mo><mi>𝛉</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mfrac><mi>n</mi><mn>2</mn></mfrac><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mi>π</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mo stretchy="true" form="prefix">|</mo><mo>−</mo><msub><mi>𝐆</mi><mi>𝐚</mi></msub><mo stretchy="true" form="postfix">|</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \ell_p(\varphi, \boldsymbol{\theta}) = \ln([\mathbf{y} | g^{-1}(\mathbf{a}), \varphi]) + \ln([\mathbf{a} | \mathbf{X}, \boldsymbol{\theta}]) + \frac{n}{2} \ln(2 \pi) - \frac{1}{2} \ln(|- \mathbf{G}_{\mathbf{a}}|).
\end{equation*}</annotation></semantics></math></p>
</div>
<div class="section level3">
<h3 id="sec:loocv-glm">
<code>loocv()</code><a class="anchor" aria-label="anchor" href="#sec:loocv-glm"></a>
</h3>
<p><code><a href="../reference/loocv.html">loocv()</a></code> for spatial generalized linear models is defined
similarly as for spatial linear models, except that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐰</mi><annotation encoding="application/x-tex">\mathbf{w}</annotation></semantics></math>
is predicted instead of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐲</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math>.
Then
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>g</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐰</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">g^{-1}(\mathbf{w})</annotation></semantics></math>
is compared to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐲</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math>
to compute mean-squared-prediction-error. That is,
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mi>s</mi><mi>p</mi><mi>e</mi><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msup><mi>g</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
 mspe = \frac{1}{n}\sum_{i = 1}^n(y_i - g^{-1}(w_i))^2.
\end{equation*}</annotation></semantics></math></p>
<p>When <code>cv_fitted = TRUE</code>, the predictions of held-out
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐰</mi><annotation encoding="application/x-tex">\mathbf{w}</annotation></semantics></math>
are returned. The standard errors of these predictions (of held-out
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐰</mi><annotation encoding="application/x-tex">\mathbf{w}</annotation></semantics></math>)
are returned when <code>se.fit = TRUE</code>. Note that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐆</mi><mrow><mo>−</mo><mi>i</mi></mrow></msub><annotation encoding="application/x-tex">\mathbf{G}_{-i}</annotation></semantics></math>
is determined from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐆</mi><annotation encoding="application/x-tex">\mathbf{G}</annotation></semantics></math>
using Helmert-Wolf blocking as is done for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝚺</mi><mrow><mo>−</mo><mi>i</mi></mrow></msub><annotation encoding="application/x-tex">\boldsymbol{\Sigma}_{-i}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝚺</mi><annotation encoding="application/x-tex">\boldsymbol{\Sigma}</annotation></semantics></math>.</p>
<div class="section level4">
<h4 id="sec:bigdata-loocv-glm">Big Data<a class="anchor" aria-label="anchor" href="#sec:bigdata-loocv-glm"></a>
</h4>
<p><code><a href="../reference/loocv.html">loocv()</a></code> for big data spatial generalized linear models
are defined similarly as for big data spatial linear models, except that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐰</mi><annotation encoding="application/x-tex">\mathbf{w}</annotation></semantics></math>
is predicted instead of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐲</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math>.
Additionally,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐆</mi><mrow><mi>l</mi><mo>,</mo><mi>l</mi></mrow></msub><annotation encoding="application/x-tex">\mathbf{G}_{l, l}</annotation></semantics></math>
is determined from each local neighborhood as is done for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝚺</mi><mrow><mi>l</mi><mo>,</mo><mi>l</mi></mrow></msub><annotation encoding="application/x-tex">\boldsymbol{\Sigma}_{l, l}</annotation></semantics></math>.</p>
</div>
</div>
<div class="section level3">
<h3 id="sec:predict-glm">
<code>predict()</code><a class="anchor" aria-label="anchor" href="#sec:predict-glm"></a>
</h3>
<div class="section level4">
<h4 id="sec:predict-none-glm">
<code>interval = "none"</code><a class="anchor" aria-label="anchor" href="#sec:predict-none-glm"></a>
</h4>
<p>Building from the previously defined empirical best linear unbiased
predictions (i.e., empirical Kriging predictions), predictions of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐰</mi><mi>u</mi></msub><annotation encoding="application/x-tex">\mathbf{w}_u</annotation></semantics></math>
are given on the link scale (<code>type = "link"</code>) by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover><mi>𝐰</mi><mo accent="true">̇</mo></mover><mi>u</mi></msub><mo>=</mo><msub><mi>𝐗</mi><mi>u</mi></msub><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo>+</mo><msub><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mi>u</mi><mi>o</mi></mrow></msub><msubsup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mi>o</mi><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>𝐰</mi><mo accent="true">̂</mo></mover><mi>o</mi></msub><mo>−</mo><msub><mi>𝐗</mi><mi>o</mi></msub><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation}\label{eq:glm_blup}
  \mathbf{\dot{w}}_u = \mathbf{X}_u \hat{\boldsymbol{\beta}} + \hat{\boldsymbol{\Sigma}}_{uo} \hat{\boldsymbol{\Sigma}}^{-1}_{o} (\hat{\mathbf{w}}_o - \mathbf{X}_o \hat{\boldsymbol{\beta}}) .
\end{equation}</annotation></semantics></math> These predictions are
given on the response (inverse link) scale
(<code>type = "response"</code>) as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>g</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>𝐰</mi><mo accent="true">̇</mo></mover><mi>u</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">g^{-1}(\mathbf{\dot{w}}_u)</annotation></semantics></math>.</p>
<p>Similar to the covariance matrix of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat{\boldsymbol{\beta}}</annotation></semantics></math>,
the covariance matrix of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝐰</mi><mo accent="true">̇</mo></mover><mi>u</mi></msub><annotation encoding="application/x-tex">\mathbf{\dot{w}}_u</annotation></semantics></math>
requires an adjustment to account for the fact that the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐰</mi><annotation encoding="application/x-tex">\mathbf{w}</annotation></semantics></math>
are not actually observed. First let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝚲</mi><mo>=</mo><msub><mi>𝐗</mi><mi>u</mi></msub><mi>𝐁</mi><mo>+</mo><msub><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mi>u</mi><mi>o</mi></mrow></msub><msubsup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mi>o</mi><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mo>+</mo><msub><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mi>u</mi><mi>o</mi></mrow></msub><msubsup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mi>o</mi><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><msub><mi>𝐗</mi><mi>o</mi></msub><mi>𝐁</mi></mrow><annotation encoding="application/x-tex">\boldsymbol{\Lambda} = \mathbf{X}_u \mathbf{B} + \hat{\boldsymbol{\Sigma}}_{uo} \hat{\boldsymbol{\Sigma}}_o^{-1} + \hat{\boldsymbol{\Sigma}}_{uo} \hat{\boldsymbol{\Sigma}}_o^{-1} \mathbf{X}_o \mathbf{B}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐁</mi><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐗</mi><mi>⊤</mi></msup><msubsup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mi>o</mi><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mi>𝐗</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>𝐗</mi><mi>⊤</mi></msup><msubsup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mi>o</mi><mrow><mo>−</mo><mn>1</mn></mrow></msubsup></mrow><annotation encoding="application/x-tex">\mathbf{B} = (\mathbf{X}^\top \hat{\boldsymbol{\Sigma}}_o^{-1} \mathbf{X})^{-1} \mathbf{X}^\top \hat{\boldsymbol{\Sigma}}_o^{-1}</annotation></semantics></math>,
and note that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover><mi>𝐰</mi><mo accent="true">̇</mo></mover><mi>u</mi></msub><mo>=</mo><mi>Λ</mi><msub><mover><mi>𝐰</mi><mo accent="true">̂</mo></mover><mi>o</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{\dot{w}}_u = \Lambda \hat{\mathbf{w}}_o</annotation></semantics></math>.
Using the law of conditional variance and conditioning on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐰</mi><mi>o</mi></msub><annotation encoding="application/x-tex">\mathbf{w}_o</annotation></semantics></math>
as if we had observed them, it follows that
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>𝐰</mi><mo accent="true">̂</mo></mover><mi>u</mi></msub><mo>−</mo><msub><mi>𝐰</mi><mi>u</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>Λ</mi><msub><mover><mi>𝐰</mi><mo accent="true">̂</mo></mover><mi>o</mi></msub><mo>−</mo><msub><mi>𝐰</mi><mi>u</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mtext mathvariant="normal">E</mtext><msub><mi>𝐰</mi><mi>o</mi></msub></msub><mrow><mo stretchy="true" form="prefix">[</mo><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>Λ</mi><msub><mover><mi>𝐰</mi><mo accent="true">̂</mo></mover><mi>o</mi></msub><mo>−</mo><msub><mi>𝐰</mi><mi>u</mi></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>𝐰</mi><mi>o</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mo>+</mo><msub><mtext mathvariant="normal">Cov</mtext><msub><mi>𝐰</mi><mi>o</mi></msub></msub><mrow><mo stretchy="true" form="prefix">[</mo><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>Λ</mi><msub><mover><mi>𝐰</mi><mo accent="true">̂</mo></mover><mi>o</mi></msub><mo>−</mo><msub><mi>𝐰</mi><mi>u</mi></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>𝐰</mi><mi>o</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \text{Cov}(\hat{\mathbf{w}}_u - \mathbf{w}_u) = \text{Cov}(\Lambda \hat{\mathbf{w}}_o - \mathbf{w}_u) = \text{E}_{\mathbf{w}_o}[\text{Cov}(\Lambda \hat{\mathbf{w}}_o - \mathbf{w}_u | \mathbf{w}_o)] + \text{Cov}_{\mathbf{w}_o}[\text{E}(\Lambda \hat{\mathbf{w}}_o - \mathbf{w}_u | \mathbf{w}_o)],
\end{equation*}</annotation></semantics></math> We assume
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝐰</mi><mo accent="true">̂</mo></mover><mi>o</mi></msub><annotation encoding="application/x-tex">\hat{\mathbf{w}}_o</annotation></semantics></math>
is unbiased for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐰</mi><mi>o</mi></msub><annotation encoding="application/x-tex">\mathbf{w}_o</annotation></semantics></math>
(i.e.,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>𝐰</mi><mo accent="true">̂</mo></mover><mi>o</mi></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>𝐰</mi><mi>o</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mi>𝐰</mi><mi>o</mi></msub></mrow><annotation encoding="application/x-tex">\text{E}(\hat{\mathbf{w}}_o | \mathbf{w}_o) = \mathbf{w}_o</annotation></semantics></math>).
Then
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="normal">Cov</mtext><msub><mi>𝐰</mi><mi>o</mi></msub></msub><mrow><mo stretchy="true" form="prefix">[</mo><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>Λ</mi><msub><mover><mi>𝐰</mi><mo accent="true">̂</mo></mover><mi>o</mi></msub><mo>−</mo><msub><mi>𝐰</mi><mi>u</mi></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>𝐰</mi><mi>o</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><msub><mtext mathvariant="normal">Cov</mtext><msub><mi>𝐰</mi><mi>o</mi></msub></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>Λ</mi><msub><mi>𝐰</mi><mi>o</mi></msub><mo>−</mo><msub><mi>𝐰</mi><mi>u</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mi>u</mi></msub><mo>−</mo><msub><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mi>u</mi><mi>o</mi></mrow></msub><msubsup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mi>o</mi><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><msubsup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mi>u</mi><mi>o</mi></mrow><mi>⊤</mi></msubsup><mo>+</mo><mi>𝐐</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mi>𝐗</mi><mi>o</mi><mi>⊤</mi></msubsup><msubsup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mi>o</mi><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><msub><mi>𝐗</mi><mi>o</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>𝐐</mi><mi>⊤</mi></msup></mrow><annotation encoding="application/x-tex">\text{Cov}_{\mathbf{w}_o}[\text{E}(\Lambda \hat{\mathbf{w}}_o - \mathbf{w}_u | \mathbf{w}_o)] = \text{Cov}_{\mathbf{w}_o}(\Lambda \mathbf{w}_o - \mathbf{w}_u) = \hat{\boldsymbol{\Sigma}}_u - \hat{\boldsymbol{\Sigma}}_{uo} \hat{\boldsymbol{\Sigma}}^{-1}_o \hat{\boldsymbol{\Sigma}}^\top_{uo} + \mathbf{Q}(\mathbf{X}_o^\top \hat{\boldsymbol{\Sigma}}_o^{-1} \mathbf{X}_o)^{-1}\mathbf{Q}^\top</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐐</mi><mo>=</mo><msub><mi>𝐗</mi><mi>u</mi></msub><mo>−</mo><msub><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mi>u</mi><mi>o</mi></mrow></msub><msubsup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mi>o</mi><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><msub><mi>𝐗</mi><mi>o</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{Q} = \mathbf{X}_u - \hat{\boldsymbol{\Sigma}}_{uo} \hat{\boldsymbol{\Sigma}}^{-1}_o \mathbf{X}_o</annotation></semantics></math>,
the usual form for the mean squared prediction error of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝐰</mi><mo accent="true">̂</mo></mover><mi>u</mi></msub><annotation encoding="application/x-tex">\hat{\mathbf{w}}_u</annotation></semantics></math>.
Next note that (viewing
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐰</mi><mi>u</mi></msub><annotation encoding="application/x-tex">\mathbf{w}_u</annotation></semantics></math>
as a constant)
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="normal">E</mtext><msub><mi>𝐰</mi><mi>o</mi></msub></msub><mrow><mo stretchy="true" form="prefix">[</mo><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>Λ</mi><msub><mover><mi>𝐰</mi><mo accent="true">̂</mo></mover><mi>o</mi></msub><mo>−</mo><msub><mi>𝐰</mi><mi>u</mi></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>𝐰</mi><mi>o</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\text{E}_{\mathbf{w}_o}[\text{Cov}(\Lambda \hat{\mathbf{w}}_o - \mathbf{w}_u | \mathbf{w}_o)]</annotation></semantics></math>
can be viewed as the observed Fisher Information inverse,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><msup><mi>𝐆</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">- \mathbf{G}^{-1}</annotation></semantics></math>.
Evaluating
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐆</mi><annotation encoding="application/x-tex">\mathbf{G}</annotation></semantics></math>
at
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐚</mi><annotation encoding="application/x-tex">\mathbf{a}</annotation></semantics></math>
eventually yields the adjusted covariance matrix of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Λ</mi><msub><mover><mi>𝐰</mi><mo accent="true">̂</mo></mover><mi>o</mi></msub><mo>−</mo><msub><mi>𝐰</mi><mi>u</mi></msub></mrow><annotation encoding="application/x-tex">\Lambda \hat{\mathbf{w}}_o - \mathbf{w}_u</annotation></semantics></math>
(<code>var_correct = TRUE</code>) given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>Λ</mi><msub><mover><mi>𝐰</mi><mo accent="true">̂</mo></mover><mi>o</mi></msub><mo>−</mo><msub><mi>𝐰</mi><mi>u</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mover><mi>𝚺</mi><mo accent="true">̇</mo></mover><mi>u</mi></msub><mo>=</mo><mi>Λ</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><mo>−</mo><msub><mi>𝐆</mi><mi>𝐚</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>Λ</mi><mi>⊤</mi></msup><mo>+</mo><msub><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mi>u</mi></msub><mo>−</mo><msub><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mi>u</mi><mi>o</mi></mrow></msub><msubsup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mi>o</mi><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><msubsup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mi>u</mi><mi>o</mi></mrow><mi>⊤</mi></msubsup><mo>+</mo><mi>𝐐</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mi>𝐗</mi><mi>o</mi><mi>⊤</mi></msubsup><msubsup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mi>o</mi><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><msub><mi>𝐗</mi><mi>o</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>𝐐</mi><mi>⊤</mi></msup><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation}\label{eq:glm_blup_cov}
  \text{Cov}(\Lambda \hat{\mathbf{w}}_o - \mathbf{w}_u) = \dot{\boldsymbol{\Sigma}}_u = \Lambda (-\mathbf{G}_\mathbf{a})^{-1} \Lambda^\top + \hat{\boldsymbol{\Sigma}}_u - \hat{\boldsymbol{\Sigma}}_{uo} \hat{\boldsymbol{\Sigma}}^{-1}_o \hat{\boldsymbol{\Sigma}}^\top_{uo} + \mathbf{Q}(\mathbf{X}_o^\top \hat{\boldsymbol{\Sigma}}_o^{-1} \mathbf{X}_o)^{-1}\mathbf{Q}^\top .
\end{equation}</annotation></semantics></math></p>
<p>The unadjusted covariance matrix of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Λ</mi><msub><mover><mi>𝐰</mi><mo accent="true">̂</mo></mover><mi>o</mi></msub><mo>−</mo><msub><mi>𝐰</mi><mi>u</mi></msub></mrow><annotation encoding="application/x-tex">\Lambda \hat{\mathbf{w}}_o - \mathbf{w}_u</annotation></semantics></math>
(<code>var_correct = FALSE</code>) can also be returned and is given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>Λ</mi><msub><mover><mi>𝐰</mi><mo accent="true">̂</mo></mover><mi>o</mi></msub><mo>−</mo><msub><mi>𝐰</mi><mi>u</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mover><mi>𝚺</mi><mo accent="true">̇</mo></mover><mi>u</mi></msub><mo>=</mo><msub><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mi>u</mi></msub><mo>−</mo><msub><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mi>u</mi><mi>o</mi></mrow></msub><msubsup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mi>o</mi><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><msubsup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mi>u</mi><mi>o</mi></mrow><mi>⊤</mi></msubsup><mo>+</mo><mi>𝐐</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mi>𝐗</mi><mi>o</mi><mi>⊤</mi></msubsup><msubsup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mi>o</mi><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><msub><mi>𝐗</mi><mi>o</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>𝐐</mi><mi>⊤</mi></msup><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation}\label{eq:glm_blup_cov_unadj}
  \text{Cov}(\Lambda \hat{\mathbf{w}}_o - \mathbf{w}_u) = \dot{\boldsymbol{\Sigma}}_u = \hat{\boldsymbol{\Sigma}}_u - \hat{\boldsymbol{\Sigma}}_{uo} \hat{\boldsymbol{\Sigma}}^{-1}_o \hat{\boldsymbol{\Sigma}}^\top_{uo} + \mathbf{Q}(\mathbf{X}_o^\top \hat{\boldsymbol{\Sigma}}_o^{-1} \mathbf{X}_o)^{-1}\mathbf{Q}^\top .
\end{equation}</annotation></semantics></math></p>
<p>When <code>se.fit = TRUE</code>, standard errors are returned on the
link scale by taking the square root of the diagonal of the relevant
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝚺</mi><mo accent="true">̇</mo></mover><mi>u</mi></msub><annotation encoding="application/x-tex">\dot{\boldsymbol{\Sigma}}_u</annotation></semantics></math>.</p>
</div>
<div class="section level4">
<h4 id="sec:predict-predict-glm">
<code>interval = "prediction"</code><a class="anchor" aria-label="anchor" href="#sec:predict-predict-glm"></a>
</h4>
<p>Predictions of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐰</mi><mi>u</mi></msub><annotation encoding="application/x-tex">\mathbf{w}_u</annotation></semantics></math>
are returned on the link scale (<code>type = "link"</code>) by
evaluating
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝐰</mi><mo accent="true">̇</mo></mover><mi>u</mi></msub><annotation encoding="application/x-tex">\mathbf{\dot{w}}_u</annotation></semantics></math>.
The (100
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mo>×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math><code>level</code>)% prediction interval for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>w</mi><mi>u</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>i</mi></msub><annotation encoding="application/x-tex">(w_u)_i</annotation></semantics></math>
is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>w</mi><mo accent="true">̇</mo></mover><mi>u</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>i</mi></msub><mo>±</mo><msup><mi>z</mi><mo>*</mo></msup><msqrt><msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>𝚺</mi><mo accent="true">̇</mo></mover><mi>u</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>i</mi><mo>,</mo><mi>i</mi></mrow></msub></msqrt></mrow><annotation encoding="application/x-tex">(\dot{w}_u)_i \pm z^* \sqrt{(\dot{\boldsymbol{\Sigma}}_u)_{i, i}}</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msqrt><msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>𝚺</mi><mo accent="true">̇</mo></mover><mi>u</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>i</mi><mo>,</mo><mi>i</mi></mrow></msub></msqrt><annotation encoding="application/x-tex">\sqrt{(\dot{\boldsymbol{\Sigma}}_u)_{i, i}}</annotation></semantics></math>
is the standard error of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>w</mi><mo accent="true">̇</mo></mover><mi>u</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>i</mi></msub><annotation encoding="application/x-tex">(\dot{w}_u)_i</annotation></semantics></math>
obtained from <code>se.fit = TRUE</code>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Φ</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>z</mi><mo>*</mo></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>−</mo><mi>α</mi><mi>/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">\Phi(z^*) = 1 - \alpha / 2</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Φ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo>⋅</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Phi(\cdot)</annotation></semantics></math>
is the standard normal (Gaussian) cumulative distribution function,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>1</mn><mo>−</mo></mrow><annotation encoding="application/x-tex">\alpha = 1 -</annotation></semantics></math><code>level</code>, and <code>level</code> is an argument to
<code><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict()</a></code>. The default for <code>level</code> is 0.95,
which corresponds to a
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>z</mi><mo>*</mo></msup><annotation encoding="application/x-tex">z^*</annotation></semantics></math>
of approximately 1.96. These predictions and corresponding prediction
intervals are returned on the response scale
(<code>type = "response"</code>) by applying
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>g</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mo>⋅</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">g^{-1}(\cdot)</annotation></semantics></math>
(inverse link) to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>w</mi><mo accent="true">̇</mo></mover><mi>u</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>i</mi></msub><annotation encoding="application/x-tex">(\dot{w}_u)_i</annotation></semantics></math>
(the prediction),
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>w</mi><mo accent="true">̇</mo></mover><mi>u</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>i</mi></msub><mo>−</mo><msup><mi>z</mi><mo>*</mo></msup><msqrt><msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>𝚺</mi><mo accent="true">̇</mo></mover><mi>u</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>i</mi><mo>,</mo><mi>i</mi></mrow></msub></msqrt></mrow><annotation encoding="application/x-tex">(\dot{w}_u)_i - z^* \sqrt{(\dot{\boldsymbol{\Sigma}}_u)_{i, i}}</annotation></semantics></math>
(the prediction interval lower bound), and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>w</mi><mo accent="true">̇</mo></mover><mi>u</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>i</mi></msub><mo>+</mo><msup><mi>z</mi><mo>*</mo></msup><msqrt><msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>𝚺</mi><mo accent="true">̇</mo></mover><mi>u</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>i</mi><mo>,</mo><mi>i</mi></mrow></msub></msqrt></mrow><annotation encoding="application/x-tex">(\dot{w}_u)_i + z^* \sqrt{(\dot{\boldsymbol{\Sigma}}_u)_{i, i}}</annotation></semantics></math>
(the prediction interval upper bound). Note that the prediction
intervals are symmetric on the link scale but are not generally
symmetric on the response scale. One could obtain symmetric prediction
intervals on the response scale using the delta method <span class="citation">(Ver Hoef 2012)</span>.</p>
</div>
<div class="section level4">
<h4 id="sec:predict-conf-glm">
<code>interval = "confidence"</code><a class="anchor" aria-label="anchor" href="#sec:predict-conf-glm"></a>
</h4>
<p>Estimates for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐗</mi><mi>u</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>i</mi></msub><mi>𝛃</mi></mrow><annotation encoding="application/x-tex">(\mathbf{X}_u)_i \boldsymbol{\beta}</annotation></semantics></math>
(the fixed effects portion of the model) are returned on the link scale
(<code>type = "link"</code>) by evaluating
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐗</mi><mi>u</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>i</mi></msub><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover></mrow><annotation encoding="application/x-tex">(\mathbf{X}_u)_i \hat{\boldsymbol{\beta}}</annotation></semantics></math>
(i.e., fitted values corresponding to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐗</mi><mi>u</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>i</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(\mathbf{X}_u)_i)</annotation></semantics></math>.
The (100
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mo>×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math><code>level</code>)% confidence interval for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐗</mi><mi>u</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>i</mi></msub><mi>𝛃</mi></mrow><annotation encoding="application/x-tex">(\mathbf{X}_u)_i \boldsymbol{\beta}</annotation></semantics></math>
is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐗</mi><mi>u</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>i</mi></msub><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo>±</mo><msup><mi>z</mi><mo>*</mo></msup><msqrt><mrow><msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐗</mi><mi>u</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>i</mi></msub><mrow><mo stretchy="true" form="prefix">[</mo><mi>𝐁</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><mo>−</mo><msub><mi>𝐆</mi><mi>a</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>𝐁</mi><mi>⊤</mi></msup><mo>+</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐗</mi><mi>⊤</mi></msup><msubsup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mi>o</mi><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mi>𝐗</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo stretchy="true" form="postfix">]</mo></mrow><msubsup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐗</mi><mi>u</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>i</mi><mi>⊤</mi></msubsup></mrow></msqrt></mrow><annotation encoding="application/x-tex">(\mathbf{X}_u)_i \hat{\boldsymbol{\beta}} \pm z^* \sqrt{(\mathbf{X}_u)_i [\mathbf{B} (-\mathbf{G}_a)^{-1} \mathbf{B}^\top + (\mathbf{X}^\top \hat{\boldsymbol{\Sigma}}^{-1}_o \mathbf{X})^{-1}](\mathbf{X}_u)_i^\top}</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐗</mi><mi>u</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>i</mi></msub><annotation encoding="application/x-tex">(\mathbf{X}_u)_i</annotation></semantics></math>
is the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>th
row of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐗</mi><mi>u</mi></msub><annotation encoding="application/x-tex">\mathbf{X}_u</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐁</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><mo>−</mo><msub><mi>𝐆</mi><mi>a</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>𝐁</mi><mi>⊤</mi></msup><mo>+</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐗</mi><mi>⊤</mi></msup><msup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐗</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{B} (-\mathbf{G}_a)^{-1} \mathbf{B}^\top + (\mathbf{X}^\top \hat{\boldsymbol{\Sigma}}^{-1} \mathbf{X})^{-1}</annotation></semantics></math>
is the standard error of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐗</mi><mi>u</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>i</mi></msub><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover></mrow><annotation encoding="application/x-tex">(\mathbf{X}_u)_i \hat{\boldsymbol{\beta}}</annotation></semantics></math>
obtained from <code>se.fit = TRUE</code>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Φ</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>z</mi><mo>*</mo></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>−</mo><mi>α</mi><mi>/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">\Phi(z^*) = 1 - \alpha / 2</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Φ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo>⋅</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Phi(\cdot)</annotation></semantics></math>
is the standard normal (Gaussian) cumulative distribution function,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>1</mn><mo>−</mo></mrow><annotation encoding="application/x-tex">\alpha = 1 -</annotation></semantics></math><code>level</code>, and <code>level</code> is an argument to
<code><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict()</a></code>. The default for <code>level</code> is 0.95,
which corresponds to a
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>z</mi><mo>*</mo></msup><annotation encoding="application/x-tex">z^*</annotation></semantics></math>
of approximately 1.96. These estimates and corresponding confidence
intervals are returned on the response scale
(<code>type = "response"</code>) by applying
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>g</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mo>⋅</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">g^{-1}(\cdot)</annotation></semantics></math>
(inverse link) to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐗</mi><mi>u</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>i</mi></msub><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover></mrow><annotation encoding="application/x-tex">(\mathbf{X}_u)_i \hat{\boldsymbol{\beta}}</annotation></semantics></math>
(the estimate),
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐗</mi><mi>u</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>i</mi></msub><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo>−</mo><msup><mi>z</mi><mo>*</mo></msup><msqrt><mrow><msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐗</mi><mi>u</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>i</mi></msub><mrow><mo stretchy="true" form="prefix">[</mo><mi>𝐁</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><mo>−</mo><msub><mi>𝐆</mi><mi>a</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>𝐁</mi><mi>⊤</mi></msup><mo>+</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐗</mi><mi>⊤</mi></msup><msubsup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mi>o</mi><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mi>𝐗</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo stretchy="true" form="postfix">]</mo></mrow><msubsup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐗</mi><mi>u</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>i</mi><mi>⊤</mi></msubsup></mrow></msqrt></mrow><annotation encoding="application/x-tex">(\mathbf{X}_u)_i \hat{\boldsymbol{\beta}} - z^* \sqrt{(\mathbf{X}_u)_i [\mathbf{B} (-\mathbf{G}_a)^{-1} \mathbf{B}^\top + (\mathbf{X}^\top \hat{\boldsymbol{\Sigma}}^{-1}_o \mathbf{X})^{-1}](\mathbf{X}_u)_i^\top}</annotation></semantics></math>
(the confidence interval lower bound), and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐗</mi><mi>u</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>i</mi></msub><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo>+</mo><msup><mi>z</mi><mo>*</mo></msup><msqrt><mrow><msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐗</mi><mi>u</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>i</mi></msub><mrow><mo stretchy="true" form="prefix">[</mo><mi>𝐁</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><mo>−</mo><msub><mi>𝐆</mi><mi>a</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>𝐁</mi><mi>⊤</mi></msup><mo>+</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐗</mi><mi>⊤</mi></msup><msubsup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mi>o</mi><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mi>𝐗</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo stretchy="true" form="postfix">]</mo></mrow><msubsup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐗</mi><mi>u</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>i</mi><mi>⊤</mi></msubsup></mrow></msqrt></mrow><annotation encoding="application/x-tex">(\mathbf{X}_u)_i \hat{\boldsymbol{\beta}} + z^* \sqrt{(\mathbf{X}_u)_i [\mathbf{B} (-\mathbf{G}_a)^{-1} \mathbf{B}^\top + (\mathbf{X}^\top \hat{\boldsymbol{\Sigma}}^{-1}_o \mathbf{X})^{-1}](\mathbf{X}_u)_i^\top}</annotation></semantics></math>
(the confidence interval upper bound). Note that the confidence
intervals are symmetric on the link scale but are generally not
symmetric on the response scale. One could obtain symmetric confidence
intervals on the response scale using the delta method <span class="citation">(Ver Hoef 2012)</span>.</p>
</div>
<div class="section level4">
<h4 id="sec:predict-spautor-glm">
<code>spgautor()</code> extra steps<a class="anchor" aria-label="anchor" href="#sec:predict-spautor-glm"></a>
</h4>
<p>The extra step required to obtain
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mi>o</mi><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><annotation encoding="application/x-tex">\hat{\boldsymbol{\Sigma}}^{-1}_o</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mi>u</mi></msub><annotation encoding="application/x-tex">\hat{\boldsymbol{\Sigma}}_u</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝚺</mi><mo accent="true">̂</mo></mover><mrow><mi>u</mi><mi>o</mi></mrow></msub><annotation encoding="application/x-tex">\hat{\boldsymbol{\Sigma}}_{uo}</annotation></semantics></math>
is the same for spatial generalized autoregressive models as it is for
spatial autoregressive models.</p>
</div>
<div class="section level4">
<h4 id="sec:bigdata-predict-glm">Big Data<a class="anchor" aria-label="anchor" href="#sec:bigdata-predict-glm"></a>
</h4>
<p><code><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict()</a></code> for big data spatial generalized linear models
is defined similarly as for big data spatial linear models, except that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐰</mi><annotation encoding="application/x-tex">\mathbf{w}</annotation></semantics></math>
are subset or predicted (instead of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐲</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math>)
to find
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝐰</mi><mo accent="true">̌</mo></mover><mi>o</mi></msub><annotation encoding="application/x-tex">\check{\mathbf{w}}_o</annotation></semantics></math>
(instead of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝐲</mi><mo accent="true">̌</mo></mover><mi>o</mi></msub><annotation encoding="application/x-tex">\check{\mathbf{y}}_o</annotation></semantics></math>).
It standard errors are required,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝐆</mi><mo accent="true">̌</mo></mover><mi>o</mi></msub><annotation encoding="application/x-tex">\check{\mathbf{G}}_o</annotation></semantics></math>
is also found.</p>
</div>
</div>
<div class="section level3">
<h3 id="sec:pr2-glm">
<code>pseudoR2()</code><a class="anchor" aria-label="anchor" href="#sec:pr2-glm"></a>
</h3>
<p><code><a href="../reference/pseudoR2.html">pseudoR2()</a></code> for spatial generalized linear models is
defined the same as for spatial linear models.</p>
</div>
<div class="section level3">
<h3 id="sec:residuals-glm">
<code>residuals()</code><a class="anchor" aria-label="anchor" href="#sec:residuals-glm"></a>
</h3>
<p>The residuals are obtained by applying standard generalized linear
model definitions after conditioning on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐰</mi><annotation encoding="application/x-tex">\mathbf{w}</annotation></semantics></math>.</p>
<p>When <code>type = "response"</code>, response residuals are returned:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐞</mi><mi>r</mi></msub><mo>=</mo><mi>𝐲</mi><mo>−</mo><msup><mi>g</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐰</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
 \mathbf{e}_{r} = \mathbf{y} - g^{-1}(\mathbf{w}).
\end{equation*}</annotation></semantics></math></p>
<p>When <code>type = "pearson"</code>, Pearson residuals are returned:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐞</mi><mi>p</mi></msub><mo>=</mo><msup><mi>𝐕</mi><mrow><mo>−</mo><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><msub><mi>𝐞</mi><mi>r</mi></msub><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
 \mathbf{e}_{p} = \mathbf{V}^{-1/2}\mathbf{e}_{r},
\end{equation*}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐕</mi><annotation encoding="application/x-tex">\mathbf{V}</annotation></semantics></math>
is a diagonal matrix with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>th
diagonal element equal to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">Var</mtext><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\text{Var}(w_i)</annotation></semantics></math>.</p>
<p>When <code>type = "deviance"</code>, deviance residuals are returned:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐞</mi><mi>d</mi></msub><mo>=</mo><mtext mathvariant="normal">sign</mtext><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐞</mi><mi>r</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>⊙</mo><msqrt><msub><mtext mathvariant="normal">deviance</mtext><mi>i</mi></msub></msqrt><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
 \mathbf{e}_{d} = \text{sign}(\mathbf{e}_r) \odot \sqrt{\text{deviance}_i},
\end{equation*}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mtext mathvariant="normal">deviance</mtext><mi>i</mi></msub><annotation encoding="application/x-tex">\text{deviance}_i</annotation></semantics></math>
is the deviance of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding="application/x-tex">y_i</annotation></semantics></math>
(conditional on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>w</mi><mi>i</mi></msub><annotation encoding="application/x-tex">w_i</annotation></semantics></math>)
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mo>⊙</mo><annotation encoding="application/x-tex">\odot</annotation></semantics></math>
denotes the Hadmard (element-wise) product.</p>
<p>When <code>type = "standardized"</code>, standardized residuals are
returned:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐞</mi><mi>s</mi></msub><mo>=</mo><msub><mi>𝐞</mi><mi>d</mi></msub><mo>⊙</mo><mfrac><mn>1</mn><msqrt><mrow><mn>1</mn><mo>−</mo><mi>d</mi><mi>i</mi><mi>a</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐇</mi><mi>c</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow></msqrt></mfrac><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
 \mathbf{e}_{s} = \mathbf{e}_{d} \odot \frac{1}{\sqrt{1 - diag(\mathbf{H}_c)}},
\end{equation*}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>i</mi><mi>a</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐇</mi><mi>c</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">diag(\mathbf{H}_c)</annotation></semantics></math>
is the diagonal of the leverage (hat) matrix conditional on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐰</mi><annotation encoding="application/x-tex">\mathbf{w}</annotation></semantics></math>
and given by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐇</mi><mi>c</mi></msub><mo>≡</mo><msub><mi>𝐗</mi><mi>v</mi></msub><msup><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mi>𝐗</mi><mi>v</mi><mi>⊤</mi></msubsup><msub><mi>𝐗</mi><mi>v</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msubsup><mi>𝐗</mi><mi>v</mi><mi>⊤</mi></msubsup></mrow><annotation encoding="application/x-tex">\mathbf{H}_c \equiv \mathbf{X}_v (\mathbf{X}_v^\top\mathbf{X}_v)^{-1} \mathbf{X}_v^\top</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐗</mi><mi>v</mi></msub><mo>=</mo><msup><mi>𝐕</mi><mrow><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><mi>𝐗</mi></mrow><annotation encoding="application/x-tex">\mathbf{X}_v = \mathbf{V}^{1/2}\mathbf{X}</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mo>⊙</mo><annotation encoding="application/x-tex">\odot</annotation></semantics></math>
denotes the Hadmard (element-wise) product.</p>
</div>
<div class="section level3">
<h3 id="sec:spgmod">
<code>spgautor()</code> and <code>spglm()</code><a class="anchor" aria-label="anchor" href="#sec:spgmod"></a>
</h3>
<p>Many of the details regarding <code><a href="../reference/spglm.html">spglm()</a></code> and
<code><a href="../reference/spgautor.html">spgautor()</a></code> for spatial generalized linear models are the
same as <code><a href="../reference/splm.html">splm()</a></code> and <code><a href="../reference/spautor.html">spautor()</a></code> for spatial
linear models, though occasional differences are noted in the following
subsection headers.</p>
<div class="section level4">
<h4 id="sec:spgautor-fn">
<code>spgautor()</code> Spatial Covariance Functions<a class="anchor" aria-label="anchor" href="#sec:spgautor-fn"></a>
</h4>
<p>Covariance functions for <code><a href="../reference/spgautor.html">spgautor()</a></code> are defined the same
as covariance functions for <code><a href="../reference/spautor.html">spautor()</a></code>.</p>
</div>
<div class="section level4">
<h4 id="sec:spglm-glm">
<code>spglm()</code> Spatial Covariance Functions<a class="anchor" aria-label="anchor" href="#sec:spglm-glm"></a>
</h4>
<p>Covariance functions for <code><a href="../reference/spglm.html">spglm()</a></code> are defined the same as
covariance functions for <code><a href="../reference/splm.html">splm()</a></code>.</p>
</div>
<div class="section level4">
<h4 id="sec:estimation-glm">Model-fitting<a class="anchor" aria-label="anchor" href="#sec:estimation-glm"></a>
</h4>
<p>Recall that the likelihood of interest is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mo>∫</mo><mi>𝐰</mi></msub><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mo>ℓ</mo><mi>𝐰</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>d</mi><mi>𝐰</mi><mo>≈</mo><mrow><mo stretchy="true" form="prefix">[</mo><mi>𝐲</mi><mrow><mo stretchy="true" form="prefix">|</mo><msup><mi>g</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐚</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>φ</mi><mo stretchy="false" form="postfix">]</mo><mo stretchy="false" form="prefix">[</mo><mi>𝐚</mi><mo stretchy="true" form="postfix">|</mo></mrow><mi>𝐗</mi><mo>,</mo><mi>𝛉</mi><mo stretchy="true" form="postfix">]</mo></mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mi>π</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>n</mi><mi>/</mi><mn>2</mn></mrow></msup><msup><mrow><mo stretchy="true" form="prefix">|</mo><mo>−</mo><msub><mi>𝐆</mi><mi>a</mi></msub><mo stretchy="true" form="postfix">|</mo></mrow><mrow><mo>−</mo><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><mo stretchy="false" form="prefix">|</mo><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \int_\mathbf{w} \exp(\ell_\mathbf{w}) d\mathbf{w} \approx [\mathbf{y} | g^{-1}(\mathbf{a}), \varphi] [\mathbf{a} | \mathbf{X}, \boldsymbol{\theta}] (2 \pi)^{n/2} |-\mathbf{G}_a|^{-1/2}|,
\end{equation*}</annotation></semantics></math> and that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐚</mi><annotation encoding="application/x-tex">\mathbf{a}</annotation></semantics></math>
is the value at which the gradient,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐠</mi><annotation encoding="application/x-tex">\mathbf{g}</annotation></semantics></math>,
equals zero. Given
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐚</mi><annotation encoding="application/x-tex">\mathbf{a}</annotation></semantics></math>,
minus twice a profiled (by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛃</mi><annotation encoding="application/x-tex">\boldsymbol{\beta}</annotation></semantics></math>)
marginal Laplace log-likelihood is given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mn>2</mn><msub><mo>ℓ</mo><mi>p</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>φ</mi><mo>,</mo><mi>𝛉</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>−</mo><mn>2</mn><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mo stretchy="true" form="prefix">[</mo><mi>𝐲</mi><mo stretchy="false" form="prefix">|</mo><msup><mi>g</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐚</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>φ</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mn>2</mn><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mo stretchy="true" form="prefix">[</mo><mi>𝐚</mi><mo stretchy="false" form="prefix">|</mo><mi>𝐗</mi><mo>,</mo><mi>𝛉</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>n</mi><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mi>π</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mo stretchy="true" form="prefix">|</mo><mo>−</mo><msub><mi>𝐆</mi><mi>𝐚</mi></msub><mo stretchy="true" form="postfix">|</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
  -2\ell_p(\varphi, \boldsymbol{\theta}) = -2\ln([\mathbf{y} | g^{-1}(\mathbf{a}), \varphi]) -2\ln([\mathbf{a} | \mathbf{X}, \boldsymbol{\theta}]) -n \ln(2 \pi) + \ln(|- \mathbf{G}_{\mathbf{a}}|).
\end{equation*}</annotation></semantics></math> Note that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>ln</mo><mrow><mo stretchy="true" form="prefix">[</mo><mi>𝐚</mi><mo stretchy="false" form="prefix">|</mo><mi>𝐗</mi><mo>,</mo><mi>𝛉</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\ln[\mathbf{a} | \mathbf{X}, \boldsymbol{\theta}]</annotation></semantics></math>
are the ML or REML log-likelihood equations the spatial linear model,
respectively, where now
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>𝛃</mi><mo accent="true">̃</mo></mover><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐗</mi><mi>⊤</mi></msup><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐗</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>𝐗</mi><mi>⊤</mi></msup><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐚</mi></mrow><annotation encoding="application/x-tex">\tilde{\boldsymbol{\beta}} = (\mathbf{X}^\top \mathbf{\Sigma}^{-1} \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{\Sigma}^{-1} \mathbf{a}</annotation></semantics></math>.</p>
<p>Assuming
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐲</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>g</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐰</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">g^{-1}(\mathbf{w})</annotation></semantics></math>
are conditionally independent and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>φ</mi><annotation encoding="application/x-tex">\varphi</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛉</mi><annotation encoding="application/x-tex">\boldsymbol{\theta}</annotation></semantics></math>
are known, it can be shown that maximizing
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mo>ℓ</mo><mi>𝐰</mi></msub><annotation encoding="application/x-tex">\ell_{\mathbf{w}}</annotation></semantics></math>
(with respect to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐰</mi><annotation encoding="application/x-tex">\mathbf{w}</annotation></semantics></math>)
amounts to maximizing (up to a constant)
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mo>ln</mo><mrow><mo stretchy="true" form="prefix">[</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>w</mi><mi>i</mi></msub><mo>,</mo><mi>φ</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>−</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐰</mi><mo>−</mo><mi>𝐗</mi><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mi>⊤</mi></msup><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐰</mi><mo>−</mo><mi>𝐗</mi><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \sum_{i = 1}^N \ln[y_i | w_i, \varphi] - \frac{1}{2}(\mathbf{w} - \mathbf{X} \hat{\boldsymbol{\beta}})^\top \boldsymbol{\Sigma}^{-1}(\mathbf{w} - \mathbf{X}\hat{\boldsymbol{\beta}}).
\end{equation*}</annotation></semantics></math> Thus the gradient
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mo>ℓ</mo><mi>𝐰</mi></msub><annotation encoding="application/x-tex">\ell_{\mathbf{w}}</annotation></semantics></math>
can be shown to equal
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐠</mi><mo>=</mo><mi>𝐝</mi><mo>−</mo><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐰</mi><mo>+</mo><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐗</mi><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo>=</mo><mi>𝐝</mi><mo>−</mo><mi>𝐏</mi><mi>𝐰</mi><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
 \mathbf{g} = \mathbf{d} - \boldsymbol{\Sigma}^{-1}\mathbf{w} + \boldsymbol{\Sigma}^{-1}\mathbf{X} \hat{\boldsymbol{\beta}} = \mathbf{d} - \mathbf{P}\mathbf{w},
\end{equation*}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>i</mi></msub><mo>=</mo><mi>∂</mi><mo>ln</mo><mrow><mo stretchy="true" form="prefix">[</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="false" form="prefix">|</mo><msup><mi>g</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>φ</mi><mo stretchy="true" form="postfix">]</mo></mrow><mi>/</mi><mi>∂</mi><msub><mi>w</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">d_i = \partial \ln[y_i | g^{-1}(w_i), \varphi] / \partial w_i</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐏</mi><mo>=</mo><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo>−</mo><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐗</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐗</mi><mi>⊤</mi></msup><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐗</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>𝐗</mi><mi>⊤</mi></msup><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{P} = \boldsymbol{\Sigma}^{-1} - \boldsymbol{\Sigma}^{-1} \mathbf{X} (\mathbf{X}^\top \boldsymbol{\Sigma}^{-1} \mathbf{X})^{-1} \mathbf{X}^\top \boldsymbol{\Sigma}^{-1}</annotation></semantics></math>.
And the Hessian of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mo>ℓ</mo><mi>𝐰</mi></msub><annotation encoding="application/x-tex">\ell_{\mathbf{w}}</annotation></semantics></math>
is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐆</mi><mo>=</mo><mi>𝐃</mi><mo>−</mo><mi>𝐏</mi><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \mathbf{G} = \mathbf{D} - \mathbf{P},
\end{equation*}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mrow><mi>i</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>=</mo><msup><mi>∂</mi><mn>2</mn></msup><mo>ln</mo><mrow><mo stretchy="true" form="prefix">[</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="false" form="prefix">|</mo><msup><mi>g</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>φ</mi><mo stretchy="true" form="postfix">]</mo></mrow><mi>/</mi><mi>∂</mi><msubsup><mi>w</mi><mi>i</mi><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">D_{i, i} = \partial^2 \ln[y_i | g^{-1}(w_i), \varphi] / \partial w_i^2</annotation></semantics></math>.
Note that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">D_{i, j} = 0</annotation></semantics></math>
for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mo>≠</mo><mi>j</mi></mrow><annotation encoding="application/x-tex">i \neq j</annotation></semantics></math>
because of conditional independence.</p>
<p>Next the Newton-Rhapson algorithm can be used to maximize
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mo>ℓ</mo><mi>𝐰</mi></msub><annotation encoding="application/x-tex">\ell_{\mathbf{w}}</annotation></semantics></math>,
where an update is given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>𝐰</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>=</mo><msup><mi>𝐰</mi><mi>k</mi></msup><mo>−</mo><mi>α</mi><msup><mi>𝐆</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐠</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
 \mathbf{w}^{k + 1} = \mathbf{w}^k - \alpha \mathbf{G}^{-1}\mathbf{g}.
\end{equation*}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn><mo>&lt;</mo><mi>α</mi><mo>≤</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">0 &lt; \alpha \leq 1</annotation></semantics></math>.
Typically,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\alpha = 1</annotation></semantics></math>,
but it can be lowered if the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐠</mi><annotation encoding="application/x-tex">\mathbf{g}</annotation></semantics></math>
is unstable. Generally, the Newton-Rhapson converges rapidly. The value
of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐰</mi><annotation encoding="application/x-tex">\mathbf{w}</annotation></semantics></math>
at convergence is defined as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐚</mi><annotation encoding="application/x-tex">\mathbf{a}</annotation></semantics></math>.</p>
<p>It follows that finding
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>φ</mi><annotation encoding="application/x-tex">\varphi</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛉</mi><annotation encoding="application/x-tex">\boldsymbol{\theta}</annotation></semantics></math>
for unknown
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐚</mi><annotation encoding="application/x-tex">\mathbf{a}</annotation></semantics></math>
requires a doubly-iterative algorithm. First, a value of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐚</mi><annotation encoding="application/x-tex">\mathbf{a}</annotation></semantics></math>
is proposed (e.g.,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐚</mi><mo>=</mo><mn>𝟎</mn></mrow><annotation encoding="application/x-tex">\mathbf{a} = \mathbf{0}</annotation></semantics></math>).
Then given
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐚</mi><mn>0</mn></msub><annotation encoding="application/x-tex">\mathbf{a}_0</annotation></semantics></math>,
the Laplace log-likelihood is maximized, yielding
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>φ</mi><mo accent="true">̂</mo></mover><mn>0</mn></msub><annotation encoding="application/x-tex">\hat{\varphi}_0</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝛉</mi><mo accent="true">̂</mo></mover><mn>0</mn></msub><annotation encoding="application/x-tex">\hat{\boldsymbol{\theta}}_0</annotation></semantics></math>.
Then given
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>φ</mi><mo accent="true">̂</mo></mover><mn>0</mn></msub><annotation encoding="application/x-tex">\hat{\varphi}_0</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝛉</mi><mo accent="true">̂</mo></mover><mn>0</mn></msub><annotation encoding="application/x-tex">\hat{\boldsymbol{\theta}}_0</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mo>ℓ</mo><mi>𝐰</mi></msub><annotation encoding="application/x-tex">\ell_{\mathbf{w}}</annotation></semantics></math>
is maximized, yielding
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐚</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\mathbf{a}_1</annotation></semantics></math>.
Then given
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐚</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\mathbf{a}_1</annotation></semantics></math>,
the Laplace log-likelihood is maximized, yielding
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>φ</mi><mo accent="true">̂</mo></mover><mn>1</mn></msub><annotation encoding="application/x-tex">\hat{\varphi}_1</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝛉</mi><mo accent="true">̂</mo></mover><mn>1</mn></msub><annotation encoding="application/x-tex">\hat{\boldsymbol{\theta}}_1</annotation></semantics></math>.
Then given
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>φ</mi><mo accent="true">̂</mo></mover><mn>1</mn></msub><annotation encoding="application/x-tex">\hat{\varphi}_1</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝛉</mi><mo accent="true">̂</mo></mover><mn>1</mn></msub><annotation encoding="application/x-tex">\hat{\boldsymbol{\theta}}_1</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mo>ℓ</mo><mi>𝐰</mi></msub><annotation encoding="application/x-tex">\ell_{\mathbf{w}}</annotation></semantics></math>
is maximized, yielding
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐚</mi><mn>2</mn></msub><annotation encoding="application/x-tex">\mathbf{a}_2</annotation></semantics></math>.
This process continues until convergence, yielding optimized values for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>φ</mi><annotation encoding="application/x-tex">\varphi</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛉</mi><annotation encoding="application/x-tex">\boldsymbol{\theta}</annotation></semantics></math>
and, using these optimized values, a value of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐚</mi><annotation encoding="application/x-tex">\mathbf{a}</annotation></semantics></math>.</p>
<p>Note that the Laplace approximation incorporates a likelihood, and as
a result, the only estimation methods available via the
<code>estmethod</code> argument are <code>"reml"</code> (the default)
and <code>"ml"</code>. The doubly-iterative algorithm used to fit
spatial generalized linear models is far more computationally expensive
than fitting spatial linear models.</p>
</div>
<div class="section level4">
<h4 id="sec:optim-glm">Optimization<a class="anchor" aria-label="anchor" href="#sec:optim-glm"></a>
</h4>
<p>Optimization for <code><a href="../reference/spglm.html">spglm()</a></code> and <code><a href="../reference/spgautor.html">spgautor()</a></code>
works as it does for <code><a href="../reference/splm.html">splm()</a></code> and <code><a href="../reference/spautor.html">spautor()</a></code>,
with one additional step. The convergence criteria for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐰</mi><annotation encoding="application/x-tex">\mathbf{w}</annotation></semantics></math>
(within each covariance parameter iteration) is achieved when the
largest absolute value of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐰</mi><mi>k</mi></msub><mo>−</mo><msub><mi>𝐰</mi><mrow><mi>k</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">\mathbf{w}_k - \mathbf{w}_{k - 1}</annotation></semantics></math>
is less than
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mi>/</mi><msup><mn>10</mn><mn>4</mn></msup></mrow><annotation encoding="application/x-tex">1/10^4</annotation></semantics></math>
or
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>&gt;</mo><mn>50</mn></mrow><annotation encoding="application/x-tex">k &gt; 50</annotation></semantics></math>
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>
indexes the Newton-Rhapson iterations).</p>
<div class="section level5">
<h5 id="sec:grid-glm">Grid Search<a class="anchor" aria-label="anchor" href="#sec:grid-glm"></a>
</h5>
<p>The grid search for <code><a href="../reference/spglm.html">spglm()</a></code> and <code><a href="../reference/spgautor.html">spgautor()</a></code>
works as it does for <code><a href="../reference/splm.html">splm()</a></code> and <code><a href="../reference/spautor.html">spautor()</a></code>
except that for <code><a href="../reference/spglm.html">spglm()</a></code> and <code><a href="../reference/spgautor.html">spgautor()</a></code>, the
grid search initial values are on the link scale and the grid search
sample variance is calculated by regressing
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐲</mi><mo>+</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\ln(\mathbf{y} + 1)</annotation></semantics></math>
on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐗</mi><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics></math>
instead of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐲</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math>
on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐗</mi><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics></math>.
For negative binomial, beta, gamma, and inverse Gaussian families, the
initial value of the dispersion parameter is set to one.</p>
</div>
</div>
<div class="section level4">
<h4 id="sec:testing-glm">Hypothesis Testing<a class="anchor" aria-label="anchor" href="#sec:testing-glm"></a>
</h4>
<p>Hypothesis testing for spatial generalized linear models is defined
the same as for spatial linear models. That is, the hypothesis tests are
asymptotic z-tests based on the normal (Gaussian) distribution (Wald
tests). The null hypothesis for the test associated with each
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>β</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><annotation encoding="application/x-tex">\hat{\beta}_i</annotation></semantics></math>
is that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>β</mi><mi>i</mi></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\beta_i = 0</annotation></semantics></math>.
The spatial generalized linear models,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\text{Cov}(\hat{\boldsymbol{\beta}})</annotation></semantics></math>
requires an adjustment to account for the fact that the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐰</mi><annotation encoding="application/x-tex">\mathbf{w}</annotation></semantics></math>
are not actually observed. First let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐁</mi><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐗</mi><mi>⊤</mi></msup><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐗</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>𝐗</mi><mi>⊤</mi></msup><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{B} = (\mathbf{X}^\top \boldsymbol{\Sigma}^{-1} \mathbf{X})^{-1} \mathbf{X}^\top \boldsymbol{\Sigma}^{-1}</annotation></semantics></math>
and note that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo>=</mo><mi>𝐁</mi><mi>𝐰</mi></mrow><annotation encoding="application/x-tex">\hat{\boldsymbol{\beta}} = \mathbf{B}\mathbf{w}</annotation></semantics></math>.
Using the law of conditional variance and conditioning on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐰</mi><annotation encoding="application/x-tex">\mathbf{w}</annotation></semantics></math>
as if we had observed them, it follows that
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐁</mi><mover><mi>𝐰</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mtext mathvariant="normal">E</mtext><mi>𝐰</mi></msub><mrow><mo stretchy="true" form="prefix">[</mo><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐁</mi><mover><mi>𝐰</mi><mo accent="true">̂</mo></mover><mo stretchy="false" form="prefix">|</mo><mi>𝐰</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mo>+</mo><msub><mtext mathvariant="normal">Cov</mtext><mi>𝐰</mi></msub><mrow><mo stretchy="true" form="prefix">[</mo><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐁</mi><mover><mi>𝐰</mi><mo accent="true">̂</mo></mover><mo stretchy="false" form="prefix">|</mo><mi>𝐰</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \text{Cov}(\mathbf{B} \hat{\mathbf{w}}) = \text{E}_\mathbf{w}[\text{Cov}(\mathbf{B}\hat{\mathbf{w}} | \mathbf{w})] + \text{Cov}_\mathbf{w}[\text{E}(\mathbf{B}\hat{\mathbf{w}} | \mathbf{w})]
\end{equation*}</annotation></semantics></math> We assume
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>𝐰</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat{\mathbf{w}}</annotation></semantics></math>
is unbiased for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐰</mi><annotation encoding="application/x-tex">\mathbf{w}</annotation></semantics></math>
(i.e.,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>𝐰</mi><mo accent="true">̂</mo></mover><mo stretchy="false" form="prefix">|</mo><mi>𝐰</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>𝐰</mi></mrow><annotation encoding="application/x-tex">\text{E}(\hat{\mathbf{w}} | \mathbf{w}) = \mathbf{w}</annotation></semantics></math>).
Then
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐁</mi><mover><mi>𝐰</mi><mo accent="true">̂</mo></mover><mo stretchy="false" form="prefix">|</mo><mi>𝐰</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐁</mi><mi>𝐰</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>𝐁</mi><mi>𝚺</mi><msup><mi>𝐁</mi><mi>⊤</mi></msup></mrow><annotation encoding="application/x-tex">\text{Cov}(\text{E}(\mathbf{B} \hat{\mathbf{w}} | \mathbf{w})) = \text{Cov}(\mathbf{B}\mathbf{w}) = \mathbf{B} \boldsymbol{\Sigma} \mathbf{B}^\top</annotation></semantics></math>,
which reduces to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐗</mi><mi>⊤</mi></msup><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐗</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">(\mathbf{X}^\top \boldsymbol{\Sigma}^{-1} \mathbf{X})^{-1}</annotation></semantics></math>,
the usual form for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\text{Cov}(\hat{\boldsymbol{\beta}})</annotation></semantics></math>.
Next note that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>𝐰</mi><mo accent="true">̂</mo></mover><mo stretchy="false" form="prefix">|</mo><mi>𝐰</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\text{Cov}(\hat{\mathbf{w}} | \mathbf{w})</annotation></semantics></math>
can be viewed as the inverse of the observed Fisher Information, which
is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><msup><mi>𝐆</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">-\mathbf{G}^{-1}</annotation></semantics></math>,
which depends on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐰</mi><annotation encoding="application/x-tex">\mathbf{w}</annotation></semantics></math>
through the diagonal elements in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐃</mi><annotation encoding="application/x-tex">\mathbf{D}</annotation></semantics></math>.
Evaluating
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mi>𝐆</mi></mrow><annotation encoding="application/x-tex">-\mathbf{G}</annotation></semantics></math>
at
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐚</mi><annotation encoding="application/x-tex">\mathbf{a}</annotation></semantics></math>
yields the adjusted covariance matrix of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat{\boldsymbol{\beta}}</annotation></semantics></math>
given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>𝐁</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><mo>−</mo><msub><mi>𝐆</mi><mi>a</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>𝐁</mi><mi>⊤</mi></msup><mo>+</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐗</mi><mi>⊤</mi></msup><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐗</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \text{Cov}(\hat{\boldsymbol{\beta}}) = \mathbf{B} (-\mathbf{G}_a)^{-1} \mathbf{B}^\top + (\mathbf{X}^\top \boldsymbol{\Sigma}^{-1} \mathbf{X})^{-1} .
\end{equation*}</annotation></semantics></math></p>
</div>
<div class="section level4">
<h4 id="sec:random-glm">Random Effects (<code>spglm()</code> only)<a class="anchor" aria-label="anchor" href="#sec:random-glm"></a>
</h4>
<p>Random effects for spatial generalized linear models are defined the
same as for spatial linear models. Note that random effects for spatial
generalized linear models are on the link scale.</p>
</div>
<div class="section level4">
<h4 id="sec:anisotropy-glm">Anisotropy (<code>spglm()</code> only)<a class="anchor" aria-label="anchor" href="#sec:anisotropy-glm"></a>
</h4>
<p>Anisotropy for spatial generalized linear models are defined the same
as for spatial linear models. Note that anisotropy parameters for
spatial generalized linear models are on the link scale.</p>
</div>
<div class="section level4">
<h4 id="sec:partition-glm">Partition Factors<a class="anchor" aria-label="anchor" href="#sec:partition-glm"></a>
</h4>
<p>Partition factors for spatial generalized linear models are defined
the same as for spatial linear models.</p>
</div>
<div class="section level4">
<h4 id="sec:bigdata-glm">Big Data (<code>spglm()</code> only)<a class="anchor" aria-label="anchor" href="#sec:bigdata-glm"></a>
</h4>
<p>Big data model-fitting for spatial generalized linear models is in
many ways the same as for spatial linear models. The <code>local</code>
argument behaves the same for spatial generalized linear models as it
does for spatial linear models. This is because fundamentally, the
“local” spatial indexing (SPIN) approach to representing
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝚺</mi><annotation encoding="application/x-tex">\boldsymbol{\Sigma}</annotation></semantics></math>
blockwise is still applied and serves as the basis for massive
computational gains when fitting spatial generalized linear models <span class="citation">(Ver Hoef, Dumelle, et al. 2023)</span>.</p>
<p>The additional step that is required to fit big data spatial
generalized linear models involves efficiently manipulating the Hessian,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐆</mi><annotation encoding="application/x-tex">\mathbf{G}</annotation></semantics></math>,
to obtain its inverse and log determinant. Before providing further
details, we review the Sherman-Morrison-Woodbury (SMW) formula <span class="citation">(Sherman 1949; Sherman and Morrison 1950; Woodbury
1950)</span>. The SMW formula states that for an
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">n \times n</annotation></semantics></math>
matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐀</mi><annotation encoding="application/x-tex">\mathbf{A}</annotation></semantics></math>,
an
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">n \times k</annotation></semantics></math>
matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐔</mi><annotation encoding="application/x-tex">\mathbf{U}</annotation></semantics></math>,
a
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>×</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">k \times k</annotation></semantics></math>
matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐂</mi><annotation encoding="application/x-tex">\mathbf{C}</annotation></semantics></math>,
and a
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">k \times n</annotation></semantics></math>
matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐕</mi><annotation encoding="application/x-tex">\mathbf{V}</annotation></semantics></math>,
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐀</mi><mo>+</mo><mi>𝐔</mi><mi>𝐂</mi><mi>𝐕</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo>=</mo><msup><mi>𝐀</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo>−</mo><msup><mi>𝐀</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐔</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐂</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo>+</mo><mi>𝐕</mi><msup><mi>𝐀</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐔</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐕</mi><msup><mi>𝐀</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\begin{equation*}
  (\mathbf{A} + \mathbf{U} \mathbf{C} \mathbf{V})^{-1} = \mathbf{A}^{-1} - \mathbf{A}^{-1} \mathbf{U} (\mathbf{C}^{-1} + \mathbf{V}\mathbf{A}^{-1}\mathbf{U})^{-1} \mathbf{V} \mathbf{A}^{-1}
\end{equation*}</annotation></semantics></math> and
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">|</mo><mi>𝐀</mi><mo>+</mo><mi>𝐔</mi><mi>𝐂</mi><mi>𝐕</mi><mo stretchy="true" form="postfix">|</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">|</mo><mi>𝐀</mi><mo stretchy="true" form="postfix">|</mo></mrow><mrow><mo stretchy="true" form="prefix">|</mo><mi>𝐂</mi><mo stretchy="true" form="postfix">|</mo></mrow><mrow><mo stretchy="true" form="prefix">|</mo><msup><mi>𝐂</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo>+</mo><mi>𝐕</mi><msup><mi>𝐀</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐔</mi><mo stretchy="true" form="postfix">|</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
  |\mathbf{A} + \mathbf{U} \mathbf{C} \mathbf{V}| = |\mathbf{A}||\mathbf{C}||\mathbf{C}^{-1} + \mathbf{V}\mathbf{A}^{-1}\mathbf{U}|.
\end{equation*}</annotation></semantics></math> The determinant result
above implies
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>ln</mo><mrow><mo stretchy="true" form="prefix">|</mo><mi>𝐀</mi><mo>+</mo><mi>𝐔</mi><mi>𝐂</mi><mi>𝐕</mi><mo stretchy="true" form="postfix">|</mo></mrow><mo>=</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">|</mo><mi>𝐀</mi><mo stretchy="true" form="postfix">|</mo></mrow><mo>+</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">|</mo><mi>𝐂</mi><mo stretchy="true" form="postfix">|</mo></mrow><mo>+</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">|</mo><msup><mi>𝐂</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo>+</mo><mi>𝐕</mi><msup><mi>𝐀</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐔</mi><mo stretchy="true" form="postfix">|</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \ln|\mathbf{A} + \mathbf{U} \mathbf{C} \mathbf{V}| = \ln|\mathbf{A}| + \ln|\mathbf{C}| + \ln|\mathbf{C}^{-1} + \mathbf{V}\mathbf{A}^{-1}\mathbf{U}|.
\end{equation*}</annotation></semantics></math> The SMW formula is
important because if the inverse and log determinant of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐀</mi><annotation encoding="application/x-tex">\mathbf{A}</annotation></semantics></math>
is efficient to compute and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>&lt;</mo><mo>&lt;</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">k &lt;&lt; n</annotation></semantics></math>,
then the inverse and log determinant of the desired sum can also be
efficient to compute. This is because except for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>𝐀</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">\mathbf{A}^{-1}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix" mathvariant="bold">|</mo><mi>𝐀</mi><mo stretchy="true" form="postfix" mathvariant="bold">|</mo></mrow><annotation encoding="application/x-tex">\mathbf{|A|}</annotation></semantics></math>,
the SMW formula only requires finding
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>×</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">k \times k</annotation></semantics></math>
inverses and log determinants.</p>
<p>Recall that the Hessian can be written as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐆</mi><mo>=</mo><mi>𝐃</mi><mo>−</mo><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo>+</mo><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐗</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐗</mi><mi>⊤</mi></msup><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐗</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>𝐗</mi><mi>⊤</mi></msup><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \mathbf{G} = \mathbf{D} - \boldsymbol{\Sigma}^{-1} + \boldsymbol{\Sigma}^{-1} \mathbf{X} (\mathbf{X}^\top \boldsymbol{\Sigma}^{-1} \mathbf{X})^{-1} \mathbf{X}^\top \boldsymbol{\Sigma}^{-1},
\end{equation*}</annotation></semantics></math> Because
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝚺</mi><annotation encoding="application/x-tex">\boldsymbol{\Sigma}</annotation></semantics></math>
can be represented blockwise,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">\boldsymbol{\Sigma}^{-1}</annotation></semantics></math>
can be represented blockwise and thus the inverse and log determinant
can be efficiently computed. Because
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐃</mi><annotation encoding="application/x-tex">\mathbf{D}</annotation></semantics></math>
is diagonal,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐃</mi><mo>−</mo><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{D} - \boldsymbol{\Sigma}^{-1}</annotation></semantics></math>
can be represented blockwise and thus the inverse and log determinant
can be efficiently computed. Then the SMW formula can be used, taking
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐀</mi><mo>=</mo><mi>𝐃</mi><mo>−</mo><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{A} = \mathbf{D} - \boldsymbol{\Sigma}^{-1}</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐔</mi><mo>=</mo><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐗</mi></mrow><annotation encoding="application/x-tex">\mathbf{U} = \boldsymbol{\Sigma}^{-1} \mathbf{X}</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐂</mi><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐗</mi><mi>⊤</mi></msup><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐗</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{C} = (\mathbf{X}^\top \boldsymbol{\Sigma}^{-1} \mathbf{X})^{-1}</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐕</mi><mo>=</mo><msup><mi>𝐗</mi><mi>⊤</mi></msup><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{V} = \mathbf{X}^\top \boldsymbol{\Sigma}^{-1}</annotation></semantics></math>.</p>
<p>As previously mentioned, fitting big data spatial generalized linear
models requires a doubly-iterative algorithm. This makes it far more
computationally expensive than fitting big data spatial linear
models.</p>
</div>
</div>
<div class="section level3">
<h3 id="sec:sim-fns">Simulation Functions (<code>sprpois()</code>,
<code>sprnbinom()</code>, <code>sprbinom()</code>,
<code>sprbeta()</code>, <code>sprgamma()</code>,
<code>sprinvgauss()</code>)<a class="anchor" aria-label="anchor" href="#sec:sim-fns"></a>
</h3>
<p>Poisson, negative binomial, binomial, beta, gamma, and inverse
Gaussian random variables can be simulated using <code><a href="../reference/sprpois.html">sprpois()</a></code>,
<code><a href="../reference/sprnbinom.html">sprnbinom()</a></code>, <code><a href="../reference/sprbinom.html">sprbinom()</a></code>,
<code><a href="../reference/sprbeta.html">sprbeta()</a></code>, <code><a href="../reference/sprgamma.html">sprgamma()</a></code>, and
<code><a href="../reference/sprinvgauss.html">sprinvgauss()</a></code>, respectively. All of these functions work
similarly. First, relevant arguments are passed to
<code><a href="../reference/sprnorm.html">sprnorm()</a></code> to simulate
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐰</mi><annotation encoding="application/x-tex">\mathbf{w}</annotation></semantics></math>
on the link scale. Then using
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐰</mi><annotation encoding="application/x-tex">\mathbf{w}</annotation></semantics></math>
and the dispersion parameter (when required), relevant generalized
linear model random variables are simulated independently for each
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>w</mi><mi>i</mi></msub><annotation encoding="application/x-tex">w_i</annotation></semantics></math>.
Note that the dispersion parameter is not required for
<code><a href="../reference/sprpois.html">sprpois()</a></code> and <code><a href="../reference/sprbinom.html">sprbinom()</a></code>.</p>
</div>
<div class="section level3">
<h3 id="sec:vcov-glm">
<code>vcov()</code><a class="anchor" aria-label="anchor" href="#sec:vcov-glm"></a>
</h3>
<p>The corrected variance-covariance matrix of the fixed effects
(<code>var_correct = TRUE</code>) is given by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐁</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><mo>−</mo><msub><mi>𝐆</mi><mi>a</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>𝐁</mi><mi>⊤</mi></msup><mo>+</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐗</mi><mi>⊤</mi></msup><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐗</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{B} (-\mathbf{G}_a)^{-1} \mathbf{B}^\top + (\mathbf{X}^\top \boldsymbol{\Sigma}^{-1} \mathbf{X})^{-1}</annotation></semantics></math>.
The uncorrected variance-covariance matrix
(<code>var_correct = FALSE</code>) is given by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐗</mi><mi>⊤</mi></msup><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐗</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\text{Cov}(\hat{\boldsymbol{\beta}}) = (\mathbf{X}^\top \boldsymbol{\Sigma}^{-1} \mathbf{X})^{-1}</annotation></semantics></math>.</p>
</div>
<div class="section level3">
<h3 id="sec:distributions-glm">Distribution Parameterizations<a class="anchor" aria-label="anchor" href="#sec:distributions-glm"></a>
</h3>
<p>Below we provide definitions associated with each of the six
generalized linear models families available in <code>spmodel</code>.
Note that the poisson, binomial, gamma, and inverse Gaussian
distributions are members of the exponential family, and their deviance
is typically expressed as twice the difference in log-likelihoods
between the saturated and fitted model times the dispersion parameter
(consistent with <code><a href="https://rdrr.io/r/stats/glm.html" class="external-link">glm()</a></code>). The negative binomial and beta
distributions are members of the exponential family when the dispersion
parameter
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>φ</mi><annotation encoding="application/x-tex">\varphi</annotation></semantics></math>)
is known, and their deviance is typically expressed as just twice the
difference in log-likelihoods between the saturated and fitted model
(consistent with <code>glm.nb()</code> from <code>MASS</code> <span class="citation">(Venables and Ripley 2002)</span> for negative binomial
regression and <code>betareg()</code> from <code>betareg</code> <span class="citation">(Cribari-Neto and Zeileis 2010)</span> for beta
regression).</p>
<div class="section level4">
<h4 id="sec:poisson">Poisson Distribution<a class="anchor" aria-label="anchor" href="#sec:poisson"></a>
</h4>
<p>The Poisson distribution is defined as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>μ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mrow><msup><mi>μ</mi><mi>y</mi></msup><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mo>−</mo><mi>μ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>y</mi><mi>!</mi></mrow></mfrac><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
  f(y | \mu) = \frac{\mu^y \exp(-\mu)}{y!},
\end{equation*}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>
is a non-negative integer,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\mu &gt; 0</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>μ</mi></mrow><annotation encoding="application/x-tex">\text{E}(y) = \mu</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">Var</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>μ</mi></mrow><annotation encoding="application/x-tex">\text{Var}(y) = \mu</annotation></semantics></math>.</p>
<p>The log-likelihood (of a single observation) is defined as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>ln</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>μ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>y</mi><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>μ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>μ</mi><mo>−</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mi>!</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \ln f(y | \mu) = y \ln(\mu) - \mu - \ln(y!).
\end{equation*}</annotation></semantics></math></p>
<p>Using the inverse log link and writing in terms of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi><mo>=</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mu = \exp(w)</annotation></semantics></math>,
the log-likelihood can be written as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>ln</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>μ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>y</mi><mi>w</mi><mo>−</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mi>!</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \ln f(y | \mu) = yw - \exp(w) - \ln(y!).
\end{equation*}</annotation></semantics></math></p>
<p>The derivative of the Poisson distribution with respect to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>w</mi><annotation encoding="application/x-tex">w</annotation></semantics></math>
is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mi>d</mi><mrow><mi>d</mi><mi>w</mi></mrow></mfrac><mo>ln</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>μ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>y</mi><mo>−</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \frac{d}{d w} \ln f(y | \mu) = y - \exp(w).
\end{equation*}</annotation></semantics></math></p>
<p>The second derivative of the Poisson distribution with respect to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>w</mi><annotation encoding="application/x-tex">w</annotation></semantics></math>
is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><msup><mi>d</mi><mn>2</mn></msup><mrow><mi>d</mi><msup><mi>w</mi><mn>2</mn></msup></mrow></mfrac><mo>ln</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>μ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>−</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \frac{d^2}{d w^2} \ln f(y | \mu) = - \exp(w).
\end{equation*}</annotation></semantics></math></p>
<p>Twice the log-likelihood of the saturated model is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>ln</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐲</mi><mo stretchy="false" form="prefix">|</mo><msub><mi>𝛍</mi><mi>s</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><munder><mo>∑</mo><mi>i</mi></munder><msub><mi>y</mi><mi>i</mi></msub><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mi>!</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
  2 \ln f(\mathbf{y} | \boldsymbol{\mu}_s) = \sum_i y_i \ln(y_i) - y_i - \ln(y_i!).
\end{equation*}</annotation></semantics></math></p>
<p>Twice the log-likelihood of the fitted (observed) model is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>ln</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐲</mi><mo stretchy="false" form="prefix">|</mo><mover><mi>𝛍</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><munder><mo>∑</mo><mi>i</mi></munder><msub><mi>y</mi><mi>i</mi></msub><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>μ</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><msub><mover><mi>μ</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><mo>−</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mi>!</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
  2 \ln f(\mathbf{y} | \hat{\boldsymbol{\mu}}) = \sum_i y_i \ln(\hat{\mu}_i) - \hat{\mu}_i - \ln(y_i!).
\end{equation*}</annotation></semantics></math></p>
<p>Thus the deviance is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><munder><mo>∑</mo><mi>i</mi></munder><msub><mi>y</mi><mi>i</mi></msub><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mi>/</mi><msub><mover><mi>μ</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mover><mi>μ</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
  2 \sum_i y_i log(y_i / \hat{\mu}_i) - (y_i - \hat{\mu}_i).
\end{equation*}</annotation></semantics></math></p>
</div>
<div class="section level4">
<h4 id="sec:nbinomial">Negative Binomial Distribution<a class="anchor" aria-label="anchor" href="#sec:nbinomial"></a>
</h4>
<p>The negative binomial distribution is defined as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>μ</mi><mo>,</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mrow><mi>Γ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo>+</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>Γ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>y</mi><mi>!</mi></mrow></mfrac><msup><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mi>μ</mi><mrow><mi>μ</mi><mo>+</mo><mi>φ</mi></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mi>y</mi></msup><msup><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mi>φ</mi><mrow><mi>μ</mi><mo>+</mo><mi>φ</mi></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mi>φ</mi></msup><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
  f(y | \mu, \varphi) = \frac{\Gamma(y + \varphi)}{\Gamma(\varphi) y!} \left( \frac{\mu}{\mu + \varphi} \right)^y \left( \frac{\varphi}{\mu + \varphi} \right)^\varphi,
\end{equation*}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>
is a non-negative integer,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\mu &gt; 0</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>φ</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\varphi &gt; 0</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>μ</mi></mrow><annotation encoding="application/x-tex">\text{E}(y) = \mu</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">Var</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>μ</mi><mo>+</mo><mfrac><msup><mi>μ</mi><mn>2</mn></msup><mi>φ</mi></mfrac></mrow><annotation encoding="application/x-tex">\text{Var}(y) = \mu + \frac{\mu^2}{\varphi}</annotation></semantics></math>.</p>
<p>The log-likelihood (of a single observation) is defined as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>ln</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>μ</mi><mo>,</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>Γ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo>+</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>Γ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mi>!</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>y</mi><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>μ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>y</mi><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>μ</mi><mo>+</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>φ</mi><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>φ</mi><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>μ</mi><mo>+</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \ln f(y | \mu, \varphi) = \ln(\Gamma(y + \varphi)) - \ln(\Gamma(\varphi)) - \ln(y!) + y \ln(\mu) - y \ln(\mu + \varphi) + \varphi \ln(\varphi) - \varphi \ln(\mu + \varphi).
\end{equation*}</annotation></semantics></math></p>
<p>Using the inverse log link and writing in terms of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi><mo>=</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mu = \exp(w)</annotation></semantics></math>,
the log-likelihood can be written as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>ln</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>μ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>Γ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo>+</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>Γ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mi>!</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>y</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>w</mi><mo>−</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mo>+</mo><mi>φ</mi><mrow><mo stretchy="true" form="prefix">[</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \ln f(y | \mu) = \ln(\Gamma(y + \varphi)) - \ln(\Gamma(\varphi)) - \ln(y!) + y [w - \ln(\exp(w) + \varphi)] + \varphi [\ln(\varphi) - \ln(\exp(w) + \varphi)].
\end{equation*}</annotation></semantics></math></p>
<p>The derivative of the negative binomial distribution with respect to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>w</mi><annotation encoding="application/x-tex">w</annotation></semantics></math>
is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mfrac><mi>d</mi><mrow><mi>d</mi><mi>w</mi></mrow></mfrac><mo>ln</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>μ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mi>y</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mfrac><mrow><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>φ</mi></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>φ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo>−</mo><mfrac><mrow><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>φ</mi></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mfrac><mrow><mi>y</mi><mi>φ</mi></mrow><mrow><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>φ</mi></mrow></mfrac><mo>−</mo><mfrac><mrow><mi>φ</mi><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>φ</mi></mrow></mfrac></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mfrac><mrow><mi>φ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo>−</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>φ</mi></mrow></mfrac><mi>.</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{equation*}
\begin{split}
  \frac{d}{d w} \ln f(y | \mu) &amp; = y\left(1 - \frac{\exp(w)}{\exp(w) + \varphi}\right) + \varphi \left(- \frac{\exp(w)}{\exp(w) + \varphi} \right) \\ 
  &amp; = \frac{y\varphi}{\exp(w) + \varphi} - \frac{\varphi \exp(w)}{\exp(w) + \varphi} \\
  &amp; =  \frac{\varphi (y - \exp(w))}{\exp(w) + \varphi}.
\end{split}
\end{equation*}</annotation></semantics></math></p>
<p>The second derivative of the negative binomial distribution with
respect to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>w</mi><annotation encoding="application/x-tex">w</annotation></semantics></math>
is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><msup><mi>d</mi><mn>2</mn></msup><mrow><mi>d</mi><msup><mi>w</mi><mn>2</mn></msup></mrow></mfrac><mo>ln</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>μ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>−</mo><mfrac><mrow><mi>φ</mi><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo>+</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>φ</mi><mo>+</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mfrac></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \frac{d^2}{d w^2} \ln f(y | \mu) = - \frac{\varphi \exp(w) (y + \varphi)}{(\varphi + \exp(w))^2}
\end{equation*}</annotation></semantics></math></p>
<p>Twice the log-likelihood of the saturated model is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>ln</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐲</mi><mo stretchy="false" form="prefix">|</mo><msub><mi>𝛍</mi><mi>s</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><munder><mo>∑</mo><mi>i</mi></munder><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>Γ</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>+</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>Γ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mi>!</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><msub><mi>y</mi><mi>i</mi></msub><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>+</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>φ</mi><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>φ</mi><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>+</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
  2 \ln f(\mathbf{y} | \boldsymbol{\mu}_s) = \sum_i \ln(\Gamma(y_i + \varphi)) - \ln(\Gamma(\varphi)) - \ln(y_i!) + y_i \ln(y_i) - y_i \ln(y_i + \varphi) + \varphi \ln(\varphi) - \varphi \ln(y_i + \varphi).
\end{equation*}</annotation></semantics></math></p>
<p>Twice the log-likelihood of the fitted (observed) model is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>ln</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐲</mi><mo stretchy="false" form="prefix">|</mo><mover><mi>𝛍</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><munder><mo>∑</mo><mi>i</mi></munder><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>Γ</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>+</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>Γ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mi>!</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><msub><mi>y</mi><mi>i</mi></msub><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>μ</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>μ</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><mo>+</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>φ</mi><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>φ</mi><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>μ</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><mo>+</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
  2 \ln f(\mathbf{y} | \hat{\boldsymbol{\mu}}) = \sum_i \ln(\Gamma(y_i + \varphi)) - \ln(\Gamma(\varphi)) - \ln(y_i!) + y_i \ln(\hat{\mu}_i) - y_i \ln(\hat{\mu}_i + \varphi) + \varphi \ln(\varphi) - \varphi \ln(\hat{\mu}_i + \varphi).
\end{equation*}</annotation></semantics></math></p>
<p>Thus the deviance is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><munder><mo>∑</mo><mi>i</mi></munder><msub><mi>y</mi><mi>i</mi></msub><mrow><mo stretchy="true" form="prefix">[</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>+</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>μ</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>μ</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><mo>+</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mo>+</mo><mi>φ</mi><mrow><mo stretchy="true" form="prefix">[</mo><mo>−</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>+</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>μ</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><mo>+</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
  2 \sum_i y_i [\ln(y_i) - \ln(y_i + \varphi) - \ln(\hat{\mu}_i) + \ln(\hat{\mu}_i + \varphi)] + \varphi [ - \ln(y_i + \varphi) + \ln(\hat{\mu}_i + \varphi)].
\end{equation*}</annotation></semantics></math></p>
</div>
<div class="section level4">
<h4 id="sec:binomial">Binomial Distribution<a class="anchor" aria-label="anchor" href="#sec:binomial"></a>
</h4>
<p>The binomial distribution is defined as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>μ</mi><mo>,</mo><mi>m</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mfrac linethickness="0"><mi>m</mi><mi>y</mi></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><msup><mi>μ</mi><mi>y</mi></msup><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>μ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>m</mi><mo>−</mo><mi>y</mi></mrow></msup><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
  f(y | \mu, m) = \binom{m}{y} \mu^y (1 - \mu)^{m - y},
\end{equation*}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>
is the (known) number of Bernoulli trials,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>
is a non-negative integer,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn><mo>≤</mo><mi>μ</mi><mo>≤</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">0 \le \mu \le 1</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>m</mi><mi>μ</mi></mrow><annotation encoding="application/x-tex">\text{E}(y) = m\mu</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">Var</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>m</mi><mi>μ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>μ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\text{Var}(y) = m\mu (1 - \mu)</annotation></semantics></math>.</p>
<p>The log-likelihood (of a single observation) is defined as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>ln</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>μ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">[</mo><mrow><mo stretchy="true" form="prefix">(</mo><mfrac linethickness="0"><mi>m</mi><mi>y</mi></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mo>+</mo><mi>y</mi><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>μ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>m</mi><mo>−</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>μ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \ln f(y | \mu) = \ln \left[ \binom{m}{y} \right] + y \ln(\mu) + (m - y) \ln(1 - \mu).
\end{equation*}</annotation></semantics></math></p>
<p>Using the inverse logit link and writing in terms of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi><mo>=</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>/</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>+</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mu = \exp(w) / (1 + \exp(w))</annotation></semantics></math>,
the log-likelihood can be written as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>ln</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>μ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">[</mo><mrow><mo stretchy="true" form="prefix">(</mo><mfrac linethickness="0"><mi>m</mi><mi>y</mi></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mo>+</mo><mi>y</mi><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>/</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>+</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>m</mi><mo>−</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>/</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>+</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \ln f(y | \mu) = \ln \left[ \binom{m}{y} \right] + y \ln(\exp(w) / (1 + \exp(w))) + (m - y) \ln(1 - \exp(w) / (1 + \exp(w))).
\end{equation*}</annotation></semantics></math></p>
<p>The derivative of the binomial distribution with respect to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>w</mi><annotation encoding="application/x-tex">w</annotation></semantics></math>
is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mi>d</mi><mrow><mi>d</mi><mi>w</mi></mrow></mfrac><mo>ln</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>μ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>y</mi><mo>−</mo><mfrac><mrow><mi>m</mi><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mn>1</mn><mo>+</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \frac{d}{d w} \ln f(y | \mu) = y - \frac{m \exp(w)}{1 + \exp(w)}
\end{equation*}</annotation></semantics></math></p>
<p>The second derivative of the binomial distribution with respect to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>w</mi><annotation encoding="application/x-tex">w</annotation></semantics></math>
is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><msup><mi>d</mi><mn>2</mn></msup><mrow><mi>d</mi><msup><mi>w</mi><mn>2</mn></msup></mrow></mfrac><mo>ln</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>μ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>−</mo><mfrac><mrow><mi>m</mi><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>+</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mfrac><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \frac{d^2}{d w^2} \ln f(y | \mu) = - \frac{m \exp(w)}{(1 + \exp(w))^2}.
\end{equation*}</annotation></semantics></math></p>
<p>Twice the log-likelihood of the saturated model is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>ln</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐲</mi><mo stretchy="false" form="prefix">|</mo><msub><mi>𝛍</mi><mi>s</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><munder><mo>∑</mo><mi>i</mi></munder><mo>ln</mo><mrow><mo stretchy="true" form="prefix">[</mo><mrow><mo stretchy="true" form="prefix">(</mo><mfrac linethickness="0"><msub><mi>m</mi><mi>i</mi></msub><msub><mi>y</mi><mi>i</mi></msub></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mo>+</mo><msub><mi>y</mi><mi>i</mi></msub><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>m</mi><mi>i</mi></msub><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>m</mi><mi>i</mi></msub><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
  2 \ln f(\mathbf{y} | \boldsymbol{\mu}_s) = \sum_i \ln \left[ \binom{m_i}{y_i} \right] + y_i \ln(y_i) + (m_i - y_i) \ln(m_i - y_i).
\end{equation*}</annotation></semantics></math></p>
<p>Twice the log-likelihood of the fitted (observed) model is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>ln</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐲</mi><mo stretchy="false" form="prefix">|</mo><mover><mi>𝛍</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><munder><mo>∑</mo><mi>i</mi></munder><mo>ln</mo><mrow><mo stretchy="true" form="prefix">[</mo><mrow><mo stretchy="true" form="prefix">(</mo><mfrac linethickness="0"><msub><mi>m</mi><mi>i</mi></msub><msub><mi>y</mi><mi>i</mi></msub></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mo>+</mo><msub><mi>y</mi><mi>i</mi></msub><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>m</mi><mi>i</mi></msub><msub><mover><mi>μ</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>m</mi><mi>i</mi></msub><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>m</mi><mi>i</mi></msub><mo>−</mo><msub><mi>m</mi><mi>i</mi></msub><msub><mover><mi>μ</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
  2 \ln f(\mathbf{y} | \hat{\boldsymbol{\mu}}) = \sum_i \ln \left[ \binom{m_i}{y_i} \right] + y_i \ln(m_i\hat{\mu}_i) + (m_i - y_i) \ln(m_i - m_i \hat{\mu}_i).
\end{equation*}</annotation></semantics></math></p>
<p>Thus the deviance is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><munder><mo>∑</mo><mi>i</mi></munder><msub><mi>y</mi><mi>i</mi></msub><mrow><mo stretchy="true" form="prefix">[</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>m</mi><mi>i</mi></msub><msub><mover><mi>μ</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>m</mi><mi>i</mi></msub><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>m</mi><mi>i</mi></msub><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>m</mi><mi>i</mi></msub><mo>−</mo><msub><mi>m</mi><mi>i</mi></msub><msub><mover><mi>μ</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\begin{equation*}
  2 \sum_i y_i[ \ln(y_i) - \ln(m_i \hat{\mu}_i)] + (m_i - y_i)[ \ln(m_i - y_i) - \ln(m_i - m_i \hat{\mu}_i) ]
\end{equation*}</annotation></semantics></math></p>
</div>
<div class="section level4">
<h4 id="sec:beta">Beta Distribution<a class="anchor" aria-label="anchor" href="#sec:beta"></a>
</h4>
<p>The beta distribution is defined as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>μ</mi><mo>,</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mrow><mi>Γ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>Γ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>μ</mi><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>Γ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>μ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><msup><mi>y</mi><mrow><mi>μ</mi><mi>φ</mi><mo>−</mo><mn>1</mn></mrow></msup><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>μ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>φ</mi><mo>−</mo><mn>1</mn></mrow></msup><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
  f(y | \mu, \varphi) = \frac{\Gamma(\varphi)}{\Gamma(\mu \varphi) \Gamma((1 - \mu)\varphi)} y^{\mu \varphi - 1} (1 - y)^{(1 - \mu)\varphi - 1},
\end{equation*}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn><mo>&lt;</mo><mi>y</mi><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">0 &lt; y &lt; 1</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn><mo>&lt;</mo><mi>μ</mi><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">0 &lt; \mu &lt; 1</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>μ</mi></mrow><annotation encoding="application/x-tex">\text{E}(y) = \mu</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">Var</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>μ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>μ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>/</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>+</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\text{Var}(y) = \mu (1 - \mu) / (1 + \varphi)</annotation></semantics></math>.</p>
<p>The log-likelihood (of a single observation) is defined as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>ln</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>μ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>Γ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>Γ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>μ</mi><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>Γ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>μ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>μ</mi><mi>φ</mi><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>μ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>φ</mi><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \ln f(y | \mu) = \ln(\Gamma(\varphi)) - \ln(\Gamma(\mu \varphi)) - \ln(\Gamma((1 - \mu)\varphi)) +  (\mu \varphi - 1) \ln(y) + ((1 - \mu)\varphi - 1) \ln(1 - y).
\end{equation*}</annotation></semantics></math></p>
<p>Using the inverse logit link and writing in terms of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi><mo>=</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mu = \exp(w)</annotation></semantics></math>,
the log-likelihood can be written as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mo>ln</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>μ</mi><mo>,</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>Γ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>Γ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mn>1</mn><mo>+</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>Γ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mfrac><mrow><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mn>1</mn><mo>+</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mn>1</mn><mo>+</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mi>φ</mi><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mfrac><mrow><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mn>1</mn><mo>+</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mi>φ</mi><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{equation*}
\begin{split}
  \ln f(y | \mu, \varphi) &amp; = \ln(\Gamma(\varphi)) - \ln(\Gamma(\frac{\exp(w)}{1 + \exp(w)} \varphi)) - \ln(\Gamma((1 - \frac{\exp(w)}{1 + \exp(w)})\varphi)) \\
  &amp; +  (\frac{\exp(w)}{1 + \exp(w)} \varphi - 1) \ln(y) + ((1 - \frac{\exp(w)}{1 + \exp(w)})\varphi - 1) \ln(1 - y).
\end{split}
\end{equation*}</annotation></semantics></math></p>
<p>It can be shown that the derivative of beta distribution with respect
to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>w</mi><annotation encoding="application/x-tex">w</annotation></semantics></math>
is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mi>d</mi><mrow><mi>d</mi><mi>w</mi></mrow></mfrac><mo>ln</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>μ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>−</mo><mfrac><mrow><mi>φ</mi><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow><msub><mi>k</mi><mn>0</mn></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="false" form="prefix">|</mo><mi>y</mi><mo>,</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>+</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mfrac><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \frac{d}{d w} \ln f(y | \mu) = - \frac{\varphi \exp(w) k_0(w | y, \varphi)}{(1 + \exp(w))^2},
\end{equation*}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>k</mi><mn>0</mn></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="false" form="prefix">|</mo><mi>y</mi><mo>,</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>ψ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>φ</mi><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mn>1</mn><mo>+</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><msup><mi>ψ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mi>φ</mi><mrow><mn>1</mn><mo>+</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mn>1</mn><mi>y</mi></mfrac><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">k_0(w | y, \varphi) = \psi^{(0)}(\frac{\varphi \exp(w)}{1 + \exp(w)}) - \psi^{(0)}(\frac{\varphi}{1 + \exp(w)}) + \ln( \frac{1}{y} - 1)</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>ψ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\psi^{(0)}</annotation></semantics></math>
is the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn><mi>t</mi><mi>h</mi></mrow><annotation encoding="application/x-tex">0th</annotation></semantics></math>
derivative of the digamma function.</p>
<p>It can be shown that the second derivative of the beta distribution
with respect to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>w</mi><annotation encoding="application/x-tex">w</annotation></semantics></math>
is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><msup><mi>d</mi><mn>2</mn></msup><mrow><mi>d</mi><msup><mi>w</mi><mn>2</mn></msup></mrow></mfrac><mo>ln</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>μ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>−</mo><mfrac><mrow><mi>φ</mi><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow><msub><mi>k</mi><mn>1</mn></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="false" form="prefix">|</mo><mi>y</mi><mo>,</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>+</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mn>4</mn></msup></mfrac><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \frac{d^2}{d w^2} \ln f(y | \mu) = - \frac{\varphi \exp(2w) k_1(w | y, \varphi)}{(1 + \exp(w))^4},
\end{equation*}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>k</mi><mn>1</mn></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="false" form="prefix">|</mo><mi>y</mi><mo>,</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>φ</mi><mrow><mo stretchy="true" form="prefix">[</mo><msup><mi>ψ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>φ</mi><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mn>1</mn><mo>+</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><msup><mi>ψ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mi>φ</mi><mrow><mn>1</mn><mo>+</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mo>−</mo><mn>2</mn><mo>sinh</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><msub><mi>k</mi><mn>0</mn></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="false" form="prefix">|</mo><mi>y</mi><mo>,</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mn>2</mn><msup><mo>tanh</mo><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mn>2</mn><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">k_1(w | y, \varphi) = \varphi [\psi^{(1)}(\frac{\varphi \exp(w)}{1 + \exp(w)}) + \psi^{(1)}( \frac{\varphi}{1 + \exp(w)})] - 2\sinh(w)[k_0(w | y, \varphi) + 2\tanh^{-1}(1 - 2y)]</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>ψ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\psi^{(n)}</annotation></semantics></math>
is the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mi>t</mi><mi>h</mi></mrow><annotation encoding="application/x-tex">nth</annotation></semantics></math>
derivative of the digamma function.</p>
<p>Twice the log-likelihood of the saturated model is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>ln</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐲</mi><mo stretchy="false" form="prefix">|</mo><msub><mi>𝛍</mi><mi>s</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><munder><mo>∑</mo><mi>i</mi></munder><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>Γ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>Γ</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>Γ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mi>φ</mi><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>φ</mi><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
  2 \ln f(\mathbf{y} | \boldsymbol{\mu}_s) = \sum_i \ln(\Gamma(\varphi)) - \ln(\Gamma(y_i \varphi)) - \ln(\Gamma((1 - y_i)\varphi)) +  (y_i \varphi - 1) \ln(y_i) + ((1 - y_i)\varphi - 1) \ln(1 - y_i).
\end{equation*}</annotation></semantics></math></p>
<p>Twice the log-likelihood of the fitted (observed) model is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>ln</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐲</mi><mo stretchy="false" form="prefix">|</mo><mover><mi>𝛍</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>Γ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>Γ</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>μ</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>Γ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><msub><mover><mi>μ</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>μ</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><mi>φ</mi><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><msub><mover><mi>μ</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>φ</mi><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
  2 \ln f(\mathbf{y} | \hat{\boldsymbol{\mu}}) = \ln(\Gamma(\varphi)) - \ln(\Gamma(\hat{\mu}_i \varphi)) - \ln(\Gamma((1 - \hat{\mu}_i)\varphi)) +  (\hat{\mu}_i \varphi - 1) \ln(y_i) + ((1 - \hat{\mu}_i)\varphi - 1) \ln(1 - y_i).
\end{equation*}</annotation></semantics></math></p>
<p>Thus the deviance is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><munder><mo>∑</mo><mi>i</mi></munder><mo>−</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>Γ</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>Γ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>Γ</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>μ</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>Γ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><msub><mover><mi>μ</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mover><mi>μ</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>φ</mi><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>μ</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>φ</mi><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\begin{equation*}
  2 \sum_i - \ln(\Gamma(y_i \varphi)) - \ln(\Gamma((1 - y_i) \varphi )) + \ln(\Gamma(\hat{\mu}_i \varphi)) + \ln(\Gamma((1 - \hat{\mu}_i) \varphi )) + (y_i - \hat{\mu}_i) \varphi \ln(y_i) + (\hat{\mu}_i - y_i) \varphi \ln(1 - y_i)
\end{equation*}</annotation></semantics></math></p>
<p>Sometimes the deviance contribution from the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>th
observation can be computationally unstable and yield a negative value
<span class="citation">(Espinheira, Ferrari, and Cribari-Neto
2008)</span>. This can happen, for example, when
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding="application/x-tex">y_i</annotation></semantics></math>
is close to zero or one. When this happens, the deviance contribution is
truncated to zero to reflect the fact that the theoretical deviance
contribution must be non-negative.</p>
</div>
<div class="section level4">
<h4 id="sec:gamma">Gamma Distribution<a class="anchor" aria-label="anchor" href="#sec:gamma"></a>
</h4>
<p>The gamma distribution is defined as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>μ</mi><mo>,</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Γ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><msup><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mi>φ</mi><mi>μ</mi></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mi>φ</mi></msup><msup><mi>y</mi><mrow><mi>φ</mi><mo>−</mo><mn>1</mn></mrow></msup><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mo>−</mo><mi>y</mi><mi>φ</mi></mrow><mi>μ</mi></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
  f(y | \mu, \varphi) = \frac{1}{\Gamma(\varphi)} \left( \frac{\varphi}{\mu} \right)^\varphi y^{\varphi - 1} \exp(\frac{-y \varphi}{\mu}),
\end{equation*}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">y &gt; 0</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\mu &gt; 0</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>μ</mi></mrow><annotation encoding="application/x-tex">\text{E}(y) = \mu</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">Var</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>μ</mi><mn>2</mn></msup><mi>/</mi><mi>φ</mi></mrow><annotation encoding="application/x-tex">\text{Var}(y) = \mu^2/\varphi</annotation></semantics></math>.</p>
<p>The log-likelihood (of a single observation) is defined as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>ln</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>μ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>−</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>Γ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>φ</mi><mrow><mo stretchy="true" form="prefix">[</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>μ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>φ</mi><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mfrac><mrow><mi>y</mi><mi>φ</mi></mrow><mi>μ</mi></mfrac><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \ln f(y | \mu) = - \ln(\Gamma(\varphi)) + \varphi [\ln(\varphi) - \ln(\mu)] + (\varphi - 1) \ln(y) - \frac{y \varphi}{\mu}.
\end{equation*}</annotation></semantics></math></p>
<p>Using the inverse log link and writing in terms of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi><mo>=</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mu = \exp(w)</annotation></semantics></math>,
the log-likelihood can be written as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>ln</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>μ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>−</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>Γ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mi>φ</mi><mrow><mo stretchy="true" form="prefix">[</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>w</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>φ</mi><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mfrac><mrow><mi>y</mi><mi>φ</mi></mrow><mrow><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \ln f(y | \mu) = - \ln(\Gamma(\varphi)) \varphi [\ln(\varphi) - w] + (\varphi - 1) \ln(y) - \frac{y \varphi}{\exp(w)}.
\end{equation*}</annotation></semantics></math></p>
<p>The derivative of the gamma distribution with respect to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>w</mi><annotation encoding="application/x-tex">w</annotation></semantics></math>
is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mi>d</mi><mrow><mi>d</mi><mi>w</mi></mrow></mfrac><mo>ln</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>μ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>−</mo><mi>φ</mi><mo>+</mo><mfrac><mrow><mi>φ</mi><mi>y</mi></mrow><mrow><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \frac{d}{d w} \ln f(y | \mu) = -\varphi + \frac{\varphi y}{\exp(w)}.
\end{equation*}</annotation></semantics></math></p>
<p>The second derivative of the gamma distribution with respect to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>w</mi><annotation encoding="application/x-tex">w</annotation></semantics></math>
is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><msup><mi>d</mi><mn>2</mn></msup><mrow><mi>d</mi><msup><mi>w</mi><mn>2</mn></msup></mrow></mfrac><mo>ln</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>μ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>−</mo><mfrac><mrow><mi>φ</mi><mi>y</mi></mrow><mrow><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \frac{d^2}{d w^2} \ln f(y | \mu) = - \frac{\varphi y}{\exp(w)}.
\end{equation*}</annotation></semantics></math></p>
<p>Twice the log-likelihood of the saturated model is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>ln</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐲</mi><mo stretchy="false" form="prefix">|</mo><msub><mi>𝛍</mi><mi>s</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>−</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>Γ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>φ</mi><mrow><mo stretchy="true" form="prefix">[</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>φ</mi><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mfrac><mrow><msub><mi>y</mi><mi>i</mi></msub><mi>φ</mi></mrow><msub><mi>y</mi><mi>i</mi></msub></mfrac><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
  2 \ln f(\mathbf{y} | \boldsymbol{\mu}_s) = - \ln(\Gamma(\varphi)) + \varphi [\ln(\varphi) - \ln(y_i)] + (\varphi - 1) \ln(y_i) - \frac{y_i \varphi}{y_i}.
\end{equation*}</annotation></semantics></math></p>
<p>Twice the log-likelihood of the fitted (observed) model is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>ln</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐲</mi><mo stretchy="false" form="prefix">|</mo><mover><mi>𝛍</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><munder><mo>∑</mo><mi>i</mi></munder><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>Γ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>φ</mi><mrow><mo stretchy="true" form="prefix">[</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>μ</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>φ</mi><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mfrac><mrow><msub><mi>y</mi><mi>i</mi></msub><mi>φ</mi></mrow><msub><mover><mi>μ</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub></mfrac><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
  2 \ln f(\mathbf{y} | \hat{\boldsymbol{\mu}}) = \sum_i y_i - \ln(\Gamma(\varphi)) + \varphi [\ln(\varphi) - \ln(\hat{\mu}_i)] + (\varphi - 1) \ln(y_i) - \frac{y_i \varphi}{\hat{\mu}_i}.
\end{equation*}</annotation></semantics></math></p>
<p>Thus the deviance is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><munder><mo>∑</mo><mi>i</mi></munder><mo>−</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mi>y</mi><msub><mover><mi>μ</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mfrac><mrow><mi>y</mi><mo>−</mo><msub><mover><mi>μ</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub></mrow><msub><mover><mi>μ</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub></mfrac></mrow><annotation encoding="application/x-tex">\begin{equation*}
  2 \sum_i - \ln(\frac{y}{\hat{\mu}_i}) + \frac{y - \hat{\mu}_i}{\hat{\mu}_i}
\end{equation*}</annotation></semantics></math> after scaling by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>φ</mi><annotation encoding="application/x-tex">\varphi</annotation></semantics></math>.</p>
</div>
<div class="section level4">
<h4 id="sec:invgauss">Inverse Gaussian Distribution<a class="anchor" aria-label="anchor" href="#sec:invgauss"></a>
</h4>
<p>The inverse Gaussian distribution is defined as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>μ</mi><mo>,</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msqrt><mfrac><mrow><mi>φ</mi><mi>μ</mi></mrow><mrow><mn>2</mn><mi>π</mi><msup><mi>y</mi><mn>3</mn></msup></mrow></mfrac></msqrt><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mo>−</mo><mfrac><mrow><mi>φ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo>−</mo><msup><mi>μ</mi><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mn>2</mn><mi>μ</mi><mi>y</mi></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
  f(y | \mu, \varphi) = \sqrt{\frac{\varphi \mu}{2 \pi y^3}} \exp \left( - \frac{\varphi (y - \mu^2)}{2 \mu y} \right),
\end{equation*}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">y &gt; 0</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\mu &gt; 0</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>μ</mi></mrow><annotation encoding="application/x-tex">\text{E}(y) = \mu</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">Var</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>μ</mi><mn>2</mn></msup><mi>/</mi><mi>φ</mi></mrow><annotation encoding="application/x-tex">\text{Var}(y) = \mu^2/\varphi</annotation></semantics></math>.</p>
<p>The log-likelihood (of a single observation) is defined as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>ln</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>μ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mrow><mo stretchy="true" form="prefix">[</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>φ</mi><mi>/</mi><mn>2</mn><mi>π</mi><msup><mi>y</mi><mn>3</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>μ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mo>−</mo><mi>φ</mi><mfrac><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo>−</mo><mi>μ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mrow><mn>2</mn><mi>μ</mi><mi>y</mi></mrow></mfrac><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \ln f(y | \mu) = \frac{1}{2}[\ln(\varphi / 2 \pi y^3)  + \ln(\mu)] - \varphi \frac{(y - \mu)^2}{2 \mu y}.
\end{equation*}</annotation></semantics></math></p>
<p>Using the inverse log link and writing in terms of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi><mo>=</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mu = \exp(w)</annotation></semantics></math>,
the log-likelihood can be written as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>ln</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>μ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mrow><mo stretchy="true" form="prefix">[</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>φ</mi><mi>/</mi><mn>2</mn><mi>π</mi><msup><mi>y</mi><mn>3</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>w</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>−</mo><mi>φ</mi><mfrac><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo>−</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mrow><mn>2</mn><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>y</mi></mrow></mfrac><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \ln f(y | \mu) = \frac{1}{2}[\ln(\varphi / 2 \pi y^3)  + w] - \varphi \frac{(y - \exp(w))^2}{2 \exp(w) y}.
\end{equation*}</annotation></semantics></math></p>
<p>The derivative of the gamma distribution with respect to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>w</mi><annotation encoding="application/x-tex">w</annotation></semantics></math>
is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mi>d</mi><mrow><mi>d</mi><mi>w</mi></mrow></mfrac><mo>ln</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>μ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>φ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mi>y</mi><mrow><mn>2</mn><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mo>−</mo><mfrac><mrow><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mn>2</mn><mi>y</mi></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \frac{d}{d w} \ln f(y | \mu) = \varphi \left( \frac{y}{2 \exp(w)} - \frac{\exp(w)}{2y} \right) + \frac{1}{2}.
\end{equation*}</annotation></semantics></math></p>
<p>The second derivative of the gamma distribution with respect to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>w</mi><annotation encoding="application/x-tex">w</annotation></semantics></math>
is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><msup><mi>d</mi><mn>2</mn></msup><mrow><mi>d</mi><msup><mi>w</mi><mn>2</mn></msup></mrow></mfrac><mo>ln</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>μ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>−</mo><mfrac><mrow><mi>φ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><msup><mi>y</mi><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mn>2</mn><mi>y</mi><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
  \frac{d^2}{d w^2} \ln f(y | \mu) = - \frac{\varphi(\exp(2w) + y^2)}{2y\exp(w)}.
\end{equation*}</annotation></semantics></math></p>
<p>Note that this is not a typical parameterization of the inverse
Gaussian distribution. The typical parameterization of the inverse
Gaussian distribution is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>μ</mi><mo>,</mo><mi>λ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msqrt><mfrac><mi>λ</mi><mrow><mn>2</mn><mi>π</mi><msup><mi>y</mi><mn>3</mn></msup></mrow></mfrac></msqrt><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mo>−</mo><mfrac><mrow><mi>λ</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo>−</mo><mi>μ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mrow><mrow><mn>2</mn><msup><mi>u</mi><mn>2</mn></msup><mi>y</mi></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
 f(y | \mu, \lambda) = \sqrt{\frac{\lambda}{2 \pi y^3}} \exp \left( - \frac{\lambda(y - \mu)^2}{2u^2y} \right),
\end{equation*}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">y &gt; 0</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\mu &gt; 0</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>μ</mi></mrow><annotation encoding="application/x-tex">\text{E}(y) = \mu</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">Var</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>μ</mi><mn>3</mn></msup><mi>/</mi><mi>λ</mi></mrow><annotation encoding="application/x-tex">\text{Var}(y) = \mu^3/\lambda</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi><mo>=</mo><mi>μ</mi><mi>φ</mi></mrow><annotation encoding="application/x-tex">\lambda = \mu \varphi</annotation></semantics></math>.</p>
<p>Twice the log-likelihood of the saturated model is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>ln</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐲</mi><mo stretchy="false" form="prefix">|</mo><msub><mi>𝛍</mi><mi>s</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mi>λ</mi><mrow><mn>2</mn><mi>π</mi><msubsup><mi>y</mi><mi>i</mi><mn>3</mn></msubsup></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\begin{equation*}
  2 \ln f(\mathbf{y} | \boldsymbol{\mu}_s) = \frac{1}{2} \left( \frac{\lambda}{2 \pi y_i^3} \right)
\end{equation*}</annotation></semantics></math></p>
<p>Twice the log-likelihood of the fitted (observed) model is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>ln</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐲</mi><mo stretchy="false" form="prefix">|</mo><mover><mi>𝛍</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mi>λ</mi><mrow><mn>2</mn><mi>π</mi><msubsup><mi>y</mi><mi>i</mi><mn>3</mn></msubsup></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mfrac><mrow><mi>λ</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mover><mi>μ</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mrow><mrow><mn>2</mn><msubsup><mover><mi>μ</mi><mo accent="true">̂</mo></mover><mi>i</mi><mn>2</mn></msubsup><msub><mi>y</mi><mi>i</mi></msub></mrow></mfrac><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
  2 \ln f(\mathbf{y} | \hat{\boldsymbol{\mu}}) = \frac{1}{2} \left( \frac{\lambda}{2 \pi y_i^3} \right) - \frac{\lambda (y_i - \hat{\mu}_i)^2}{2 \hat{\mu}_i^2 y_i}.
\end{equation*}</annotation></semantics></math></p>
<p>Thus the deviance is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><munder><mo>∑</mo><mi>i</mi></munder><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mover><mi>μ</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mi>/</mi><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mover><mi>μ</mi><mo accent="true">̂</mo></mover><mi>i</mi><mn>2</mn></msubsup><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
 \sum_i (y_i - \hat{\mu}_i)^2 / (\hat{\mu}_i^2 y_i),
\end{equation*}</annotation></semantics></math> after scaling by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math>.</p>
</div>
<div class="section level4">
<h4 id="sec:di_and_Di">Table of Inverse Link Functions,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>d</mi><mi>i</mi></msub><annotation encoding="application/x-tex">d_i</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>D</mi><mrow><mi>i</mi><mo>,</mo><mi>i</mi></mrow></msub><annotation encoding="application/x-tex">D_{i, i}</annotation></semantics></math><a class="anchor" aria-label="anchor" href="#sec:di_and_Di"></a>
</h4>
<p>The following table contains a table of inverse link functions,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>d</mi><mi>i</mi></msub><annotation encoding="application/x-tex">d_i</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>D</mi><mrow><mi>i</mi><mo>,</mo><mi>i</mi></mrow></msub><annotation encoding="application/x-tex">D_{i, i}</annotation></semantics></math>
for each spatial generalized linear model family. See more details for
each family in the previous subsections.</p>
<table class="table">
<caption>A table of inverse link functions and relevant quantities for
each spatial generalized linear model family.</caption>
<colgroup>
<col width="23%">
<col width="53%">
<col width="8%">
<col width="13%">
</colgroup>
<thead><tr class="header">
<th>Family</th>
<th><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mo>=</mo><msup><mi>g</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">u = g^{-1}(w)</annotation></semantics></math></th>
<th><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>d</mi><mi>i</mi></msub><annotation encoding="application/x-tex">d_i</annotation></semantics></math></th>
<th><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>D</mi><mrow><mi>i</mi><mo>,</mo><mi>i</mi></mrow></msub><annotation encoding="application/x-tex">D_{i,i}</annotation></semantics></math></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Poisson</td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi><mo>=</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mu = \exp(w)</annotation></semantics></math></td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">y_i - \exp(w_i)</annotation></semantics></math></td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">- \exp(w_i)</annotation></semantics></math></td>
</tr>
<tr class="even">
<td>Negative Binomial</td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi><mo>=</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mu = \exp(w)</annotation></semantics></math></td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mrow><mi>φ</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>φ</mi></mrow></mfrac><annotation encoding="application/x-tex">\frac{\varphi (y_i - \exp(w_i))}{\exp(w_i) + \varphi}</annotation></semantics></math></td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mfrac><mrow><mi>φ</mi><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>+</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>φ</mi><mo>+</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mfrac></mrow><annotation encoding="application/x-tex">- \frac{\varphi \exp(w_i) (y_i + \varphi)}{(\varphi + \exp(w_i))^2}</annotation></semantics></math></td>
</tr>
<tr class="odd">
<td>Binomial</td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi><mo>=</mo><mfrac><mrow><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mn>1</mn><mo>+</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac></mrow><annotation encoding="application/x-tex">\mu = \frac{\exp(w)}{1 + \exp(w)}</annotation></semantics></math></td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><mfrac><mrow><msub><mi>m</mi><mi>i</mi></msub><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mn>1</mn><mo>+</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac></mrow><annotation encoding="application/x-tex">y_i - \frac{m_i \exp(w_i)}{1 + \exp(w_i)}</annotation></semantics></math></td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mfrac><mrow><msub><mi>m</mi><mi>i</mi></msub><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>+</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mfrac></mrow><annotation encoding="application/x-tex">- \frac{m_i \exp(w_i)}{(1 + \exp(w_i))^2}</annotation></semantics></math></td>
</tr>
<tr class="even">
<td>Beta</td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi><mo>=</mo><mfrac><mrow><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mn>1</mn><mo>+</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac></mrow><annotation encoding="application/x-tex">\mu = \frac{\exp(w)}{1 + \exp(w)}</annotation></semantics></math></td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mfrac><mrow><mi>φ</mi><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><msub><mi>k</mi><mn>0</mn></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>y</mi><mi>i</mi></msub><mo>,</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>+</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mfrac></mrow><annotation encoding="application/x-tex">- \frac{\varphi \exp(w_i) k_0(w_i | y_i, \varphi)}{(1 + \exp(w_i))^2}</annotation></semantics></math></td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mfrac><mrow><mi>φ</mi><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><msub><mi>w</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><msub><mi>k</mi><mn>1</mn></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>y</mi><mi>i</mi></msub><mo>,</mo><mi>φ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>+</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mn>4</mn></msup></mfrac></mrow><annotation encoding="application/x-tex">- \frac{\varphi \exp(2w_i) k_1(w_i | y_i, \varphi)}{(1 + \exp(w_i))^4}</annotation></semantics></math></td>
</tr>
<tr class="odd">
<td>Gamma</td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi><mo>=</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mu = \exp(w)</annotation></semantics></math></td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mi>φ</mi><mo>+</mo><mfrac><mrow><mi>φ</mi><msub><mi>y</mi><mi>i</mi></msub></mrow><mrow><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac></mrow><annotation encoding="application/x-tex">-\varphi + \frac{\varphi y_i}{\exp(w_i)}</annotation></semantics></math></td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mfrac><mrow><mi>φ</mi><msub><mi>y</mi><mi>i</mi></msub></mrow><mrow><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac></mrow><annotation encoding="application/x-tex">- \frac{\varphi y_i}{\exp(w_i)}</annotation></semantics></math></td>
</tr>
<tr class="even">
<td>Inverse Gaussian</td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi><mo>=</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mu = \exp(w)</annotation></semantics></math></td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>φ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><msub><mi>y</mi><mi>i</mi></msub><mrow><mn>2</mn><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mo>−</mo><mfrac><mrow><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mn>2</mn><msub><mi>y</mi><mi>i</mi></msub></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow><annotation encoding="application/x-tex">\varphi \left( \frac{y_i}{2 \exp(w_i)} - \frac{\exp(w_i)}{2y_i} \right) + \frac{1}{2}</annotation></semantics></math></td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mfrac><mrow><mi>φ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><msub><mi>w</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><msubsup><mi>y</mi><mi>i</mi><mn>2</mn></msubsup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mn>2</mn><msub><mi>y</mi><mi>i</mi></msub><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac></mrow><annotation encoding="application/x-tex">- \frac{\varphi(\exp(2w_i) + y_i^2)}{2y_i\exp(w_i)}</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="section level2">
<h2 id="sec:esv">The Empirical Semivariogram (<code>esv()</code>)<a class="anchor" aria-label="anchor" href="#sec:esv"></a>
</h2>
<p>The empirical semivariogram is a moment-based estimate of the
theoretical semivariogram. The empirical semivariogram quantifies half
of the average squared difference in the response among observations in
several distance classes. More formally, the empirical semivariogram is
defined as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>γ</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mrow><mn>2</mn><mrow><mo stretchy="true" form="prefix">|</mo><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">|</mo></mrow></mrow></mfrac><munder><mo>∑</mo><mrow><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></munder><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mi>y</mi><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation}\label{eq:esv}
  \hat{\gamma}(h) = \frac{1}{2|N(h)|} \sum_{N(h)} (y_i - y_j)^2, 
\end{equation}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">N(h)</annotation></semantics></math>
is the set of observations in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐲</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math>
that are
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation></semantics></math>
distance apart (distance classes) and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">|</mo><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">|</mo></mrow><annotation encoding="application/x-tex">|N(h)|</annotation></semantics></math>
is the cardinality of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">N(h)</annotation></semantics></math><span class="citation">(Cressie 1993)</span>. Often the set
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">N(h)</annotation></semantics></math>
contains observations that are
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mo>±</mo><mi>c</mi></mrow><annotation encoding="application/x-tex">h \pm c</annotation></semantics></math>
apart, where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>c</mi><annotation encoding="application/x-tex">c</annotation></semantics></math>
is some constant. This approach is known as “binning” the empirical
semivariogram. The default in <code>spmodel</code> is to construct the
semivariogram using 15 equally spaced bins where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation></semantics></math>
is contained in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">(</mo><mn>0</mn><mo>,</mo><msub><mi>h</mi><mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub><mo stretchy="false" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">(0, h_{max}]</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>h</mi><mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub><annotation encoding="application/x-tex">h_{max}</annotation></semantics></math>
is known as a “distance cutoff”. Distance cutoffs are commonly used when
constructing
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>γ</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\hat{\gamma}(h)</annotation></semantics></math>
because there tend to be few pairs with large distances. The default in
<code>spmodel</code> is to use a cutoff of half the maximum distance
(hypotenuse) of the domain’s bounding box.</p>
<p>The main purpose of the empirical semivariogram is its use in
semivariogram weighted least squares estimation for spatial linear
models, though it can also be used as a visual diagnostic to assess the
fit of a spatial covariance function.</p>
</div>
<div class="section level2">
<h2 id="sec:iprod">A Note on Covariance Square Roots and Inverse Products<a class="anchor" aria-label="anchor" href="#sec:iprod"></a>
</h2>
<p>Often
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">\boldsymbol{\Sigma}^{-1}</annotation></semantics></math>
is not strictly needed for estimation, prediction, or other purposes,
but at least the product between
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">\boldsymbol{\Sigma}^{-1}</annotation></semantics></math>
and some other matrix is needed. Consider the example of the covariance
matrix of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat{\boldsymbol{\beta}}</annotation></semantics></math>
and observe
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>𝐗</mi><mi>⊤</mi></msup><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐗</mi></mrow><annotation encoding="application/x-tex">\mathbf{X}^\top \boldsymbol{\Sigma}^{-1} \mathbf{X}</annotation></semantics></math>
is needed. The most direct way to find this product is certainly to
obtain
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">\boldsymbol{\Sigma}^{-1}</annotation></semantics></math>
and then multiply by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>𝐗</mi><mi>⊤</mi></msup><annotation encoding="application/x-tex">\mathbf{X}^\top</annotation></semantics></math>
on the left and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐗</mi><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics></math>
on the right. This is both computationally expensive and cannot be used
to compute products that involve
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><annotation encoding="application/x-tex">\boldsymbol{\Sigma}^{-1/2}</annotation></semantics></math>,
which are often useful. It is helpful to define
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝚺</mi><mo>=</mo><mi>𝐒</mi><msup><mi>𝐒</mi><mi>⊤</mi></msup></mrow><annotation encoding="application/x-tex">\boldsymbol{\Sigma} = \mathbf{S} \mathbf{S}^\top</annotation></semantics></math>
for some matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐒</mi><annotation encoding="application/x-tex">\mathbf{S}</annotation></semantics></math>
and rewrite
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>𝐗</mi><mi>⊤</mi></msup><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐗</mi></mrow><annotation encoding="application/x-tex">\mathbf{X}^\top \boldsymbol{\Sigma}^{-1} \mathbf{X}</annotation></semantics></math>
as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>𝐗</mi><mi>⊤</mi></msup><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐒</mi><mi>⊤</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>𝐒</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐗</mi><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐒</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐗</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>⊤</mi></msup><msup><mi>𝐒</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐗</mi></mrow><annotation encoding="application/x-tex">\mathbf{X}^\top (\mathbf{S}^\top)^{-1} \mathbf{S}^{-1} \mathbf{X} = (\mathbf{S}^{-1} \mathbf{X})^\top \mathbf{S}^{-1} \mathbf{X}</annotation></semantics></math>.
Then one computes the inverse products by finding
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐒</mi><annotation encoding="application/x-tex">\mathbf{S}</annotation></semantics></math>.</p>
<p>One way to find
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐒</mi><annotation encoding="application/x-tex">\mathbf{S}</annotation></semantics></math>
is to use an eigendecomposition. The eigendecomposition of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝚺</mi><annotation encoding="application/x-tex">\boldsymbol{\Sigma}</annotation></semantics></math>
(which is real and symmetric) is given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝚺</mi><mo>=</mo><mi>𝐔</mi><mi>𝐃</mi><msup><mi>𝐔</mi><mi>⊤</mi></msup><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation*}
\boldsymbol{\Sigma} = \mathbf{U} \mathbf{D} \mathbf{U}^\top,
\end{equation*}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐔</mi><annotation encoding="application/x-tex">\mathbf{U}</annotation></semantics></math>
is an orthogonal matrix of eigenvectors of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝚺</mi><annotation encoding="application/x-tex">\boldsymbol{\Sigma}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐃</mi><annotation encoding="application/x-tex">\mathbf{D}</annotation></semantics></math>
is a diagonal matrix with eigenvalues of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝚺</mi><annotation encoding="application/x-tex">\boldsymbol{\Sigma}</annotation></semantics></math>
on the diagonal. Then taking
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐒</mi><mo>=</mo><mi>𝐔</mi><msup><mi>𝐃</mi><mrow><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{S} = \mathbf{U}\mathbf{D}^{1/2}</annotation></semantics></math>
implies
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>𝐒</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo>=</mo><msup><mi>𝐃</mi><mrow><mo>−</mo><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><msup><mi>𝐔</mi><mi>⊤</mi></msup></mrow><annotation encoding="application/x-tex">\mathbf{S}^{-1} = \mathbf{D}^{-1/2} \mathbf{U}^\top</annotation></semantics></math>,
which follows because
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐔</mi><annotation encoding="application/x-tex">\mathbf{U}</annotation></semantics></math>
is orthonormal
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>𝐔</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo>=</mo><msup><mi>𝐔</mi><mi>⊤</mi></msup></mrow><annotation encoding="application/x-tex">\mathbf{U}^{-1} = \mathbf{U}^\top</annotation></semantics></math>)
and is straightforward to calculate as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>𝐃</mi><mrow><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><annotation encoding="application/x-tex">\mathbf{D}^{1/2}</annotation></semantics></math>
is diagonal. Also notice that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>𝚺</mi><mrow><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><mo>=</mo><mi>𝐔</mi><msup><mi>𝐃</mi><mrow><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><msup><mi>𝐔</mi><mi>⊤</mi></msup></mrow><annotation encoding="application/x-tex">\boldsymbol{\Sigma}^{1/2} = \mathbf{U} \mathbf{D}^{1/2} \mathbf{U}^\top</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>𝐃</mi><mrow><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><annotation encoding="application/x-tex">\mathbf{D}^{1/2}</annotation></semantics></math>
is a diagonal matrix with square roots of eigenvalues of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝚺</mi><annotation encoding="application/x-tex">\boldsymbol{\Sigma}</annotation></semantics></math>
on the diagonal. This result follows because
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>𝚺</mi><mrow><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><msup><mi>𝚺</mi><mrow><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><mo>=</mo><mi>𝐔</mi><msup><mi>𝐃</mi><mrow><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><msup><mi>𝐔</mi><mi>⊤</mi></msup><mi>𝐔</mi><msup><mi>𝐃</mi><mrow><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><msup><mi>𝐔</mi><mi>⊤</mi></msup><mo>=</mo><mi>𝐔</mi><msup><mi>𝐃</mi><mrow><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐔</mi><mi>⊤</mi></msup><mi>𝐔</mi><mo stretchy="true" form="postfix">)</mo></mrow><msup><mi>𝐃</mi><mrow><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><msup><mi>𝐔</mi><mi>⊤</mi></msup><mo>=</mo><mi>𝐔</mi><mi>𝐃</mi><msup><mi>𝐔</mi><mi>⊤</mi></msup><mo>=</mo><mi>𝚺</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation*}
\boldsymbol{\Sigma}^{1/2}\boldsymbol{\Sigma}^{1/2} = \mathbf{U} \mathbf{D}^{1/2} \mathbf{U}^\top \mathbf{U} \mathbf{D}^{1/2} \mathbf{U}^\top = \mathbf{U} \mathbf{D}^{1/2} (\mathbf{U}^\top \mathbf{U}) \mathbf{D}^{1/2} \mathbf{U}^\top = \mathbf{U} \mathbf{D} \mathbf{U}^\top = \boldsymbol{\Sigma}.
\end{equation*}</annotation></semantics></math> So not only does the
eigendecomposition approach give us the inverse products, it also gives
us
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>𝚺</mi><mrow><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><annotation encoding="application/x-tex">\boldsymbol{\Sigma}^{1/2}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><annotation encoding="application/x-tex">\boldsymbol{\Sigma}^{-1/2}</annotation></semantics></math>.
While straightforward, this approach is less efficient than the Cholesky
decomposition <span class="citation">(Golub and Van Loan 2013)</span>,
which we discuss next.</p>
<p>The Cholesky decomposition decomposes
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝚺</mi><annotation encoding="application/x-tex">\boldsymbol{\Sigma}</annotation></semantics></math>
into the product between
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐂</mi><annotation encoding="application/x-tex">\mathbf{C}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>𝐂</mi><mi>⊤</mi></msup><annotation encoding="application/x-tex">\mathbf{C}^\top</annotation></semantics></math>
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝚺</mi><mo>=</mo><mi>𝐂</mi><msup><mi>𝐂</mi><mi>⊤</mi></msup></mrow><annotation encoding="application/x-tex">\boldsymbol{\Sigma} = \mathbf{C}\mathbf{C}^\top</annotation></semantics></math>),
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐂</mi><annotation encoding="application/x-tex">\mathbf{C}</annotation></semantics></math>
is a lower triangular matrix. Note that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐂</mi><annotation encoding="application/x-tex">\mathbf{C}</annotation></semantics></math>
is generally not equal to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>𝚺</mi><mrow><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><annotation encoding="application/x-tex">\boldsymbol{\Sigma}^{1/2}</annotation></semantics></math>.
Taking
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐒</mi><annotation encoding="application/x-tex">\mathbf{S}</annotation></semantics></math>
to be
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐂</mi><annotation encoding="application/x-tex">\mathbf{C}</annotation></semantics></math>,
we see that finding the inverse products requires solving
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>𝐂</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐗</mi></mrow><annotation encoding="application/x-tex">\mathbf{C}^{-1}\mathbf{X}</annotation></semantics></math>.
Observe that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>𝐂</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐗</mi><mo>=</mo><mi>𝐀</mi></mrow><annotation encoding="application/x-tex">\mathbf{C}^{-1}\mathbf{X} = \mathbf{A}</annotation></semantics></math>
for some matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐀</mi><annotation encoding="application/x-tex">\mathbf{A}</annotation></semantics></math>.
This implies
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐗</mi><mo>=</mo><mi>𝐂</mi><mi>𝐀</mi></mrow><annotation encoding="application/x-tex">\mathbf{X} = \mathbf{C}\mathbf{A}</annotation></semantics></math>,
which for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐀</mi><annotation encoding="application/x-tex">\mathbf{A}</annotation></semantics></math>
can be efficiently solved using forward substitution because
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐂</mi><annotation encoding="application/x-tex">\mathbf{C}</annotation></semantics></math>
is lower triangular.</p>
<p>The products in this document that involve
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>𝚺</mi><mrow><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><annotation encoding="application/x-tex">\boldsymbol{\Sigma}^{1/2}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">\boldsymbol{\Sigma}^{-1}</annotation></semantics></math>
are generally implemented in <code>spmodel</code> using
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐂</mi><annotation encoding="application/x-tex">\mathbf{C}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>𝐂</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">\mathbf{C}^{-1}</annotation></semantics></math>.
The products in this document that involve
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><annotation encoding="application/x-tex">\boldsymbol{\Sigma}^{-1/2}</annotation></semantics></math>
still rely on an eigendecomposition (because recall that generally,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>𝐂</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐀</mi><mo>≠</mo><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><mi>𝐀</mi></mrow><annotation encoding="application/x-tex">\mathbf{C}^{-1} \mathbf{A} \neq \boldsymbol{\Sigma}^{-1/2} \mathbf{A}</annotation></semantics></math>).
An example is computing the Pearson residuals.</p>
</div>
<div class="section level2">
<h2 id="sec:computational">A Note on Computational Stability<a class="anchor" aria-label="anchor" href="#sec:computational"></a>
</h2>
<p>Spatial covariance matrices that have approximately no independent
error variance
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>σ</mi><mrow><mi>i</mi><mi>e</mi></mrow><mn>2</mn></msubsup><annotation encoding="application/x-tex">\sigma^2_{ie}</annotation></semantics></math>)
can have unstable inverses. When this occurs, a small value can be added
to the diagonal of the covariance matrix (via updating
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>σ</mi><mrow><mi>i</mi><mi>e</mi></mrow><mn>2</mn></msubsup><annotation encoding="application/x-tex">\sigma^2_{ie}</annotation></semantics></math>)
to impose some computational stability. In <code>spmodel</code>, if
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>σ</mi><mrow><mi>i</mi><mi>e</mi></mrow><mn>2</mn></msubsup><annotation encoding="application/x-tex">\sigma^2_{ie}</annotation></semantics></math>
is approximately zero, a small amount is added to the diagonal of the
covariance matrix. Specifically, for spatial linear models,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>σ</mi><mrow><mi>i</mi><mi>e</mi><mo>,</mo><mi>u</mi><mi>p</mi></mrow><mn>2</mn></msubsup><mo>=</mo><mtext mathvariant="normal">max</mtext><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mi>σ</mi><mrow><mi>i</mi><mi>e</mi></mrow><mn>2</mn></msubsup><mo>,</mo><msubsup><mi>σ</mi><mrow><mi>d</mi><mi>e</mi></mrow><mn>2</mn></msubsup><mi>/</mi><msup><mn>10</mn><mn>4</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\sigma^2_{ie, up} = \text{max}(\sigma^2_{ie}, \sigma^2_{de}/10^4)</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>σ</mi><mrow><mi>i</mi><mi>e</mi><mo>,</mo><mi>u</mi><mi>p</mi></mrow><mn>2</mn></msubsup><annotation encoding="application/x-tex">\sigma^2_{ie, up}</annotation></semantics></math>
denotes an “updated” version of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>σ</mi><mrow><mi>i</mi><mi>e</mi></mrow><mn>2</mn></msubsup><annotation encoding="application/x-tex">\sigma^2_{ie}</annotation></semantics></math>.
For spatial generalized linear models,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>σ</mi><mrow><mi>i</mi><mi>e</mi><mo>,</mo><mi>u</mi><mi>p</mi></mrow><mn>2</mn></msubsup><mo>=</mo><mtext mathvariant="normal">max</mtext><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mi>σ</mi><mrow><mi>i</mi><mi>e</mi></mrow><mn>2</mn></msubsup><mo>,</mo><msubsup><mi>σ</mi><mrow><mi>d</mi><mi>e</mi></mrow><mn>2</mn></msubsup><mi>/</mi><msup><mn>10</mn><mn>4</mn></msup><mo>,</mo><mi>d</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\sigma^2_{ie, up} = \text{max}(\sigma^2_{ie}, \sigma^2_{de}/10^4, d)</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mo>=</mo><mtext mathvariant="normal">max</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mi>/</mi><msup><mn>10</mn><mn>4</mn></msup><mo>,</mo><msup><mi>s</mi><mn>2</mn></msup><mi>/</mi><msup><mn>10</mn><mn>4</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">d = \text{max}(1/10^4, s^2/10^4)</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>s</mi><mn>2</mn></msup><annotation encoding="application/x-tex">s^2</annotation></semantics></math>
is the sample variance of a linear regression of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐲</mi><mo>+</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\ln(\mathbf{y} + 1)</annotation></semantics></math>
on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐗</mi><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics></math>.
This value of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>σ</mi><mrow><mi>i</mi><mi>e</mi><mo>,</mo><mi>u</mi><mi>p</mi></mrow><mn>2</mn></msubsup><annotation encoding="application/x-tex">\sigma^2_{ie, up}</annotation></semantics></math>
is also added to the diagonal of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>𝐗</mi><mi>⊤</mi></msup><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐗</mi><mo>+</mo><msup><mi>𝐗</mi><mi>⊤</mi></msup><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐃</mi><mo>−</mo><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>𝚺</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐗</mi></mrow><annotation encoding="application/x-tex">\mathbf{X}^\top \boldsymbol{\Sigma}^{-1} \mathbf{X} + \mathbf{X}^\top \boldsymbol{\Sigma}^{-1} (\mathbf{D} - \boldsymbol{\Sigma}^{-1})^{-1} \boldsymbol{\Sigma}^{-1} \mathbf{X}</annotation></semantics></math>,
used via the Sherman-Morrison-Woodbury formula required to efficiently
find the log determinant and inverse of the Hessian,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐆</mi><annotation encoding="application/x-tex">\mathbf{G}</annotation></semantics></math>,
when using spatial indexing for big data. For more on stability of
spatial covariance matrices, see <span class="citation">Diamond and
Armstrong (1984)</span>, <span class="citation">Posa (1989)</span>,
<span class="citation">O’Dowd (1991)</span>, <span class="citation">Ababou, Bagtzoglou, and Wood (1994)</span>, <span class="citation">Booker et al. (1999)</span>, <span class="citation">Martin and Simpson (2005)</span>, <span class="citation">Bivand, Pebesma, and Gomez-Rubio (2013)</span>, and
<span class="citation">Ver Hoef (2018)</span>, among others.</p>
</div>
<div class="section level2">
<h2 class="unnumbered" id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-ababou1994condition" class="csl-entry">
Ababou, Rachid, Amvrossios C Bagtzoglou, and Eric F Wood. 1994.
<span>“On the Condition Number of Covariance Matrices in Kriging,
Estimation, and Simulation of Random Fields.”</span> <em>Mathematical
Geology</em> 26: 99–133.
</div>
<div id="ref-anselin2010geoda" class="csl-entry">
Anselin, Luc, Ibnu Syabri, and Youngihn Kho. 2010. <span>“GeoDa: An
Introduction to Spatial Data Analysis.”</span> In <em>Handbook of
Applied Spatial Analysis</em>, 73–89. Springer.
</div>
<div id="ref-bivand2013sp" class="csl-entry">
Bivand, Roger S., Edzer Pebesma, and Virgilio Gomez-Rubio. 2013.
<em>Applied Spatial Data Analysis with <span>R</span>, Second
Edition</em>. Springer, NY. <a href="https://asdar-book.org/" class="external-link">https://asdar-book.org/</a>.
</div>
<div id="ref-booker1999rigorous" class="csl-entry">
Booker, Andrew J, John E Dennis, Paul D Frank, David B Serafini,
Virginia Torczon, and Michael W Trosset. 1999. <span>“A Rigorous
Framework for Optimization of Expensive Functions by Surrogates.”</span>
<em>Structural Optimization</em> 17: 1–13.
</div>
<div id="ref-breiman2001random" class="csl-entry">
Breiman, Leo. 2001. <span>“Random Forests.”</span> <em>Machine
Learning</em> 45 (1): 5–32.
</div>
<div id="ref-brent1971algorithm" class="csl-entry">
Brent, Richard P. 1971. <span>“An Algorithm with Guaranteed Convergence
for Finding a Zero of a Function.”</span> <em>The Computer Journal</em>
14 (4): 422–25.
</div>
<div id="ref-cook1979influential" class="csl-entry">
Cook, R Dennis. 1979. <span>“Influential Observations in Linear
Regression.”</span> <em>Journal of the American Statistical
Association</em> 74 (365): 169–74.
</div>
<div id="ref-cook1982residuals" class="csl-entry">
Cook, R Dennis, and Sanford Weisberg. 1982. <em>Residuals and Influence
in Regression</em>. New York: Chapman; Hall.
</div>
<div id="ref-cressie1985fitting" class="csl-entry">
Cressie, Noel. 1985. <span>“Fitting Variogram Models by Weighted Least
Squares.”</span> <em>Journal of the International Association for
Mathematical Geology</em> 17 (5): 563–86.
</div>
<div id="ref-cressie1993statistics" class="csl-entry">
———. 1993. <em>Statistics for Spatial Data</em>. John Wiley &amp; Sons.
</div>
<div id="ref-cribari2010beta" class="csl-entry">
Cribari-Neto, Francisco, and Achim Zeileis. 2010. <span>“Beta Regression
in <span>R</span>.”</span> <em>Journal of Statistical Software</em> 34
(2): 1–24. <a href="https://doi.org/10.18637/jss.v034.i02" class="external-link">https://doi.org/10.18637/jss.v034.i02</a>.
</div>
<div id="ref-curriero1999composite" class="csl-entry">
Curriero, Frank C, and Subhash Lele. 1999. <span>“A Composite Likelihood
Approach to Semivariogram Estimation.”</span> <em>Journal of
Agricultural, Biological, and Environmental Statistics</em>, 9–28.
</div>
<div id="ref-diamond1984robustness" class="csl-entry">
Diamond, Phil, and Margaret Armstrong. 1984. <span>“Robustness of
Variograms and Conditioning of Kriging Matrices.”</span> <em>Journal of
the International Association for Mathematical Geology</em> 16: 809–22.
</div>
<div id="ref-espinheira2008beta" class="csl-entry">
Espinheira, Patrícia L, Silvia LP Ferrari, and Francisco Cribari-Neto.
2008. <span>“On Beta Regression Residuals.”</span> <em>Journal of
Applied Statistics</em> 35 (4): 407–19.
</div>
<div id="ref-fawcett2006introduction" class="csl-entry">
Fawcett, Tom. 2006. <span>“An Introduction to ROC Analysis.”</span>
<em>Pattern Recognition Letters</em> 27 (8): 861–74.
</div>
<div id="ref-fox2020comparing" class="csl-entry">
Fox, Eric W, Jay M Ver Hoef, and Anthony R Olsen. 2020. <span>“Comparing
Spatial Regression to Random Forests for Large Environmental Data
Sets.”</span> <em>PloS One</em> 15 (3): e0229509.
</div>
<div id="ref-goldman2000statistical" class="csl-entry">
Goldman, Nick, and Simon Whelan. 2000. <span>“Statistical Tests of
Gamma-Distributed Rate Heterogeneity in Models of Sequence Evolution in
Phylogenetics.”</span> <em>Molecular Biology and Evolution</em> 17 (6):
975–78.
</div>
<div id="ref-golub2013matrix" class="csl-entry">
Golub, Gene H, and Charles F Van Loan. 2013. <em>Matrix
Computations</em>. JHU press.
</div>
<div id="ref-hanley1982meaning" class="csl-entry">
Hanley, James A, and Barbara J McNeil. 1982. <span>“The Meaning and Use
of the Area Under a Receiver Operating Characteristic (ROC)
Curve.”</span> <em>Radiology</em> 143 (1): 29–36.
</div>
<div id="ref-harville1977maximum" class="csl-entry">
Harville, David A. 1977. <span>“Maximum Likelihood Approaches to
Variance Component Estimation and to Related Problems.”</span>
<em>Journal of the American Statistical Association</em> 72 (358):
320–38.
</div>
<div id="ref-harville1992mean" class="csl-entry">
Harville, David A, and Daniel R Jeske. 1992. <span>“Mean Squared Error
of Estimation or Prediction Under a General Linear Model.”</span>
<em>Journal of the American Statistical Association</em> 87 (419):
724–31.
</div>
<div id="ref-henderson1975best" class="csl-entry">
Henderson, Charles R. 1975. <span>“Best Linear Unbiased Estimation and
Prediction Under a Selection Model.”</span> <em>Biometrics</em>, 423–47.
</div>
<div id="ref-hoeting2006model" class="csl-entry">
Hoeting, Jennifer A, Richard A Davis, Andrew A Merton, and Sandra E
Thompson. 2006. <span>“Model Selection for Geostatistical
Models.”</span> <em>Ecological Applications</em> 16 (1): 87–98.
</div>
<div id="ref-hrong1996approximate" class="csl-entry">
Hrong-Tai Fai, Alex, and Paul L Cornelius. 1996. <span>“Approximate
f-Tests of Multiple Degree of Freedom Hypotheses in Generalized Least
Squares Analyses of Unbalanced Split-Plot Experiments.”</span>
<em>Journal of Statistical Computation and Simulation</em> 54 (4):
363–78.
</div>
<div id="ref-james2013introduction" class="csl-entry">
James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.
2013. <em>An Introduction to Statistical Learning</em>. Vol. 112.
Springer.
</div>
<div id="ref-kackar1984approximations" class="csl-entry">
Kackar, Raghu N, and David A Harville. 1984. <span>“Approximations for
Standard Errors of Estimators of Fixed and Random Effects in Mixed
Linear Models.”</span> <em>Journal of the American Statistical
Association</em> 79 (388): 853–62.
</div>
<div id="ref-kenward1997small" class="csl-entry">
Kenward, Michael G, and James H Roger. 1997. <span>“Small Sample
Inference for Fixed Effects from Restricted Maximum Likelihood.”</span>
<em>Biometrics</em>, 983–97.
</div>
<div id="ref-kenward2009improved" class="csl-entry">
———. 2009. <span>“An Improved Approximation to the Precision of Fixed
Effects from Restricted Maximum Likelihood.”</span> <em>Computational
Statistics &amp; Data Analysis</em> 53 (7): 2583–95.
</div>
<div id="ref-littell2006sas" class="csl-entry">
Littell, Ramon C, George A Milliken, Walter W Stroup, Russell D
Wolfinger, and Schabenberber Oliver. 2006. <em>SAS for Mixed
Models</em>. SAS publishing.
</div>
<div id="ref-macqueen1967classification" class="csl-entry">
MacQueen, J. 1967. <span>“Classification and Analysis of Multivariate
Observations.”</span> In <em>5th Berkeley Symp. Math. Statist.
Probability</em>, 281–97. University of California Los Angeles LA USA.
</div>
<div id="ref-martin2005use" class="csl-entry">
Martin, Jay D, and Timothy W Simpson. 2005. <span>“Use of Kriging Models
to Approximate Deterministic Computer Models.”</span> <em>AIAA
Journal</em> 43 (4): 853–63.
</div>
<div id="ref-meinshausen2006quantile" class="csl-entry">
Meinshausen, Nicolai, and Greg Ridgeway. 2006. <span>“Quantile
Regression Forests.”</span> <em>Journal of Machine Learning
Research</em> 7 (6).
</div>
<div id="ref-montgomery2021introduction" class="csl-entry">
Montgomery, Douglas C, Elizabeth A Peck, and G Geoffrey Vining. 2021.
<em>Introduction to Linear Regression Analysis</em>. John Wiley &amp;
Sons.
</div>
<div id="ref-muschelli2020roc" class="csl-entry">
Muschelli III, John. 2020. <span>“ROC and AUC with a Binary Predictor: A
Potentially Misleading Metric.”</span> <em>Journal of
Classification</em> 37 (3): 696–708.
</div>
<div id="ref-myers2012generalized" class="csl-entry">
Myers, Raymond H, Douglas C Montgomery, G Geoffrey Vining, and Timothy J
Robinson. 2012. <em>Generalized Linear Models: With Applications in
Engineering and the Sciences</em>. John Wiley &amp; Sons.
</div>
<div id="ref-nelder1965simplex" class="csl-entry">
Nelder, John A, and Roger Mead. 1965. <span>“A Simplex Method for
Function Minimization.”</span> <em>The Computer Journal</em> 7 (4):
308–13.
</div>
<div id="ref-odowd1991conditioning" class="csl-entry">
O’Dowd, RJ. 1991. <span>“Conditioning of Coefficient Matrices of
Ordinary Kriging.”</span> <em>Mathematical Geology</em> 23: 721–39.
</div>
<div id="ref-patterson1971recovery" class="csl-entry">
Patterson, Desmond, and Robin Thompson. 1971. <span>“Recovery of
Inter-Block Information When Block Sizes Are Unequal.”</span>
<em>Biometrika</em> 58 (3): 545–54.
</div>
<div id="ref-pinheiro2006mixed" class="csl-entry">
Pinheiro, José, and Douglas Bates. 2006. <em>Mixed-Effects Models in s
and s-PLUS</em>. Springer science &amp; business media.
</div>
<div id="ref-posa1989conditioning" class="csl-entry">
Posa, D. 1989. <span>“Conditioning of the Stationary Kriging Matrices
for Some Well-Known Covariance Models.”</span> <em>Mathematical
Geology</em> 21: 755–65.
</div>
<div id="ref-prasad1990estimation" class="csl-entry">
Prasad, NG Narasimha, and Jon NK Rao. 1990. <span>“The Estimation of the
Mean Squared Error of Small-Area Estimators.”</span> <em>Journal of the
American Statistical Association</em> 85 (409): 163–71.
</div>
<div id="ref-robin2011proc" class="csl-entry">
Robin, Xavier, Natacha Turck, Alexandre Hainard, Natalia Tiberti,
Frédérique Lisacek, Jean-Charles Sanchez, and Markus Müller. 2011.
<span>“pROC: An Open-Source Package for r and s+ to Analyze and Compare
ROC Curves.”</span> <em>BMC Bioinformatics</em> 12: 77.
</div>
<div id="ref-satterthwaite1946approximate" class="csl-entry">
Satterthwaite, Franklin E. 1946. <span>“An Approximate Distribution of
Estimates of Variance Components.”</span> <em>Biometrics Bulletin</em> 2
(6): 110–14.
</div>
<div id="ref-schluchter1990small" class="csl-entry">
Schluchter, Mark D, and Janet T Elashoff. 1990. <span>“Small-Sample
Adjustments to Tests with Unbalanced Repeated Measures Assuming Several
Covariance Structures.”</span> <em>Journal of Statistical Computation
and Simulation</em> 37 (1-2): 69–87.
</div>
<div id="ref-schwarz1978estimating" class="csl-entry">
Schwarz, Gideon. 1978. <span>“Estimating the Dimension of a
Model.”</span> <em>The Annals of Statistics</em>, 461–64.
</div>
<div id="ref-searle2009variance" class="csl-entry">
Searle, Shayle R, George Casella, and Charles E McCulloch. 2009.
<em>Variance Components</em>. John Wiley &amp; Sons.
</div>
<div id="ref-self1987asymptotic" class="csl-entry">
Self, Steven G, and Kung-Yee Liang. 1987. <span>“Asymptotic Properties
of Maximum Likelihood Estimators and Likelihood Ratio Tests Under
Nonstandard Conditions.”</span> <em>Journal of the American Statistical
Association</em> 82 (398): 605–10.
</div>
<div id="ref-sherman1949adjustment" class="csl-entry">
Sherman, Jack. 1949. <span>“Adjustment of an Inverse Matrix
Corresponding to Changes in the Elements of a Given Column or a Given
Row of the Original Matrix.”</span> <em>Annals of Mathematical
Statistics</em> 20 (4): 621.
</div>
<div id="ref-sherman1950adjustment" class="csl-entry">
Sherman, Jack, and Winifred J Morrison. 1950. <span>“Adjustment of an
Inverse Matrix Corresponding to a Change in One Element of a Given
Matrix.”</span> <em>The Annals of Mathematical Statistics</em> 21 (1):
124–27.
</div>
<div id="ref-stram1994variance" class="csl-entry">
Stram, Daniel O, and Jae Won Lee. 1994. <span>“Variance Components
Testing in the Longitudinal Mixed Effects Model.”</span>
<em>Biometrics</em>, 1171–77.
</div>
<div id="ref-tobler1970computer" class="csl-entry">
Tobler, Waldo R. 1970. <span>“A Computer Movie Simulating Urban Growth
in the Detroit Region.”</span> <em>Economic Geography</em> 46 (sup1):
234–40.
</div>
<div id="ref-venables2002mass" class="csl-entry">
Venables, W. N., and B. D. Ripley. 2002. <em>Modern Applied Statistics
with s</em>. Fourth. New York: Springer. <a href="http://www.stats.ox.ac.uk/pub/MASS4/" class="external-link">http://www.stats.ox.ac.uk/pub/MASS4/</a>.
</div>
<div id="ref-ver2012invented" class="csl-entry">
Ver Hoef, Jay M. 2012. <span>“Who Invented the Delta Method?”</span>
<em>The American Statistician</em> 66 (2): 124–27.
</div>
<div id="ref-ver2018kriging" class="csl-entry">
———. 2018. <span>“Kriging Models for Linear Networks and Non-Euclidean
Distances: Cautions and Solutions.”</span> <em>Methods in Ecology and
Evolution</em> 9 (6): 1600–1613.
</div>
<div id="ref-ver2023marginal" class="csl-entry">
Ver Hoef, Jay M, Eryn Blagg, Michael Dumelle, Philip M Dixon, Dale L
Zimmerman, and Paul Conn. 2023. <span>“Marginal Inference for
Hierarchical Generalized Linear Mixed Models with Patterned Covariance
Matrices Using the Laplace Approximation.”</span> <em>arXiv Preprint
arXiv:2305.02978</em>.
</div>
<div id="ref-ver2023indexing" class="csl-entry">
Ver Hoef, Jay M, Michael Dumelle, Matt Higham, Erin E Peterson, and
Daniel J Isaak. 2023. <span>“Indexing and Partitioning the Spatial
Linear Model for Large Data Sets.”</span> <em>arXiv Preprint
arXiv:2305.07811</em>.
</div>
<div id="ref-ver2018spatial" class="csl-entry">
Ver Hoef, Jay M, Erin E Peterson, Mevin B Hooten, Ephraim M Hanks, and
Marie-Josèe Fortin. 2018. <span>“Spatial Autoregressive Models for
Statistical Inference from Ecological Data.”</span> <em>Ecological
Monographs</em> 88 (1): 36–59.
</div>
<div id="ref-wolf1978helmert" class="csl-entry">
Wolf, Helmut. 1978. <span>“The Helmert Block Method-Its Origin and
Development.”</span> In <em>Proceedings of the Second International
Symposium on Problems Related to the Redefinition of North American
Geodetic Networks,(NOAA, Arlington-Va, 1978)</em>, 319–26.
</div>
<div id="ref-wolfinger1994computing" class="csl-entry">
Wolfinger, Russ, Randy Tobias, and John Sall. 1994. <span>“Computing
Gaussian Likelihoods and Their Derivatives for General Linear Mixed
Models.”</span> <em>SIAM Journal on Scientific Computing</em> 15 (6):
1294–1310.
</div>
<div id="ref-woodbury1950inverting" class="csl-entry">
Woodbury, Max A. 1950. <em>Inverting Modified Matrices</em>. Department
of Statistics, Princeton University.
</div>
<div id="ref-wright2015ranger" class="csl-entry">
Wright, Marvin N, and Andreas Ziegler. 2015. <span>“Ranger: A Fast
Implementation of Random Forests for High Dimensional Data in c++ and
r.”</span> <em>arXiv Preprint arXiv:1508.04409</em>.
</div>
<div id="ref-zimmerman2024spatial" class="csl-entry">
Zimmerman, Dale L, and Jay M Ver Hoef. 2024. <em>Spatial Linear Models
for Environmental Data</em>. CRC Press.
</div>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Michael Dumelle, Matt Higham, Jay M. Ver Hoef.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.0.</p>
</div>

    </footer>
</div>





  </body>
</html>
