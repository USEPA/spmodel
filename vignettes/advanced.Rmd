---
title: "Advanced spmodel Features"
author: "Michael Dumelle, Matt Higham, and Jay M. Ver Hoef"
output: 
  html_document:
    theme: default
    number_sections: true
    highlighted: default 
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: no
    toc_depth: 2
vignette: >
  %\VignetteIndexEntry{advanced}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
library(ggplot2)
```

# Introduction

The `spmodel` R package is used to fit and summarize spatial models and make predictions at unobserved locations (Kriging). This vignette covers advanced features of `spmodel`, which can be read in any order: 

*   Random effects for incorporating additional sources of variability.
*   Specifying fixed (known) values of spatial and/or random effect covariance parameters for model-fitting.
*   Anisotropy for spatial covariance functions that behave differently in different directions.
*   Partition factors that assume observations are uncorrelated if they are from different levels of a factor variable.
*   Simulating spatially dependent normal (Gaussian) random variables.
*   Big data approaches to model-fitting and prediction.
*   Spatial autoregressive models for areal data.

We load `spmodel` by running
```{r}
library(spmodel)
```

We will use the `moss` data throughout the vignette. The first few rows can be viewed by running
```{r}
moss
```
The purpose of this vignette is to gain experience using advanced features in `spmodel`, not necessarily to find the best fitting model to the data.

While introducing the advanced features, we often compare against a basic spatial linear model of log zinc concentration on the log distance to the road using an exponential spatial covariance function.
```{r}
basic <- splm(log_Zn ~ log_dist2road, moss, "exponential")
glance(basic)
```

If you use `spmodel` in a formal publication or report, please cite it. Citing `spmodel` lets us devote more resources to it in the future. We view the `spmodel` citation by running
```{r}
citation(package = "spmodel")
```

There are three additional `spmodel` vignettes:

* An overview of basic features: `vignette("basic", "spmodel")`
* A detailed guide with relevant methodological background: `vignette("guide", "spmodel")`
* Technical details regarding many functions: `vignette("technical", "spmodel")`

# Random Effects

Sometimes additional sources of variability are incorporated into linear models via random effects. The spatial linear model with random effects can be written as
\begin{equation*}
\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \mathbf{Z}_1 u_1 + ... + \mathbf{Z}_m u_m + \boldsymbol{\tau} + \boldsymbol{\epsilon},
\end{equation*} where $\mathbf{y}$ is a vector of responses, $\mathbf{X}$ is a matrix of predictor variables, $\boldsymbol{\beta}$ is a vector of fixed effects, $\mathbf{Z}_i$ is a matrix that describes the $i$th random effect, $u_i$, $\boldsymbol{\tau}$ is a vector of spatially dependent (correlated) random errors, and $\boldsymbol{\epsilon}$ is a vector of spatially independent (not correlated) random errors.

`splm()` and [`spautor()`](#autoreg) (for spatial autoregressive models) incorporate random effects using similar syntax as used by the `lme()` function from the [nlme](https://cran.r-project.org/package=nlme) R package and the `lmer()` function from the [lme4](https://cran.r-project.org/package=lme4) R package. Random effects are specified using the `random` argument, which is a formula built from variables in `data`. A couple of common ways to specify random effects in `random` include:


*   `~ (1 | group)` : Random intercepts for each level of `group`. `~ group` is shorthand for `~ (1 | group)`.
*   `~ (x | group)`: Random intercepts for each level of `group` and random slopes that depend on the variable `x` for each level of `group`.

Some additional syntax for more complicated random effects structures include:

*   `~ (x - 1 | group)`: Random slopes (without intercepts) that depend on the variable `x` for each level of `group`.
*   `~ (1 | group:subgroup)`: Random intercepts for each combination of levels in `group` and levels in `subgroup`. `~ group:subgroup` is shorthand for `~ (1 | group:subgroup)`.
*   `~ (x | group:subgroup)`: Random intercepts for each combination of levels in `group` and levels in `subgroup` and random slopes that depend on the variable `x` for each combination of levels in `group` and levels in `subgroup`.
*   `~ (x - 1 | group:subgroup)`: Random slopes (without intercepts) that depend on the variable `x` for each combination of levels in `group` and levels in `subgroup`.
*   `~ (1 | group/subgroup)`: Shorthand for `~ (1 | group) + (1 | group:subgroup)`. Commonly, the `group/subgroup` notation implies `subgroup` is nested within `group`.
*   `~ x | group/subgroup`: Shorthand for `~ (x | group) + (x | group:subgroup)`. Commonly, the `group/subgroup` notation implies `subgroup` is nested within `group`.
*   `~ (x - 1 | group/subgroup)`: Shorthand for `~ (x - 1 | group) + (x - 1 | group:subgroup)`. Commonly, the `group/subgroup` notation implies `subgroup` is nested within `group`.

Distinct random effects terms are separated in `random` by `+`. Each term **must** be wrapped in parentheses. For example, to incorporate random intercepts for `group` and `subgroup`, `random` looks like `~ (1 | group) + (1 | subgroup)`. For random intercepts, recall that `~ group` is shorthand for `~ (1 | group)`. Thus, an equivalent representation of `~ (1 | group) + (1 | subgroup)` is `~ group + subgroup`. Note that for both random intercepts and random slopes, the variable on the right-hand side of `|` **must** be a factor (or character) variable.

## Random Intercepts

The `year` variable in `moss` indicates which year the sample was taken (`2001` or `2006`). It is likely that there is some variability in log zinc attributable to the different years. This year-to-year variability can be incorporated using a random intercept for `year`. This random effect lets each year have a different intercept, or "starting mean." We fit a spatial linear model of log zinc concentration on the log distance to the road using an exponential spatial covariance function with a random effect for year by running

```{r}
rand1 <- splm(log_Zn ~ log_dist2road, data = moss,
              spcov_type = "exponential", random = ~ year)
```

We glance at the models fit without (`basic`) and with (`rand1`) the random intercept by running
```{r}
glances(basic, rand1)
```

The model with the random effect has a much lower AIC, indicating it is a better fit to the data. This is confirmed when performing a likelihood ratio test comparing the full model with the random effect to the reduced model (which is nested in the full model) without the random effect:
```{r}
tidy(anova(basic, rand1))
```

We summarize the random intercept model by running
```{r}
summary(rand1)
```

We tidy the random effect by running
```{r}
tidy(rand1, effects = "randcov")
```

We obtain best linear unbiased predictions (BLUPs) of the random effects (similar to `ranef()` from nlme or lme4) by running
```{r}
fitted(rand1, type = "randcov")
```

Because of the random effect, observations in 2001 have higher intercepts than observations in 2006.

## Random Intercepts and Slopes

Suppose that in addition to random intercepts for year, the association between `log_dist2road` on `log_Zn` differs between the years. In this context, we can include a random slope for `log_dist2road` within year. We incorporate the random slope by running
```{r}
rand2 <- splm(log_Zn ~ log_dist2road, data = moss,
              spcov_type = "exponential", random = ~ log_dist2road | year)
```

# Specifying Known and Initial Covariance Parameter Values {#initvals}

Sometimes we would like to assume certain covariance parameters are known (i.e., fixed and not requiring estimation). Perhaps we have expert knowledge about the covariance structure and know the true value of a parameter. Or perhaps we want to compare a model assuming all parameters are unknown to a model that assumes some parameters are known. We specify known covariance parameter values in `splm()` or [`spautor()`](#autoreg) using the `spcov_initial` argument. Suppose we want to fix the `ie` parameter at zero (i.e., fit a model without a nugget). To do this, we first create an `spcov_initial` object with the `spcov_initial()` function. The first argument to `spcov_initial()` is the spatial covariance type. When `spcov_initial` is specified, `spcov_type` is ignored. Additional arguments to `spcov_initial()` indicate initial values for specific parameters to be used in optimization. The `known` argument indicates which parameters should be assumed known. Recall we will assume `ie` is zero and known. We specify the `spcov_initial()` object by running
```{r}
params1 <- spcov_initial("exponential", ie = 0, known = "ie")
params1
```
`params1` is a list with two elements: `initial` and `is_known`.`initial` specifies the initial parameter values, and `is_known` indicates whether any of the parameters are assumed known. The class of `params1` is the spatial covariance type (here `"exponential"`).

We then fit a model by running
```{r}
init1 <- splm(log_Zn ~ log_dist2road, data = moss, spcov_initial = params1)
summary(init1)
```

We compare to the basic model by running
```{r}
glances(basic, init1)
```
The AIC is lower for the model that does not assume `ie` is zero, which indicates that `ie` is important to the model.

`randcov_initial` works similarly and is used for specifying known and initial values of random effect variances:
```{r}
params2 <- randcov_initial(year = 1, known = "year")
params2
init2 <- splm(log_Zn ~ log_dist2road, moss, spcov_initial = params1,
              random = ~ year, randcov_initial = params2)
summary(init2)
```

# Anisotropy

A common assumption for spatial covariance functions is isotropy. An isotropic spatial covariance function behaves similarly in all directions (i.e., is independent of direction) as a function of distance. An anisotropic covariance function does not behave similarly in all directions. Consider the spatial covariance imposed by an eastward-moving wind pattern. It is likely that a one-unit distance in the x-direction means something different than a one-unit distance in the y-direction. Observe the figures below:

```{r, echo = FALSE, out.width = "49%", fig.show="hold"}
# PRELIMINARIES 
r <- 1
theta_seq <- seq(0, 2 * pi, length.out = 1000)
x_orig <- r * cos(theta_seq)
y_orig <- r * sin(theta_seq)
df_orig <- data.frame(x = x_orig, y = y_orig)

# FIRST FIGURE
ggplot(df_orig, aes(x = x, y = y)) + 
  geom_point() +
  scale_x_continuous(limits = c(-1, 1), breaks = c(0)) + 
  scale_y_continuous(limits = c(-1, 1), breaks = c(0)) +
  labs(x = "x-distance", y = "y-distance") +
  theme(axis.text = element_text(size = 18),
        axis.title = element_text(size = 18)) +
  coord_fixed() # theme_gray() causes polygon edge not found bug

# SECOND FIGURE
theta <- pi / 4 # (30 degrees)
R <- 1 / 3
scale <- matrix(c(1, 0, 0, R), nrow = 2, byrow = TRUE)
rotate <- matrix(c(cos(theta), -sin(theta), sin(theta), cos(theta)), nrow = 2, byrow = TRUE)
transform <- rotate %*% scale
coords <- transform %*% rbind(df_orig[["x"]], df_orig[["y"]])
df <- data.frame(x = coords[1, ], y = coords[2, ])
ggplot(df, aes(x = x, y = y)) + 
  geom_point() + 
  scale_x_continuous(limits = c(-1, 1), breaks = c(0)) + 
  scale_y_continuous(limits = c(-1, 1), breaks = c(0)) +
  labs(x = "x-distance", y = "y-distance") +
  theme(axis.text = element_text(size = 18),
        axis.title = element_text(size = 18)) +
  coord_fixed() # theme_gray() causes polygon edge not found bug
```

The black outline of each ellipse is a level curve of equal correlation. The left ellipse (a circle) represents an isotropic covariance function. The distance at which the correlation between two observations lays on the level curve is the same in all directions. The right ellipse represents an anisotropic covariance function. The distance at which the correlation between two observations lays on the level curve is different in different directions.

To accommodate spatial anisotropy, the original coordinates must be transformed such that they resemble an isotropic process. This transformation involves a rotation and a scaling. Consider a set of anisotropic $x$ and $y$ coordinates that should be transformed into isotropic $x^*$ and $y^*$ coordinates. This transformation is formally defined as 
$$
\begin{bmatrix}
x^* \\
y^* \\
\end{bmatrix} = 
\begin{bmatrix}
1 & 0 \\
0 & 1 / S \\
\end{bmatrix} 
\begin{bmatrix}
cos(\alpha) & sin(\alpha) \\
-sin(\alpha) & cos(\alpha) \\
\end{bmatrix}
\begin{bmatrix}
x \\
y \\
\end{bmatrix}
$$


The original coordinates are first multiplied by the rotation matrix, which rotates the coordinates clockwise by angle $\alpha$. They are then multiplied by the scaling matrix, which scales the minor axis of the spatial covariance ellipse by the reciprocal of $S$. The transformed coordinates are then used to compute distances and spatial covariances. This type of anisotropy is more formally known as "geometric" anisotropy because it involves a geometric transformation of the coordinates.

In `spmodel`, $\alpha$ and $R$ from the above matrices are called `rotate` and `scale`, respectively. These parameters are estimated in `splm()` if `estmethod` is `"reml"` or `"ml"` by setting `anisotropy` to `TRUE`:
```{r}
anis1 <- splm(log_Zn ~ log_dist2road, moss, "exponential", anisotropy = TRUE)
summary(anis1)
```

We compare the fit of the anisotropy model to the fit of the basic model that assumes isotropy by running
```{r}
glances(basic, anis1)
```

The anisotropy model has a slightly lower AIC, indicating that the model is a slightly better fit to the data (at a cost of increased computational burden).

Anisotropy parameters are also estimated (if `estmethod` is `"reml"` or `"ml"`) when either `rotate` or `scale` is specified in the `spcov_initial` argument (whether the parameters are assumed known or not):
```{r}
params1 <- spcov_initial("exponential", scale = 0.5)
anis2 <- splm(log_Zn ~ log_dist2road, moss, spcov_initial = params1)
summary(anis2)
```

Anisotropy parameters cannot be estimated when `estmethod` is `"sv-wls"` or `"sv-cl"`. Anisotropy can be accommodated, however, when `estmethod` is `"sv-wls"` or `"sv-cl"` and `rotate` or `scale` are assumed known in the `spcov_initial` argument.

The computational demand for fitting anisotropic models is much higher than for fitting isotropic models, both because there are more parameters to estimate and because the objective function is more complicated.

# Partition Factors

A partition factor is a factor (or categorical) variable in which observations from different levels of the partition factor are assumed uncorrelated. For example, suppose we conduct an experiment in separate fields and we want to model spatial covariance within a field but assume that there is no spatial covariance across fields (i.e., observations from separate fields are uncorrelated). Or perhaps we observe the same spatial locations in different years and want to assume observations in different years are uncorrelated. Both of these examples rely on partition factors.

To specify a partition factor in `splm()` or [`spautor()`](#autoreg), use the `partition_factor` argument. The `partition_factor` argument is a formula that contains a single variable. In the `moss` data, suppose we want to assume observations across years are uncorrelated. We fit this model by running
```{r}
part1 <- splm(log_Zn ~ log_dist2road, moss, "exponential", partition_factor = ~ year)
```

We compare this fitted model to the basic fitted model which allows observations across years to be correlated by running
```{r}
glances(basic, part1)
```

The model with the partition factor has the lower AIC, indicating that it is a better fit to the data. Assuming observations across years are uncorrelated lets the model more accurately describe within-year correlation than a model that tries to incorporate within-year and between-year correlation all at once.

# Simulating Data {#simdata}

The `sprnorm()` function is used to simulate spatially dependent normal (Gaussian) random variables. Suppose we want to simulate at a set of 3000 locations in the unit square. First, we set a reproducible seed and then randomly generate 3000 x-coordinates and y-coordinates by running
```{r}
set.seed(0)
n <- 3000
x <- runif(n)
y <- runif(n)
sim_coords <- tibble::tibble(x, y)
```

We view the first few locations by running
```{r}
sim_coords
```

The first argument to `sprnorm()` takes an `spcov_params` object. An `spcov_params` object specifies the desired spatial covariance structure and can be created using the `spcov_params()` function. The `spcov_params()` function takes arguments for the spatial covariance type and for each parameter:
```{r}
sim_params <- spcov_params("exponential", de = 5, ie = 1, range = 0.5)
```

The next argument to `sprnorm()` is `mean`, which specifies the mean vector. This can either be length one and recycled or have length equal to the number of observations. The default is `0`. The next argument is `samples`, which indicates the number of desired samples. The default is `1`, though the computational cost for generating $m$ samples is nearly the same as for generating 1 sample. If more than one sample is desired, the output is a matrix where rows indicate observations and columns indicate independent samples. The next argument to `sprnorm()` is `data`, which contains geographic information. If the data are an `sp` or `sf` object, the geographic information is taken from the object's geometry. If the data are a data frame, either coordinates via `xcoord` and `ycoord` (if the data are point-referenced) or a weight matrix via `W` (for autoregressive models with areal data) are specified. Additional arguments include `randcov_params` for incorporating random effects and `partition_factor` for incorporating a partition factor. Next we simulate a response variable using `sim_params` and `sim_coords`.
```{r}
sim_resp <- sprnorm(sim_params, data = sim_coords, xcoord = x, ycoord = y)
sim_data <- tibble::tibble(sim_coords, sim_resp)
```

The spatial dependence in `sim_data` is noticeable, with clusters of observations having similar values:
```{r, echo = FALSE}
sim_data_sf <- sf::st_as_sf(sim_data, coords = c("x", "y"))
plot(
  sim_data_sf["sim_resp"],
  pal = hcl.colors,
  pch = 19,
  bgc = "grey95",
  graticule = TRUE,
  key.pos = 4,
  breaks = seq(-7, 7, length.out = 10)
)

```

```{r, echo = FALSE}
# ggplot(sim_data, aes(x = x, y = y, color = sim_resp)) +
#   geom_point() +
#   theme_gray(base_size = 20) +
#   scale_colour_viridis_c()
```

# Big Data

`spmodel` has options for accommodating big data using approximations. Big data options for model-fitting are available for `splm()` objects, while big data options for prediction are available for `splm()` objects and [`spautor()`](#autoreg) objects. Before we explore these big data options, if the `sim_data` object from the [Simulating Data section](#simdata) has not yet been created, it can be created by running
```{r, eval = FALSE}
set.seed(0)
n <- 3000
x <- runif(n)
y <- runif(n)
sim_coords <- tibble::tibble(x, y)

sim_params <- spcov_params("exponential", de = 5, ie = 1, range = 0.5)
sim_resp <- sprnorm(sim_params, data = sim_coords, xcoord = x, ycoord = y)
sim_data <- tibble::tibble(sim_coords, sim_resp)
```

We then simulate 1000 locations in the unit square requiring prediction by running
```{r}
n_pred <- 1000
x <- runif(n_pred)
y <- runif(n_pred)
sim_preds <- tibble::tibble(x = x, y = y)
```

## Model-fitting

First we explore big data options for `splm()` objects, which are capable of fitting models with hundreds of thousands to millions of observations using an approximation. These options are controlled by the `local` argument to `splm()`. `local` is a list with several possible elements:

* `index`: The indexing variable. Observations in different levels of `index` are assumed to be uncorrelated for the purposes of model-fitting. If `index` is not provided, it is determined by specifying `method` and either `size` or `groups`.
* `method`: The method big data approximation method used to determine `index`. Ignored if `index` is provided. If `method = "random"`, observations are randomly assigned to `index` based on `size`. If `method = "kmeans"`, observations are re-assigned to `index` based on k-means clustering on the coordinates. The default is `"random"`. Note that both methods have a random component, which means that you may get different results from separate model-fitting calls. To ensure consistent results, specify `index` or set a seed via `set.seed()`.
* `size`: The number of observations in each index group when `method` is `"random"`. If the number of observations is not divisible by `size`, some levels get `size - 1` observations. The default is 50.
* `groups`: The number of unique groups in `index`. If method is `"random"`, `size` is `ceiling(n / groups)`, where `n` is the sample size. Automatically determined if `size` is specified. If method is `"kmeans"`, `groups` is the number of clusters.
* `var_adjust`: The approach for adjusting the variance-covariance matrix of the fixed effects. `"none"` for no adjustment, `"theoretical"` for the theoretically-correct (and computationally-intensive) adjustment, `"pooled"` for the pooled adjustment, and `"empirical"` for the empirical adjustment. The default is `"none"`.
* `parallel`: If `TRUE`, parallel processing via the parallel package is automatically used. The default is `FALSE`.
* `ncores`: If `parallel = TRUE`, the number of cores use for parallelization. The default is the number of available cores on your machine.

The simplest way to implement the default big data approximation is to set the `local` argument equal to `TRUE`. This is shorthand for `local = list(method = "random", size = 50, var_adjust = "none", parallel = FALSE)`:
```{r}
big1 <- splm(sim_resp ~ 1, sim_data, "exponential", x, y, local = TRUE)
summary(big1)
```

We specify a more complicated version of `local` by running
```{r}
local_list <- list(
  groups = 30,
  method = "kmeans",
  var_adjust = "theoretical",
  parallel = TRUE,
  ncores = 2
)
big2 <- splm(sim_resp ~ 1, sim_data, "exponential", x, y, local = local_list)
summary(big2)
```

As previously mentioned, if `local` is specified using a list structure and only some elements are provided, defaults are chosen for the remaining elements. For example, `local = list(method = "kmeans")` is shorthand for `local = list(method = "kmeans", size = 50, var_adjust = "none", parallel = FALSE)`.

When comparing model fits, we note that the big data approximation model fits should not be compared against model fits for all the data when using likelihood-based statistics like `AIC()`, `AICc()`, `logLik()`, and `deviance()` because the big data approximation does not use the true likelihood.

## Prediction

Next we explore big data prediction options for `splm()` objects, which are capable of making predictions for hundreds of thousands to millions of observations using approximations. Like with model-fitting, these big data options are controlled using the `local` argument. The `local` argument is different for prediction than for model-fitting, however. For prediction, the `local` argument is a list with several possible elements:

* `method`: The big data approximation method. If `method = "all"`, all observations are used and `size` is ignored. If `method = "distance"`, the `size` data observations closest (in terms of Euclidean distance) to the observation requiring prediction are used. If `method = "covariance"`, the `size` data observations having the highest covariance with the observation requiring prediction are used. If random effects and partition factors are not used in estimation and the spatial covariance function is monotone decreasing, `"distance"` and `"covariance"` are equivalent. The default is `"covariance"`. 
* `size`: The number of data observations to use when method is `"distance"` or `"covariance"`. The default is 50.
* `parallel`: If `TRUE`, parallel processing via the parallel package is automatically used. The default is `FALSE`.
* `ncores`: If `parallel = TRUE`, the number of cores use for parallelization. The default is the number of available cores on your machine.

For models fit using `splm()`, the simplest way to implement the default big data approximation for prediction is to set the `local` argument equal to `TRUE`. This is shorthand for `local = list(method = "covariance", size = 50, parallel = FALSE)`:
```{r, results = "hide"}
predict(big1, sim_preds, local = TRUE)
```

Or we could augment the prediction data with the predictions:
```{r}
augment(big1, newdata = sim_preds, local = TRUE)
```

These observed data and predictions are visualized below:
```{r, echo = FALSE, fig.show="hold"}
sim_data_sf <- sf::st_as_sf(sim_data, coords = c("x", "y"))
plot(
    sim_data_sf["sim_resp"],
    pal = hcl.colors,
    pch = 19,
    bgc = "grey95",
    graticule = TRUE,
    key.pos = 4,
    breaks = seq(-7, 7, length.out = 10)
)

sim_preds$preds <- predict(big1, sim_preds, local = TRUE)

sim_preds_sf <- sf::st_as_sf(sim_preds, coords = c("x", "y"))
plot(
    sim_preds_sf["preds"],
    pal = hcl.colors,
    pch = 19,
    bgc = "grey95",
    graticule = TRUE,
    key.pos = 4,
    breaks = seq(-7, 7, length.out = 10)
)

sim_preds$preds <- NULL
```


The predictions display a similar pattern as the observed data.

We could alternatively specify a more complicated version of `local` by running
```{r, results = "hide"}
predict(big1, sim_preds, local = list(method = "distance", size = 30, parallel = TRUE, ncores = 2))
```

As previously mentioned, if `local` is specified using a list structure and only some elements are provided, defaults are chosen for the remaining ones. For example, `local = list(method = "distance")` is shorthand for `local = list(method = "distance", size = 50, parallel = FALSE)`.

For models fit using [`spautor()`](#autoreg), no big data approximations are available, but prediction can be sped up using parallelization. As a result, the only `local` options available for [`spautor()`](#autoreg) objects are `parallel` and `ncores`. Currently, there is no `local = TRUE` shorthand for [`spautor()`](#autoreg) objects, and `parallel` and `ncores` must be specified directly via the list structure (e.g., `local = list(parallel = TRUE)`).

# Spatial Autoregressive Models with `spautor()` {#autoreg}

Spatial autoregressive models are spatial linear models that allow for spatial dependence defined through a neighborhood structure rather than a Euclidean distance. Often this neighborhood structure is imposed via a set of polygons, and polygons sharing a boundary are considered neighbors. For example, a neighborhood structure could be state boundaries within a country. A matrix that contains neighborhood relationships among all observations is called a weight matrix.

Spatial autoregressive models are fit in `spmodel` using the `spautor()` function. The `spautor()` function is very similar to the `splm()` function: both functions generally require the `formula`, `data`, and `spcov_type` arguments. There are some optional arguments for `spautor()` that relate to the data's neighborhood structure. Next we use `spautor()` to fit a spatial autoregressive model to the `seal` data, an `sf` object with log harbor seal abundance across polygons in Alaska.

We view the first few rows of the `seal` data by running
```{r}
seal
```

We visualize the distribution of log abundance by running
```{r}
plot(
  seal["log_abund"],
  pal = hcl.colors,
  pch = 19,
  bgc = "grey95",
  graticule = TRUE,
  key.pos = 4,
)
```


```{r, echo = FALSE}
# # using ggplot2
# ggplot(seal, aes(fill = log_abund)) + 
#   geom_sf() +
#   scale_fill_viridis_c()

# ggplot(seal, aes(fill = log_abund)) +
#   theme(axis.text = element_blank(),
#         axis.title = element_blank()) +
#   geom_sf() +
#   scale_fill_viridis_c()

# # workaround using sf
# plot(
#   seal["log_abund"],
#   pal = hcl.colors,
#   pch = 19,
#   bgc = "grey95",
#   graticule = TRUE,
#   key.pos = 4,
# )
```


The grey polygons indicate that a response was not collected. Of the 62 polygons in the data, 28 are missing a response. Unlike for models fit using `splm()`, models fit using `spautor()` must incorporate these missing-data polygons while modeling spatial dependence if prediction for these polygons is desired.

We fit a spatial autoregressive model of log abundance on an intercept using a conditional autoregressive (CAR) spatial covariance by running
```{r}
sealmod <- spautor(log_abund ~ 1, data = seal, spcov_type = "car")
```

By default, polygons that share a border are considered neighbors and the weight matrix is calculated internally, though a weight matrix can instead be explicitly given using using the `W` argument.

## Model Summaries

`summary()` works the same for `spautor()` as it does for `splm()`. We summarize the fitted model by running
```{r}
summary(sealmod)
```

`de` is the estimated variance of the spatially dependent random error for sites with at least one neighbor, `range` is the parameter that controls the strength of dependence among neighbors, and `extra` is the estimated variance of the random error for sites with no neighbors (the `seal` data has islands with no neighbors). If all sites have at least one neighbor, `extra` is ignored. By default, `ie`, the estimated variance of the spatially independent random error, is assumed to be zero, though this can default can be overridden by specifying `ie` in the `spcov_initial` argument.

We tidy the fixed effects by running
```{r}
tidy(sealmod)
```

We glance at the model-fit statistics by running
```{r}
glance(sealmod)
```

We augment the data with diagnostics by running
```{r}
augment(sealmod)
```

## Prediction (Kriging)

We previously mentioned that for spatial autoregressive models, prediction can only be performed for missing-data polygons that were incorporated into model fitting. Thus, prediction using `spautor()` does not accommodate "new" locations like prediction using `splm()` does. We predict log seal abundance at the 28 sites in `seal` with missing-data polygons by running `predict(sealmod)`. We augment prediction data by running
```{r}
augment(sealmod, newdata = sealmod$newdata)
```

These predictions are visualized alongside the observed data below:
```{r, echo = FALSE}
seal_preds2 <- augment(sealmod, newdata = sealmod$newdata)
seal2 <- seal[!is.na(seal$log_abund), , drop = FALSE]
seal_preds2 <- seal_preds2[, -1]
names(seal_preds2)[1] <- "log_abund"
seal2 <- rbind(seal2, seal_preds2)
plot(
  seal2["log_abund"],
  pal = hcl.colors,
  pch = 19,
  bgc = "grey95",
  graticule = TRUE,
  key.pos = 4
)
```


```{r, echo = FALSE}
# # using ggplot2
# seal_preds <- augment(sealmod, newdata = sealmod$newdata)
# ggplot() +
#   geom_sf(data = seal, aes(fill = log_abund)) + 
#   geom_sf(data = seal_preds, aes(fill = .fitted)) +
#   scale_fill_viridis_c()

# seal_preds <- augment(sealmod, newdata = sealmod$newdata)
# ggplot() +
#   theme(axis.text = element_blank(),
#         axis.title = element_blank()) +
#   geom_sf(data = seal, aes(fill = log_abund)) +
#   geom_sf(data = seal_preds, aes(fill = .fitted)) +
#   scale_fill_viridis_c()

# workaround using sf
# seal_preds2 <- augment(sealmod, newdata = sealmod$newdata)
# seal2 <- seal[!is.na(seal$log_abund), , drop = FALSE]
# seal_preds2 <- seal_preds2[, -1]
# names(seal_preds2)[1] <- "log_abund"
# seal2 <- rbind(seal2, seal_preds2)
# plot(
#   seal2["log_abund"],
#   pal = hcl.colors,
#   pch = 19,
#   bgc = "grey95",
#   graticule = TRUE,
#   key.pos = 4
# )
```

While the fitted model in this example only uses an intercept, the same code is used for prediction with fitted models having predictor variables.

## An Additional Example

Here we revisit the `caribou` data used in the basic features vignette from a spatial autoregressive modeling perspective. Recall the first few rows of the `caribou` data:
```{r}
caribou
```

Because `caribou` are not an `sf` object, we must create a weights matrix. First we compute distances among all locations:
```{r}
coords <- cbind(caribou$x, caribou$y)
dists <- as.matrix(dist(coords))
```

Next, we specify two observations as neighbors if they are adjacent (directly east, west, north, or south) to one another. It follows that observations are neighbors if the distance between them equals one. Note that in spatial autoregressive models, an observation is not a neighbor with itself:
```{r}
W <- dists == 1
```

Currently, `W` is a logical matrix with `TRUE`s and `FALSE`s. We coerce it to a numeric matrix by running
```{r}
W <- W * 1
```

The $ij$th value in `W` is `1` if the observation in the $i$th row is neighbors with the observation in the $j$th row and `0` otherwise. We fit a spatial autoregressive model of the nitrogen percentage (the response, `z`) on water presence (`water`) and tarp cover (`tarp`) by running
```{r}
cariboumod <- spautor(z ~ water + tarp, 
                 data = caribou, spcov_type = "car", W = W)
```

We tidy an analysis of variance by running
```{r}
tidy(anova(cariboumod))
```

The analysis of variance provides significant evidence that at least one level of tarp cover is related to nitrogen percentage.
